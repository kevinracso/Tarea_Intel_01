{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Tarea1_Notebook.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevinracso/Tarea_Intel_01/blob/master/Tarea1_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gmbRYKD1odtB"
      },
      "source": [
        "# Tarea 1: Perceptrón Multicapa\n",
        "### EL4106 Inteligencia Computacional\n",
        "\n",
        "Profesor de Cátedra: Pablo Estévez<br>\n",
        "Profesor Auxiliar: Ignacio Reyes<br>\n",
        "Ayudantes: Germán García, Esteban Reyes, Mauricio Romero, Nicolás Tapia, Miguel Videla"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gr3dCq8pYd_w"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wiv1sT9tpZO-",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix as sk_conf_mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lAqEQZkkRgTJ"
      },
      "source": [
        "## Preparación de la base de datos MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WwD-4ePxRuzZ",
        "colab": {}
      },
      "source": [
        "# ----- Digito a identificar.\n",
        "RUT_veri_number = 8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RahVklIFRfbE",
        "outputId": "e2923959-a59b-43be-b956-57a0c606e55a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "def process_dataset(images, labels, selected_class):\n",
        "    \"\"\"Revuelve datos y selecciona subconjunto segun la clase seleccionada.\n",
        "    \n",
        "    Los datos de imagenes y etiquetas son revueltos, se seleccionan aquellos\n",
        "    que coinciden con la etiqueta de la clase seleccionada, y un subconjunto\n",
        "    del mismo tamaño que el anterior es seleccionado de entre todas las demas\n",
        "    clases para obtener un problema balanceado.\n",
        "    \"\"\"\n",
        "    shuffled_indexes = np.random.permutation(len(labels))  # Primer shuffle\n",
        "    images = images[shuffled_indexes]\n",
        "    labels = labels[shuffled_indexes]\n",
        "    selected_column = labels[:, selected_class]\n",
        "    selected_images_indexes = np.where(selected_column == 1)[0]\n",
        "    selected_size = len(selected_images_indexes)\n",
        "    non_selected_indexes_subset = np.where(selected_column == 0)[0][:selected_size]\n",
        "    indexes = np.concatenate(\n",
        "        (selected_images_indexes, \n",
        "        non_selected_indexes_subset),\n",
        "        axis=0)\n",
        "    # No queremos que el modelo primero vea todos los datos de una clase y\n",
        "    # despues todos los de la otra, asi que volvemos a revolver.\n",
        "    np.random.shuffle(indexes)\n",
        "    images_subset = images[indexes]\n",
        "    labels_subset = selected_column[indexes]\n",
        "    labels_subset = np.array(labels_subset, dtype=np.int32)\n",
        "    return images_subset, labels_subset\n",
        "\n",
        "\n",
        "# ----- Carga de la base de datos MNIST\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)  \n",
        "\n",
        "# ----- Preprocesamiento de datos de entrenamiento, validacion, y test\n",
        "training_images, training_labels = process_dataset(\n",
        "    mnist.train.images,\n",
        "    mnist.train.labels,\n",
        "    RUT_veri_number)\n",
        "validation_images, validation_labels = process_dataset(\n",
        "    mnist.validation.images,\n",
        "    mnist.validation.labels,\n",
        "    RUT_veri_number)\n",
        "testing_images, testing_labels = process_dataset(\n",
        "    mnist.test.images,\n",
        "    mnist.test.labels,\n",
        "    RUT_veri_number)\n",
        "print('Processing ready.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "Processing ready.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RLcbP5NJRyH_",
        "outputId": "996ee6f1-ec55-4e11-aa3c-9418ec0d31dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "source": [
        "# ----- Visualizacion de algunas imagenes de los datos de entrenamiento\n",
        "\n",
        "chosen_idx = np.random.choice(training_images.shape[0], size=6, replace=False)\n",
        "\n",
        "fig, ax = plt.subplots(1, 6, figsize=(10,5))\n",
        "for i, idx in enumerate(chosen_idx):\n",
        "    image = training_images[idx, :]\n",
        "    digit = training_labels[idx]\n",
        "    ax[i].imshow(image.reshape((28, 28)))\n",
        "    ax[i].set_title(\"Etiqueta: %d\" % digit)\n",
        "    ax[i].axis('off')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAAB/CAYAAAAgh/yPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x360 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q4f4FFyhpdDA"
      },
      "source": [
        "## Definición de Clasificador MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nfe-XLlTWnsf"
      },
      "source": [
        "### Función del modelo de red neuronal "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MS9viD6ipcIt",
        "colab": {}
      },
      "source": [
        "def model_fn(inputs, layer_sizes):\n",
        "    \"\"\"Construye el grafo computacional de la red MLP.\n",
        "    \n",
        "    Se procesa 'inputs' a través de un perceptron multicapa, cuya salida se\n",
        "    retorna en forma de logits y probabilidades de cada clase.\n",
        "    Summaries son agregados para su visualizacion en Tensorboard.\n",
        "    \n",
        "    Args:\n",
        "        inputs: Tensor de entrada de dimensiones (batch_size, n_features).\n",
        "        layer_sizes: Lista de enteros que indica el tamaño de cada capa de\n",
        "            neuronas. La salida de la capa i-esima posee dimensiones\n",
        "            (batch_size, layer_sizes[i]). El ultimo numero de la lista indica el\n",
        "            tamaño de la capa de salida, que debe ser igual al numero de clases.\n",
        "    \n",
        "    Returns:\n",
        "        logits: Tensor de salida lineal de dimensiones (batch_size, n_classes).\n",
        "        probabilities: Tensor de salida con activacion softmax.\n",
        "    \"\"\"\n",
        "\n",
        "    layer = inputs\n",
        "    n_layers = len(layer_sizes)   \n",
        "    # Capas neuronales\n",
        "    for i in range(n_layers):\n",
        "        with tf.variable_scope('layer_'+str(i)):          \n",
        "            previous_size = layer.shape[1].value  # Tamaño de entrada a la capa          \n",
        "            # Pesos de la capa oculta i-esima\n",
        "            weights = tf.get_variable(\n",
        "                name='weights_'+str(i),\n",
        "                shape=[previous_size, layer_sizes[i]],\n",
        "                initializer=tf.glorot_uniform_initializer())\n",
        "            # Summary de la distribucion de los pesos\n",
        "            tf.summary.histogram('weights_'+str(i), weights)\n",
        "\n",
        "            # Sesgos de la capa oculta i-esima\n",
        "            biases = tf.get_variable(\n",
        "                name='biases_'+str(i),\n",
        "                shape=[layer_sizes[i]],\n",
        "                initializer=tf.zeros_initializer())\n",
        "            # Summary de la distribucion de los sesgos\n",
        "            tf.summary.histogram('biases_'+str(i), biases)\n",
        "            # Aplicacion de pesos y sesgos\n",
        "            layer = tf.matmul(layer, weights) + biases          \n",
        "            if i < n_layers - 1:\n",
        "                # Aplicacion de funcion de activacion de capa oculta\n",
        "                layer = tf.nn.sigmoid(layer)\n",
        "            else:\n",
        "                # Aplicacion de funcion de activacion de capa de salida\n",
        "                logits = layer\n",
        "                probabilities = tf.nn.softmax(logits)   \n",
        "    return logits, probabilities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7Bip-9FV1_WJ"
      },
      "source": [
        "### Función de costo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CjgsGFG31-no",
        "colab": {}
      },
      "source": [
        "def loss_fn(logits, labels, loss_function_name):\n",
        "    \"\"\"Construye el grafo computacional del calculo de la funcion de costo.\n",
        "    \n",
        "    Se aplica el loss 'loss_function_name' entre los labels reales y la salida\n",
        "    de la MLP. Ademas, se calcula el accuracy.\n",
        "    Summaries son agregados para su visualizacion en Tensorboard.\n",
        "    \n",
        "    Args:\n",
        "        logits: Tensor de dimensiones (batch_size, n_classes) con los logits\n",
        "        de la salida de la MLP\n",
        "        labels: Tensor de dimensiones (batch_size,) con las etiquetas reales.\n",
        "        loss_function_name: 'cross_entropy' o 'mse', selecciona el costo.\n",
        "    \n",
        "    Returns:\n",
        "        loss: Tensor escalar que corresponde al costo calculado.\n",
        "        accuracy: Tensor escalar que corresponde al accuracy calculado.\n",
        "        val_summaries: summaries que son de interes al predecir en la validacion\n",
        "    \"\"\"\n",
        "    \n",
        "    # Codificacion 'one hot' para las etiquetas de clase\n",
        "    n_classes = logits.shape[1].value\n",
        "    one_hot_labels = tf.one_hot(labels, n_classes)\n",
        "    val_summaries = []\n",
        "    with tf.variable_scope('loss'):\n",
        "        if loss_function_name == 'cross_entropy':\n",
        "            # Cross Entropy loss\n",
        "            loss = tf.reduce_mean(\n",
        "                tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "                    logits=logits,\n",
        "                    labels=one_hot_labels),\n",
        "                name='xentropy'\n",
        "            )\n",
        "            # Summary de loss\n",
        "            loss_sum = tf.summary.scalar('xentropy_loss', loss)\n",
        "        elif loss_function_name == 'mse':\n",
        "            # Mean Squared Error loss\n",
        "            probabilities = tf.nn.softmax(logits)\n",
        "            loss = tf.reduce_mean(\n",
        "                tf.square(one_hot_labels - probabilities),\n",
        "                name='mse'\n",
        "            )\n",
        "            # Summary de loss\n",
        "            loss_sum = tf.summary.scalar('mse_loss', loss)\n",
        "        else:\n",
        "            raise ValueError('Wrong value for loss_function_name')\n",
        "    val_summaries.append(loss_sum)\n",
        "    with tf.variable_scope('accuracy'):\n",
        "        predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "        correct_predictions = tf.equal(labels, predictions)\n",
        "        accuracy = tf.reduce_mean(\n",
        "            tf.cast(correct_predictions, tf.float32),\n",
        "            name='accuracy')\n",
        "        # Summary de accuracy\n",
        "        acc_sum = tf.summary.scalar('accuracy', accuracy)\n",
        "    val_summaries.append(acc_sum)\n",
        "    return loss, accuracy, val_summaries"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WXb1NDd96-4r"
      },
      "source": [
        "### Función del optimizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cnngKAFgv7kZ",
        "colab": {}
      },
      "source": [
        "def optimizer_fn(loss, learning_rate):\n",
        "    \"\"\"Construye el grafo computacional de la actualizacion por gradiente.\n",
        "    \n",
        "    Se aplica el algoritmo de optimizacion 'sgd' para ejecutar una\n",
        "    iteracion de minimizacion por gradiente sobre el loss entregado.\n",
        "    \n",
        "    Args:\n",
        "        loss: Tensor escalar que corresponde al costo calculado.\n",
        "        learning_rate: Escalar que indica la tasa de aprendizaje. Al seleccionar\n",
        "            Adam este parametro es ignorado.\n",
        "        optimizer_name: 'sgd' o 'adam', selecciona el optimizador.\n",
        "    \n",
        "    Returns:\n",
        "        train_step: Operacion que ejecuta una iteracion de gradiente.\n",
        "    \"\"\"\n",
        "    \n",
        "    with tf.variable_scope('optimizer'):\n",
        "        # Gradient Descent\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "        # Minimizacion de la funcion de costo con el optimizador elegido\n",
        "        train_step = optimizer.minimize(loss)\n",
        "    return train_step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Mx8DACFS8TxR"
      },
      "source": [
        "### Clase de Clasificador MLP\n",
        "Esta clase utiliza las funciones anteriores para la construcción del grafo computacional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fITl9UaSydAf",
        "colab": {}
      },
      "source": [
        "class MLPClassifier(object):\n",
        "    \"\"\"Implementacion de clasificador Perceptron Multicapa.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        n_features,\n",
        "        layer_sizes,\n",
        "        loss_function_name='cross_entropy',\n",
        "        learning_rate=0.1,\n",
        "        batch_size=32,\n",
        "        max_epochs=100,\n",
        "        early_stopping=None,\n",
        "        logdir='logs'\n",
        "    ):\n",
        "        \"\"\"Construye un clasificador Perceptron Multicapa.\n",
        "        \n",
        "        Args:\n",
        "            n_features: Entero que indica el numero de caracteristicas de las entradas.\n",
        "            layer_sizes: Lista de enteros que indica el tamaño de cada capa de\n",
        "            neuronas. La salida de la capa i-esima posee dimensiones\n",
        "            (batch_size, layer_sizes[i]). El ultimo numero de la lista indica el\n",
        "            tamaño de la capa de salida, que debe ser igual al numero de clases.\n",
        "            loss_function_name: 'cross_entropy' o 'mse', selecciona el costo.\n",
        "                Por defecto es 'cross_entropy'.\n",
        "            learning_rate: Escalar que indica la tasa de aprendizaje. Al\n",
        "                seleccionar Adam este parametro es ignorado. Por defecto es 0.1\n",
        "            batch_size: Entero que indica el tamaño de los mini-batches para\n",
        "                el entrenamiento de la red.\n",
        "            max_epochs: Entero que indica el maximo numero de epocas de\n",
        "                entrenamiento (pasadas completas por los datos de entrada) \n",
        "            early_stopping: Indica cuantas veces las verificaciones en la\n",
        "                validacion deben indicar que el costo esta aumentando para\n",
        "                realizar una detencion temprana. Por defecto es None, lo cual\n",
        "                desactiva la detencion temprana.\n",
        "            logdir: String que indica el directorio en donde guardar los\n",
        "                archivos del entrenamiento. Por defecto es 'logs'.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Limpiar grafo computacional\n",
        "        tf.reset_default_graph()\n",
        "        # Agregar parametros al objeto\n",
        "        self.n_features = n_features\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.loss_function_name = loss_function_name\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.max_epochs = max_epochs\n",
        "        self.early_stopping = early_stopping\n",
        "        self.logdir = logdir\n",
        "        # Tensor que reserva espacio para las imagenes de entrada a la red\n",
        "        self.inputs_ph = tf.placeholder(tf.float32, shape=[None, n_features],\n",
        "                                   name='image_placeholder')\n",
        "        # Tensor que reserva espacio para las etiquetas de la entrada\n",
        "        self.labels_ph = tf.placeholder(tf.int32, shape=None,\n",
        "                                        name='label_placeholder')\n",
        "        # Construccion del grafo computacional\n",
        "        self.logits, self.proba = model_fn(\n",
        "            self.inputs_ph, layer_sizes)\n",
        "        self.loss, self.accuracy, self.val_summ = loss_fn(\n",
        "            self.logits, self.labels_ph, loss_function_name)\n",
        "        self.train_step = optimizer_fn(\n",
        "            self.loss, learning_rate)\n",
        "        # Fusion de todos los summaries\n",
        "        self.summ = tf.summary.merge_all()\n",
        "        self.val_summ = tf.summary.merge(self.val_summ)\n",
        "        # Crear sesion de tensorflow para administrar grafo\n",
        "        self.sess = tf.Session()\n",
        "        \n",
        "    def fit(self, X_train, y_train, X_val, y_val):\n",
        "        \"\"\"Entrenamiento del clasificador con los hiperparametros escogidos.\n",
        "        \n",
        "        Args:\n",
        "            X_train: Entradas del entrenamiento con dimensiones (n_ejemplos, n_features).\n",
        "            y_train: Etiquetas del entrenamiento con dimensiones (n_ejemplos,)\n",
        "            X_val: Entradas de la validacion con dimensiones (n_ejemplos, n_features).\n",
        "            y_val: Etiquetas de la validacion con dimensiones (n_ejemplos,)\n",
        "            \n",
        "        Returns:\n",
        "            train_stats: Diccionario con datos historicos del entrenamiento.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Creacion de 'writers' que guardan datos para Tensorboard\n",
        "        writer_train = tf.summary.FileWriter(self.logdir + '/train')\n",
        "        writer_val = tf.summary.FileWriter(self.logdir + '/val')\n",
        "        writer_train.add_graph(self.sess.graph)\n",
        "        print('\\n\\n[Beginning training of MLP at logdir \"%s\"]\\n' % (self.logdir,))    \n",
        "        # Inicializacion de todas las variables\n",
        "        self.sess.run(tf.global_variables_initializer())     \n",
        "        # Definicion de variables utiles para el entrenamiento\n",
        "        n_batches = int(X_train.shape[0] / self.batch_size)\n",
        "        prev_validation_loss = 100.0\n",
        "        validation_period = 10\n",
        "        early_stop_flag = False\n",
        "        start_time = time.time()\n",
        "        iteration_history = []\n",
        "        train_loss_history = []\n",
        "        train_acc_history = []\n",
        "        val_loss_history = []\n",
        "        val_acc_history = []\n",
        "        \n",
        "        # Ciclo que recorre una epoca completa de los datos cada vez\n",
        "        for epoch in range(self.max_epochs):\n",
        "            if early_stop_flag:\n",
        "                # Si early stopping se activo, detener el entrenamiento\n",
        "                break\n",
        "            # Para cada nueva epoca, hacer un shuffle al set de train\n",
        "            new_indexes = np.random.permutation(X_train.shape[0])\n",
        "            X_train = X_train[new_indexes, :]\n",
        "            y_train = y_train[new_indexes]\n",
        "            \n",
        "            # Ciclo que recorre los mini batches del set de train\n",
        "            for i in range(n_batches):\n",
        "                if early_stop_flag:\n",
        "                    # Si early stopping se activo, detener el entrenamiento\n",
        "                    break  \n",
        "                iteration = epoch * n_batches + i           \n",
        "                # Obtencion del minibatch actual\n",
        "                start = i * self.batch_size\n",
        "                end = (i+1) * self.batch_size\n",
        "                X_batch = X_train[start:end, :]\n",
        "                y_batch = y_train[start:end]          \n",
        "                # Ejecutar una iteracion de gradiente\n",
        "                feed_dict = {self.inputs_ph: X_batch, self.labels_ph: y_batch}\n",
        "                self.sess.run(self.train_step, feed_dict=feed_dict)\n",
        "                # Obtener estadisticas del entrenamiento\n",
        "                if iteration % validation_period == 0:\n",
        "                    iteration_history.append(iteration)\n",
        "                    # Estadisticas en el set de validacion\n",
        "                    feed_dict = {self.inputs_ph: X_val, self.labels_ph: y_val}\n",
        "                    val_loss, val_acc, val_summ = self.sess.run(\n",
        "                        [self.loss, self.accuracy, self.val_summ],\n",
        "                        feed_dict=feed_dict)\n",
        "                    writer_val.add_summary(val_summ, iteration)\n",
        "                    val_loss_history.append(val_loss)\n",
        "                    val_acc_history.append(val_acc)\n",
        "                    # Estadisticas en el set de entrenamiento\n",
        "                    feed_dict = {self.inputs_ph: X_train, self.labels_ph: y_train}\n",
        "                    train_loss, train_acc, train_summ = self.sess.run(\n",
        "                        [self.loss, self.accuracy, self.summ],\n",
        "                        feed_dict=feed_dict)\n",
        "                    writer_train.add_summary(train_summ, iteration)\n",
        "                    train_loss_history.append(train_loss)\n",
        "                    train_acc_history.append(train_acc)\n",
        "                    \n",
        "                    print('Epoch: %d/%d, iter: %d. ' %\n",
        "                          (epoch+1, self.max_epochs, iteration), end='')\n",
        "                    print('Loss (train/val): %.3f / %.3f. Val. acc: %.1f%%' %\n",
        "                          (train_loss, val_loss, val_acc * 100), end='')\n",
        "                    \n",
        "                    # Chequear condicion de early_stopping\n",
        "                    if self.early_stopping is not None:\n",
        "                        if val_loss > prev_validation_loss:\n",
        "                            validation_checks += 1\n",
        "                        else:\n",
        "                            validation_checks = 0\n",
        "                            prev_validation_loss = val_loss\n",
        "                        print(', Val. checks: %d/%d' %\n",
        "                              (validation_checks, self.early_stopping))\n",
        "                        if validation_checks >= self.early_stopping:\n",
        "                            early_stop_flag = True\n",
        "                            print('Early stopping')\n",
        "                    else:\n",
        "                        print('')\n",
        "            elap_time = time.time()-start_time\n",
        "            print(\"Epoch finished. Elapsed time %1.4f [s]\\n\" % (elap_time,))\n",
        "        writer_train.flush()\n",
        "        writer_val.flush()\n",
        "        # Guardar estadisticas en un diccionario\n",
        "        train_stats = {\n",
        "            'iteration_history': np.array(iteration_history),\n",
        "            'train_loss_history': np.array(train_loss_history),\n",
        "            'train_acc_history': np.array(train_acc_history),\n",
        "            'val_loss_history': np.array(val_loss_history),\n",
        "            'val_acc_history': np.array(val_acc_history)\n",
        "        }\n",
        "        return train_stats\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Retorna las probabilidades de clase para los datos de entrada.\n",
        "        \"\"\"\n",
        "        # Obtener las probabilidades de salida de cada clase\n",
        "        predicted_proba = self.sess.run(self.proba, feed_dict={self.inputs_ph: X})\n",
        "        return predicted_proba\n",
        "    \n",
        "    def predict_label(self, X):\n",
        "        \"\"\"Retorna la etiqueta predicha para los datos de entrada.\n",
        "        \"\"\"\n",
        "        # Obtener la probabilidad de cada clase\n",
        "        predicted_proba = self.predict_proba(X)\n",
        "        # Etiquetar segun la etiqueta mas probable\n",
        "        predicted_labels = np.argmax(predicted_proba, axis=1)\n",
        "        return predicted_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5HUxx8YiRbDl"
      },
      "source": [
        "## Entrenamiento de MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aYshAaUvRaZa",
        "colab": {}
      },
      "source": [
        "# ----- Directorio para logs\n",
        "experiment_name = \"exp_xentropy\"\n",
        "\n",
        "# --- NO TOCAR\n",
        "logdir_father = \"./tarea_1_logs/\"\n",
        "logdir = logdir_father + experiment_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GWrYVCRWpAXw",
        "colab": {}
      },
      "source": [
        "# ----- Ejecutar si es que se quiere un directorio limpio en logdir\n",
        "# Si existe el directorio, ejecutar esta celda movera los archivos a otra\n",
        "# carpeta para limpiar el logdir elegido.\n",
        "\n",
        "%%bash -s \"$logdir\"\n",
        "if [ -d $1 ]; then\n",
        "    this_date=\"$(date \"+%H%M%S-%y%m%d\")\"\n",
        "    new_dir=\"$1_old_$this_date\"\n",
        "    echo \"moving files from $1 to $new_dir\"\n",
        "    mv $1 $new_dir\n",
        "fi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xmNz6vshn1q2",
        "outputId": "d380d0f7-2c92-418c-ff9b-d1beef866987",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Aqui se modifican parametros\n",
        "\n",
        "run_n_times = 5\n",
        "stats_history = []\n",
        "stats_history_ce = []\n",
        "stats_history_mse = []\n",
        "\n",
        "\n",
        "for run in range(run_n_times):\n",
        "    # ----- Creacion de MLP\n",
        "    mlp = MLPClassifier(\n",
        "        n_features=28*28,\n",
        "        layer_sizes=[25, 2],\n",
        "        loss_function_name='cross_entropy',\n",
        "        learning_rate=0.1,\n",
        "        batch_size=32,\n",
        "        max_epochs=100,\n",
        "        early_stopping=15,\n",
        "        logdir=logdir+'run_%d' % run)\n",
        "\n",
        "    # ----- Entrenamiento de MLP\n",
        "    train_stats = mlp.fit(training_images, training_labels, validation_images, validation_labels)\n",
        "    stats_history.append(train_stats)\n",
        "    \n",
        "    train_stats_ce = train_stats\n",
        "    stats_history_ce.append(train_stats)\n",
        "    \n",
        "    #train_stats_mse = train_stats\n",
        "    #stats_history_mse.append(train_stats)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "[Beginning training of MLP at logdir \"./tarea_1_logs/exp_xentropyrun_0\"]\n",
            "\n",
            "Epoch: 1/100, iter: 0. Loss (train/val): 0.673 / 0.684. Val. acc: 51.4%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 10. Loss (train/val): 0.591 / 0.601. Val. acc: 79.1%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 20. Loss (train/val): 0.548 / 0.557. Val. acc: 78.8%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 30. Loss (train/val): 0.501 / 0.511. Val. acc: 83.5%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 40. Loss (train/val): 0.466 / 0.476. Val. acc: 84.7%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 50. Loss (train/val): 0.440 / 0.451. Val. acc: 84.1%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 60. Loss (train/val): 0.427 / 0.431. Val. acc: 83.8%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 70. Loss (train/val): 0.393 / 0.399. Val. acc: 85.5%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 80. Loss (train/val): 0.376 / 0.383. Val. acc: 86.1%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 90. Loss (train/val): 0.370 / 0.374. Val. acc: 86.3%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 100. Loss (train/val): 0.352 / 0.360. Val. acc: 86.5%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 110. Loss (train/val): 0.343 / 0.349. Val. acc: 87.0%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 120. Loss (train/val): 0.333 / 0.339. Val. acc: 87.3%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 130. Loss (train/val): 0.325 / 0.332. Val. acc: 87.1%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 140. Loss (train/val): 0.319 / 0.324. Val. acc: 87.0%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 150. Loss (train/val): 0.318 / 0.324. Val. acc: 87.4%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 160. Loss (train/val): 0.308 / 0.309. Val. acc: 87.9%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 170. Loss (train/val): 0.307 / 0.311. Val. acc: 87.3%, Val. checks: 1/15\n",
            "Epoch: 1/100, iter: 180. Loss (train/val): 0.300 / 0.302. Val. acc: 88.2%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 190. Loss (train/val): 0.297 / 0.299. Val. acc: 88.2%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 200. Loss (train/val): 0.305 / 0.300. Val. acc: 87.9%, Val. checks: 1/15\n",
            "Epoch: 1/100, iter: 210. Loss (train/val): 0.290 / 0.289. Val. acc: 88.5%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 220. Loss (train/val): 0.294 / 0.295. Val. acc: 88.0%, Val. checks: 1/15\n",
            "Epoch: 1/100, iter: 230. Loss (train/val): 0.292 / 0.285. Val. acc: 89.3%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 240. Loss (train/val): 0.281 / 0.276. Val. acc: 89.2%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 250. Loss (train/val): 0.279 / 0.273. Val. acc: 89.3%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 260. Loss (train/val): 0.292 / 0.281. Val. acc: 88.6%, Val. checks: 1/15\n",
            "Epoch: 1/100, iter: 270. Loss (train/val): 0.274 / 0.268. Val. acc: 89.4%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 280. Loss (train/val): 0.272 / 0.269. Val. acc: 89.4%, Val. checks: 1/15\n",
            "Epoch: 1/100, iter: 290. Loss (train/val): 0.269 / 0.262. Val. acc: 89.6%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 300. Loss (train/val): 0.271 / 0.261. Val. acc: 89.9%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 310. Loss (train/val): 0.266 / 0.260. Val. acc: 89.6%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 320. Loss (train/val): 0.266 / 0.255. Val. acc: 89.7%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 330. Loss (train/val): 0.263 / 0.255. Val. acc: 89.6%, Val. checks: 0/15\n",
            "Epoch finished. Elapsed time 0.8790 [s]\n",
            "\n",
            "Epoch: 2/100, iter: 340. Loss (train/val): 0.261 / 0.251. Val. acc: 89.7%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 350. Loss (train/val): 0.259 / 0.251. Val. acc: 89.6%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 360. Loss (train/val): 0.262 / 0.252. Val. acc: 90.4%, Val. checks: 2/15\n",
            "Epoch: 2/100, iter: 370. Loss (train/val): 0.257 / 0.248. Val. acc: 89.8%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 380. Loss (train/val): 0.271 / 0.265. Val. acc: 88.7%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 390. Loss (train/val): 0.261 / 0.245. Val. acc: 90.8%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 400. Loss (train/val): 0.257 / 0.250. Val. acc: 90.3%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 410. Loss (train/val): 0.262 / 0.250. Val. acc: 90.0%, Val. checks: 2/15\n",
            "Epoch: 2/100, iter: 420. Loss (train/val): 0.261 / 0.247. Val. acc: 90.7%, Val. checks: 3/15\n",
            "Epoch: 2/100, iter: 430. Loss (train/val): 0.257 / 0.242. Val. acc: 90.8%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 440. Loss (train/val): 0.285 / 0.266. Val. acc: 90.6%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 450. Loss (train/val): 0.267 / 0.260. Val. acc: 88.6%, Val. checks: 2/15\n",
            "Epoch: 2/100, iter: 460. Loss (train/val): 0.255 / 0.241. Val. acc: 91.1%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 470. Loss (train/val): 0.252 / 0.242. Val. acc: 89.6%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 480. Loss (train/val): 0.251 / 0.242. Val. acc: 89.4%, Val. checks: 2/15\n",
            "Epoch: 2/100, iter: 490. Loss (train/val): 0.251 / 0.238. Val. acc: 90.8%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 500. Loss (train/val): 0.246 / 0.236. Val. acc: 90.0%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 510. Loss (train/val): 0.248 / 0.236. Val. acc: 90.9%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 520. Loss (train/val): 0.244 / 0.236. Val. acc: 90.3%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 530. Loss (train/val): 0.257 / 0.248. Val. acc: 89.1%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 540. Loss (train/val): 0.244 / 0.234. Val. acc: 89.7%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 550. Loss (train/val): 0.246 / 0.235. Val. acc: 89.6%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 560. Loss (train/val): 0.243 / 0.232. Val. acc: 90.0%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 570. Loss (train/val): 0.256 / 0.245. Val. acc: 89.2%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 580. Loss (train/val): 0.241 / 0.229. Val. acc: 90.9%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 590. Loss (train/val): 0.245 / 0.232. Val. acc: 91.3%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 600. Loss (train/val): 0.253 / 0.239. Val. acc: 92.0%, Val. checks: 2/15\n",
            "Epoch: 2/100, iter: 610. Loss (train/val): 0.268 / 0.251. Val. acc: 91.3%, Val. checks: 3/15\n",
            "Epoch: 2/100, iter: 620. Loss (train/val): 0.241 / 0.232. Val. acc: 89.7%, Val. checks: 4/15\n",
            "Epoch: 2/100, iter: 630. Loss (train/val): 0.239 / 0.232. Val. acc: 89.6%, Val. checks: 5/15\n",
            "Epoch: 2/100, iter: 640. Loss (train/val): 0.237 / 0.228. Val. acc: 90.4%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 650. Loss (train/val): 0.241 / 0.232. Val. acc: 89.9%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 660. Loss (train/val): 0.237 / 0.229. Val. acc: 91.3%, Val. checks: 2/15\n",
            "Epoch: 2/100, iter: 670. Loss (train/val): 0.236 / 0.231. Val. acc: 90.4%, Val. checks: 3/15\n",
            "Epoch finished. Elapsed time 1.7097 [s]\n",
            "\n",
            "Epoch: 3/100, iter: 680. Loss (train/val): 0.237 / 0.226. Val. acc: 92.1%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 690. Loss (train/val): 0.236 / 0.227. Val. acc: 90.6%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 700. Loss (train/val): 0.238 / 0.228. Val. acc: 90.3%, Val. checks: 2/15\n",
            "Epoch: 3/100, iter: 710. Loss (train/val): 0.234 / 0.222. Val. acc: 91.5%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 720. Loss (train/val): 0.233 / 0.220. Val. acc: 91.8%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 730. Loss (train/val): 0.234 / 0.223. Val. acc: 90.6%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 740. Loss (train/val): 0.234 / 0.220. Val. acc: 91.2%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 750. Loss (train/val): 0.234 / 0.221. Val. acc: 91.8%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 760. Loss (train/val): 0.231 / 0.218. Val. acc: 91.9%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 770. Loss (train/val): 0.229 / 0.219. Val. acc: 90.7%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 780. Loss (train/val): 0.228 / 0.219. Val. acc: 90.4%, Val. checks: 2/15\n",
            "Epoch: 3/100, iter: 790. Loss (train/val): 0.229 / 0.216. Val. acc: 92.0%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 800. Loss (train/val): 0.229 / 0.218. Val. acc: 91.7%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 810. Loss (train/val): 0.226 / 0.218. Val. acc: 90.9%, Val. checks: 2/15\n",
            "Epoch: 3/100, iter: 820. Loss (train/val): 0.228 / 0.221. Val. acc: 91.1%, Val. checks: 3/15\n",
            "Epoch: 3/100, iter: 830. Loss (train/val): 0.257 / 0.243. Val. acc: 92.0%, Val. checks: 4/15\n",
            "Epoch: 3/100, iter: 840. Loss (train/val): 0.224 / 0.216. Val. acc: 91.1%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 850. Loss (train/val): 0.238 / 0.232. Val. acc: 89.5%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 860. Loss (train/val): 0.224 / 0.217. Val. acc: 91.0%, Val. checks: 2/15\n",
            "Epoch: 3/100, iter: 870. Loss (train/val): 0.227 / 0.219. Val. acc: 91.9%, Val. checks: 3/15\n",
            "Epoch: 3/100, iter: 880. Loss (train/val): 0.225 / 0.220. Val. acc: 90.7%, Val. checks: 4/15\n",
            "Epoch: 3/100, iter: 890. Loss (train/val): 0.223 / 0.218. Val. acc: 90.8%, Val. checks: 5/15\n",
            "Epoch: 3/100, iter: 900. Loss (train/val): 0.225 / 0.217. Val. acc: 92.0%, Val. checks: 6/15\n",
            "Epoch: 3/100, iter: 910. Loss (train/val): 0.220 / 0.212. Val. acc: 91.5%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 920. Loss (train/val): 0.219 / 0.211. Val. acc: 91.7%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 930. Loss (train/val): 0.218 / 0.212. Val. acc: 91.6%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 940. Loss (train/val): 0.218 / 0.211. Val. acc: 91.9%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 950. Loss (train/val): 0.217 / 0.210. Val. acc: 91.6%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 960. Loss (train/val): 0.226 / 0.222. Val. acc: 90.0%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 970. Loss (train/val): 0.217 / 0.212. Val. acc: 91.1%, Val. checks: 2/15\n",
            "Epoch: 3/100, iter: 980. Loss (train/val): 0.233 / 0.226. Val. acc: 92.2%, Val. checks: 3/15\n",
            "Epoch: 3/100, iter: 990. Loss (train/val): 0.216 / 0.212. Val. acc: 91.5%, Val. checks: 4/15\n",
            "Epoch: 3/100, iter: 1000. Loss (train/val): 0.214 / 0.209. Val. acc: 91.7%, Val. checks: 0/15\n",
            "Epoch finished. Elapsed time 2.5777 [s]\n",
            "\n",
            "Epoch: 4/100, iter: 1010. Loss (train/val): 0.216 / 0.212. Val. acc: 90.8%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1020. Loss (train/val): 0.213 / 0.207. Val. acc: 92.0%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1030. Loss (train/val): 0.212 / 0.208. Val. acc: 91.3%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1040. Loss (train/val): 0.218 / 0.214. Val. acc: 90.6%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1050. Loss (train/val): 0.225 / 0.217. Val. acc: 92.2%, Val. checks: 3/15\n",
            "Epoch: 4/100, iter: 1060. Loss (train/val): 0.209 / 0.203. Val. acc: 91.9%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1070. Loss (train/val): 0.218 / 0.210. Val. acc: 92.4%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1080. Loss (train/val): 0.211 / 0.206. Val. acc: 91.6%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1090. Loss (train/val): 0.208 / 0.202. Val. acc: 92.0%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1100. Loss (train/val): 0.209 / 0.202. Val. acc: 92.0%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1110. Loss (train/val): 0.214 / 0.207. Val. acc: 91.6%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1120. Loss (train/val): 0.206 / 0.199. Val. acc: 92.1%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1130. Loss (train/val): 0.205 / 0.197. Val. acc: 92.0%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1140. Loss (train/val): 0.216 / 0.208. Val. acc: 91.5%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1150. Loss (train/val): 0.203 / 0.194. Val. acc: 92.6%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1160. Loss (train/val): 0.216 / 0.206. Val. acc: 92.7%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1170. Loss (train/val): 0.202 / 0.195. Val. acc: 92.1%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1180. Loss (train/val): 0.205 / 0.198. Val. acc: 92.5%, Val. checks: 3/15\n",
            "Epoch: 4/100, iter: 1190. Loss (train/val): 0.203 / 0.197. Val. acc: 91.9%, Val. checks: 4/15\n",
            "Epoch: 4/100, iter: 1200. Loss (train/val): 0.202 / 0.196. Val. acc: 91.8%, Val. checks: 5/15\n",
            "Epoch: 4/100, iter: 1210. Loss (train/val): 0.200 / 0.194. Val. acc: 92.3%, Val. checks: 6/15\n",
            "Epoch: 4/100, iter: 1220. Loss (train/val): 0.205 / 0.199. Val. acc: 93.0%, Val. checks: 7/15\n",
            "Epoch: 4/100, iter: 1230. Loss (train/val): 0.203 / 0.194. Val. acc: 91.6%, Val. checks: 8/15\n",
            "Epoch: 4/100, iter: 1240. Loss (train/val): 0.204 / 0.197. Val. acc: 91.8%, Val. checks: 9/15\n",
            "Epoch: 4/100, iter: 1250. Loss (train/val): 0.203 / 0.195. Val. acc: 93.1%, Val. checks: 10/15\n",
            "Epoch: 4/100, iter: 1260. Loss (train/val): 0.197 / 0.187. Val. acc: 92.7%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1270. Loss (train/val): 0.195 / 0.188. Val. acc: 92.4%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1280. Loss (train/val): 0.194 / 0.187. Val. acc: 92.7%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1290. Loss (train/val): 0.195 / 0.190. Val. acc: 91.9%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1300. Loss (train/val): 0.194 / 0.189. Val. acc: 92.6%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1310. Loss (train/val): 0.194 / 0.186. Val. acc: 92.7%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1320. Loss (train/val): 0.197 / 0.189. Val. acc: 93.2%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1330. Loss (train/val): 0.191 / 0.186. Val. acc: 92.6%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1340. Loss (train/val): 0.192 / 0.183. Val. acc: 92.9%, Val. checks: 0/15\n",
            "Epoch finished. Elapsed time 3.4533 [s]\n",
            "\n",
            "Epoch: 5/100, iter: 1350. Loss (train/val): 0.191 / 0.185. Val. acc: 93.0%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1360. Loss (train/val): 0.200 / 0.195. Val. acc: 92.1%, Val. checks: 2/15\n",
            "Epoch: 5/100, iter: 1370. Loss (train/val): 0.190 / 0.185. Val. acc: 92.3%, Val. checks: 3/15\n",
            "Epoch: 5/100, iter: 1380. Loss (train/val): 0.189 / 0.182. Val. acc: 93.3%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1390. Loss (train/val): 0.190 / 0.183. Val. acc: 92.9%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1400. Loss (train/val): 0.189 / 0.181. Val. acc: 93.3%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1410. Loss (train/val): 0.196 / 0.189. Val. acc: 93.4%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1420. Loss (train/val): 0.186 / 0.179. Val. acc: 93.0%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1430. Loss (train/val): 0.185 / 0.178. Val. acc: 92.7%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1440. Loss (train/val): 0.192 / 0.186. Val. acc: 92.3%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1450. Loss (train/val): 0.184 / 0.178. Val. acc: 93.2%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1460. Loss (train/val): 0.184 / 0.178. Val. acc: 93.2%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1470. Loss (train/val): 0.184 / 0.180. Val. acc: 92.9%, Val. checks: 2/15\n",
            "Epoch: 5/100, iter: 1480. Loss (train/val): 0.189 / 0.184. Val. acc: 92.7%, Val. checks: 3/15\n",
            "Epoch: 5/100, iter: 1490. Loss (train/val): 0.182 / 0.178. Val. acc: 92.7%, Val. checks: 4/15\n",
            "Epoch: 5/100, iter: 1500. Loss (train/val): 0.181 / 0.178. Val. acc: 93.0%, Val. checks: 5/15\n",
            "Epoch: 5/100, iter: 1510. Loss (train/val): 0.189 / 0.186. Val. acc: 93.2%, Val. checks: 6/15\n",
            "Epoch: 5/100, iter: 1520. Loss (train/val): 0.179 / 0.177. Val. acc: 92.7%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1530. Loss (train/val): 0.178 / 0.175. Val. acc: 93.0%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1540. Loss (train/val): 0.178 / 0.176. Val. acc: 93.1%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1550. Loss (train/val): 0.178 / 0.176. Val. acc: 92.7%, Val. checks: 2/15\n",
            "Epoch: 5/100, iter: 1560. Loss (train/val): 0.183 / 0.180. Val. acc: 93.3%, Val. checks: 3/15\n",
            "Epoch: 5/100, iter: 1570. Loss (train/val): 0.176 / 0.173. Val. acc: 93.5%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1580. Loss (train/val): 0.176 / 0.175. Val. acc: 93.1%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1590. Loss (train/val): 0.176 / 0.175. Val. acc: 93.0%, Val. checks: 2/15\n",
            "Epoch: 5/100, iter: 1600. Loss (train/val): 0.174 / 0.171. Val. acc: 93.2%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1610. Loss (train/val): 0.178 / 0.176. Val. acc: 93.4%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1620. Loss (train/val): 0.175 / 0.171. Val. acc: 93.5%, Val. checks: 2/15\n",
            "Epoch: 5/100, iter: 1630. Loss (train/val): 0.178 / 0.172. Val. acc: 93.1%, Val. checks: 3/15\n",
            "Epoch: 5/100, iter: 1640. Loss (train/val): 0.172 / 0.167. Val. acc: 93.3%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1650. Loss (train/val): 0.174 / 0.169. Val. acc: 93.9%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1660. Loss (train/val): 0.180 / 0.173. Val. acc: 92.7%, Val. checks: 2/15\n",
            "Epoch: 5/100, iter: 1670. Loss (train/val): 0.170 / 0.165. Val. acc: 93.4%, Val. checks: 0/15\n",
            "Epoch finished. Elapsed time 4.3132 [s]\n",
            "\n",
            "Epoch: 6/100, iter: 1680. Loss (train/val): 0.172 / 0.167. Val. acc: 93.6%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1690. Loss (train/val): 0.169 / 0.164. Val. acc: 93.3%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1700. Loss (train/val): 0.173 / 0.169. Val. acc: 93.7%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1710. Loss (train/val): 0.168 / 0.163. Val. acc: 93.1%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1720. Loss (train/val): 0.174 / 0.169. Val. acc: 93.0%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1730. Loss (train/val): 0.168 / 0.164. Val. acc: 93.4%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 1740. Loss (train/val): 0.166 / 0.160. Val. acc: 93.6%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1750. Loss (train/val): 0.166 / 0.159. Val. acc: 93.5%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1760. Loss (train/val): 0.166 / 0.162. Val. acc: 93.5%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1770. Loss (train/val): 0.165 / 0.162. Val. acc: 93.5%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 1780. Loss (train/val): 0.164 / 0.161. Val. acc: 93.5%, Val. checks: 3/15\n",
            "Epoch: 6/100, iter: 1790. Loss (train/val): 0.164 / 0.161. Val. acc: 93.2%, Val. checks: 4/15\n",
            "Epoch: 6/100, iter: 1800. Loss (train/val): 0.166 / 0.164. Val. acc: 93.8%, Val. checks: 5/15\n",
            "Epoch: 6/100, iter: 1810. Loss (train/val): 0.164 / 0.160. Val. acc: 93.3%, Val. checks: 6/15\n",
            "Epoch: 6/100, iter: 1820. Loss (train/val): 0.162 / 0.158. Val. acc: 93.2%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1830. Loss (train/val): 0.167 / 0.164. Val. acc: 93.3%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1840. Loss (train/val): 0.162 / 0.160. Val. acc: 93.3%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 1850. Loss (train/val): 0.161 / 0.157. Val. acc: 94.2%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1860. Loss (train/val): 0.160 / 0.156. Val. acc: 93.5%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1870. Loss (train/val): 0.161 / 0.159. Val. acc: 93.5%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1880. Loss (train/val): 0.159 / 0.155. Val. acc: 93.6%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1890. Loss (train/val): 0.159 / 0.155. Val. acc: 93.8%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1900. Loss (train/val): 0.158 / 0.155. Val. acc: 94.0%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1910. Loss (train/val): 0.160 / 0.158. Val. acc: 93.5%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 1920. Loss (train/val): 0.158 / 0.157. Val. acc: 93.7%, Val. checks: 3/15\n",
            "Epoch: 6/100, iter: 1930. Loss (train/val): 0.155 / 0.154. Val. acc: 93.7%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1940. Loss (train/val): 0.155 / 0.154. Val. acc: 93.9%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1950. Loss (train/val): 0.160 / 0.158. Val. acc: 94.2%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 1960. Loss (train/val): 0.154 / 0.152. Val. acc: 94.0%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1970. Loss (train/val): 0.157 / 0.155. Val. acc: 93.8%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1980. Loss (train/val): 0.154 / 0.151. Val. acc: 93.8%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1990. Loss (train/val): 0.175 / 0.173. Val. acc: 94.7%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 2000. Loss (train/val): 0.160 / 0.158. Val. acc: 93.3%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 2010. Loss (train/val): 0.151 / 0.149. Val. acc: 93.8%, Val. checks: 0/15\n",
            "Epoch finished. Elapsed time 5.1900 [s]\n",
            "\n",
            "Epoch: 7/100, iter: 2020. Loss (train/val): 0.161 / 0.157. Val. acc: 93.8%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2030. Loss (train/val): 0.156 / 0.153. Val. acc: 94.5%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2040. Loss (train/val): 0.150 / 0.148. Val. acc: 93.8%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2050. Loss (train/val): 0.153 / 0.151. Val. acc: 94.3%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2060. Loss (train/val): 0.150 / 0.147. Val. acc: 94.4%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2070. Loss (train/val): 0.151 / 0.149. Val. acc: 93.8%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2080. Loss (train/val): 0.151 / 0.147. Val. acc: 94.2%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2090. Loss (train/val): 0.147 / 0.143. Val. acc: 94.0%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2100. Loss (train/val): 0.147 / 0.144. Val. acc: 94.3%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2110. Loss (train/val): 0.148 / 0.145. Val. acc: 94.5%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2120. Loss (train/val): 0.146 / 0.144. Val. acc: 94.4%, Val. checks: 3/15\n",
            "Epoch: 7/100, iter: 2130. Loss (train/val): 0.147 / 0.143. Val. acc: 94.6%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2140. Loss (train/val): 0.146 / 0.143. Val. acc: 94.9%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2150. Loss (train/val): 0.145 / 0.142. Val. acc: 94.2%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2160. Loss (train/val): 0.144 / 0.141. Val. acc: 94.3%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2170. Loss (train/val): 0.145 / 0.142. Val. acc: 94.7%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2180. Loss (train/val): 0.144 / 0.142. Val. acc: 94.6%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2190. Loss (train/val): 0.144 / 0.142. Val. acc: 94.5%, Val. checks: 3/15\n",
            "Epoch: 7/100, iter: 2200. Loss (train/val): 0.145 / 0.142. Val. acc: 94.9%, Val. checks: 4/15\n",
            "Epoch: 7/100, iter: 2210. Loss (train/val): 0.142 / 0.139. Val. acc: 94.0%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2220. Loss (train/val): 0.142 / 0.141. Val. acc: 94.9%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2230. Loss (train/val): 0.141 / 0.140. Val. acc: 94.3%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2240. Loss (train/val): 0.143 / 0.142. Val. acc: 94.3%, Val. checks: 3/15\n",
            "Epoch: 7/100, iter: 2250. Loss (train/val): 0.143 / 0.142. Val. acc: 94.4%, Val. checks: 4/15\n",
            "Epoch: 7/100, iter: 2260. Loss (train/val): 0.141 / 0.139. Val. acc: 94.0%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2270. Loss (train/val): 0.143 / 0.141. Val. acc: 94.6%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2280. Loss (train/val): 0.140 / 0.138. Val. acc: 94.4%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2290. Loss (train/val): 0.139 / 0.137. Val. acc: 94.6%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2300. Loss (train/val): 0.139 / 0.136. Val. acc: 94.7%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2310. Loss (train/val): 0.148 / 0.146. Val. acc: 94.3%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2320. Loss (train/val): 0.139 / 0.139. Val. acc: 94.4%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2330. Loss (train/val): 0.139 / 0.138. Val. acc: 94.2%, Val. checks: 3/15\n",
            "Epoch: 7/100, iter: 2340. Loss (train/val): 0.147 / 0.146. Val. acc: 94.0%, Val. checks: 4/15\n",
            "Epoch: 7/100, iter: 2350. Loss (train/val): 0.139 / 0.141. Val. acc: 94.7%, Val. checks: 5/15\n",
            "Epoch finished. Elapsed time 6.0616 [s]\n",
            "\n",
            "Epoch: 8/100, iter: 2360. Loss (train/val): 0.135 / 0.136. Val. acc: 94.2%, Val. checks: 6/15\n",
            "Epoch: 8/100, iter: 2370. Loss (train/val): 0.138 / 0.138. Val. acc: 94.9%, Val. checks: 7/15\n",
            "Epoch: 8/100, iter: 2380. Loss (train/val): 0.134 / 0.132. Val. acc: 94.7%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2390. Loss (train/val): 0.134 / 0.132. Val. acc: 94.7%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2400. Loss (train/val): 0.134 / 0.132. Val. acc: 94.5%, Val. checks: 2/15\n",
            "Epoch: 8/100, iter: 2410. Loss (train/val): 0.134 / 0.133. Val. acc: 94.4%, Val. checks: 3/15\n",
            "Epoch: 8/100, iter: 2420. Loss (train/val): 0.137 / 0.139. Val. acc: 93.8%, Val. checks: 4/15\n",
            "Epoch: 8/100, iter: 2430. Loss (train/val): 0.133 / 0.134. Val. acc: 93.9%, Val. checks: 5/15\n",
            "Epoch: 8/100, iter: 2440. Loss (train/val): 0.136 / 0.136. Val. acc: 94.6%, Val. checks: 6/15\n",
            "Epoch: 8/100, iter: 2450. Loss (train/val): 0.133 / 0.131. Val. acc: 95.1%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2460. Loss (train/val): 0.134 / 0.133. Val. acc: 94.9%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2470. Loss (train/val): 0.130 / 0.131. Val. acc: 94.7%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2480. Loss (train/val): 0.131 / 0.132. Val. acc: 94.9%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2490. Loss (train/val): 0.131 / 0.132. Val. acc: 94.3%, Val. checks: 2/15\n",
            "Epoch: 8/100, iter: 2500. Loss (train/val): 0.131 / 0.133. Val. acc: 94.2%, Val. checks: 3/15\n",
            "Epoch: 8/100, iter: 2510. Loss (train/val): 0.130 / 0.133. Val. acc: 94.5%, Val. checks: 4/15\n",
            "Epoch: 8/100, iter: 2520. Loss (train/val): 0.130 / 0.132. Val. acc: 94.7%, Val. checks: 5/15\n",
            "Epoch: 8/100, iter: 2530. Loss (train/val): 0.129 / 0.130. Val. acc: 94.8%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2540. Loss (train/val): 0.128 / 0.129. Val. acc: 94.9%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2550. Loss (train/val): 0.129 / 0.131. Val. acc: 94.7%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2560. Loss (train/val): 0.129 / 0.128. Val. acc: 95.0%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2570. Loss (train/val): 0.128 / 0.129. Val. acc: 94.6%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2580. Loss (train/val): 0.126 / 0.128. Val. acc: 94.5%, Val. checks: 2/15\n",
            "Epoch: 8/100, iter: 2590. Loss (train/val): 0.127 / 0.130. Val. acc: 94.8%, Val. checks: 3/15\n",
            "Epoch: 8/100, iter: 2600. Loss (train/val): 0.131 / 0.130. Val. acc: 94.4%, Val. checks: 4/15\n",
            "Epoch: 8/100, iter: 2610. Loss (train/val): 0.126 / 0.126. Val. acc: 94.9%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2620. Loss (train/val): 0.127 / 0.127. Val. acc: 94.8%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2630. Loss (train/val): 0.141 / 0.142. Val. acc: 94.3%, Val. checks: 2/15\n",
            "Epoch: 8/100, iter: 2640. Loss (train/val): 0.125 / 0.126. Val. acc: 94.9%, Val. checks: 3/15\n",
            "Epoch: 8/100, iter: 2650. Loss (train/val): 0.126 / 0.128. Val. acc: 95.0%, Val. checks: 4/15\n",
            "Epoch: 8/100, iter: 2660. Loss (train/val): 0.126 / 0.127. Val. acc: 94.8%, Val. checks: 5/15\n",
            "Epoch: 8/100, iter: 2670. Loss (train/val): 0.123 / 0.123. Val. acc: 95.2%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2680. Loss (train/val): 0.122 / 0.124. Val. acc: 94.9%, Val. checks: 1/15\n",
            "Epoch finished. Elapsed time 6.9082 [s]\n",
            "\n",
            "Epoch: 9/100, iter: 2690. Loss (train/val): 0.124 / 0.126. Val. acc: 94.7%, Val. checks: 2/15\n",
            "Epoch: 9/100, iter: 2700. Loss (train/val): 0.124 / 0.125. Val. acc: 95.1%, Val. checks: 3/15\n",
            "Epoch: 9/100, iter: 2710. Loss (train/val): 0.121 / 0.123. Val. acc: 94.9%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2720. Loss (train/val): 0.123 / 0.124. Val. acc: 95.2%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2730. Loss (train/val): 0.124 / 0.125. Val. acc: 95.2%, Val. checks: 2/15\n",
            "Epoch: 9/100, iter: 2740. Loss (train/val): 0.122 / 0.122. Val. acc: 95.1%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2750. Loss (train/val): 0.121 / 0.121. Val. acc: 95.2%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2760. Loss (train/val): 0.120 / 0.121. Val. acc: 95.2%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2770. Loss (train/val): 0.123 / 0.126. Val. acc: 94.7%, Val. checks: 2/15\n",
            "Epoch: 9/100, iter: 2780. Loss (train/val): 0.122 / 0.124. Val. acc: 94.9%, Val. checks: 3/15\n",
            "Epoch: 9/100, iter: 2790. Loss (train/val): 0.129 / 0.132. Val. acc: 95.0%, Val. checks: 4/15\n",
            "Epoch: 9/100, iter: 2800. Loss (train/val): 0.120 / 0.123. Val. acc: 95.1%, Val. checks: 5/15\n",
            "Epoch: 9/100, iter: 2810. Loss (train/val): 0.123 / 0.126. Val. acc: 95.0%, Val. checks: 6/15\n",
            "Epoch: 9/100, iter: 2820. Loss (train/val): 0.123 / 0.127. Val. acc: 94.9%, Val. checks: 7/15\n",
            "Epoch: 9/100, iter: 2830. Loss (train/val): 0.122 / 0.127. Val. acc: 94.8%, Val. checks: 8/15\n",
            "Epoch: 9/100, iter: 2840. Loss (train/val): 0.120 / 0.124. Val. acc: 95.0%, Val. checks: 9/15\n",
            "Epoch: 9/100, iter: 2850. Loss (train/val): 0.117 / 0.121. Val. acc: 95.1%, Val. checks: 10/15\n",
            "Epoch: 9/100, iter: 2860. Loss (train/val): 0.117 / 0.121. Val. acc: 95.1%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2870. Loss (train/val): 0.124 / 0.129. Val. acc: 95.1%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2880. Loss (train/val): 0.116 / 0.118. Val. acc: 95.2%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2890. Loss (train/val): 0.120 / 0.124. Val. acc: 95.0%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2900. Loss (train/val): 0.115 / 0.118. Val. acc: 95.5%, Val. checks: 2/15\n",
            "Epoch: 9/100, iter: 2910. Loss (train/val): 0.115 / 0.121. Val. acc: 95.1%, Val. checks: 3/15\n",
            "Epoch: 9/100, iter: 2920. Loss (train/val): 0.119 / 0.125. Val. acc: 95.0%, Val. checks: 4/15\n",
            "Epoch: 9/100, iter: 2930. Loss (train/val): 0.114 / 0.119. Val. acc: 95.2%, Val. checks: 5/15\n",
            "Epoch: 9/100, iter: 2940. Loss (train/val): 0.115 / 0.119. Val. acc: 95.3%, Val. checks: 6/15\n",
            "Epoch: 9/100, iter: 2950. Loss (train/val): 0.113 / 0.118. Val. acc: 95.0%, Val. checks: 7/15\n",
            "Epoch: 9/100, iter: 2960. Loss (train/val): 0.113 / 0.119. Val. acc: 95.0%, Val. checks: 8/15\n",
            "Epoch: 9/100, iter: 2970. Loss (train/val): 0.113 / 0.117. Val. acc: 95.3%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2980. Loss (train/val): 0.114 / 0.118. Val. acc: 95.2%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2990. Loss (train/val): 0.112 / 0.117. Val. acc: 95.1%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 3000. Loss (train/val): 0.120 / 0.123. Val. acc: 95.3%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 3010. Loss (train/val): 0.112 / 0.115. Val. acc: 95.3%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 3020. Loss (train/val): 0.114 / 0.117. Val. acc: 95.5%, Val. checks: 1/15\n",
            "Epoch finished. Elapsed time 7.8623 [s]\n",
            "\n",
            "Epoch: 10/100, iter: 3030. Loss (train/val): 0.116 / 0.122. Val. acc: 95.2%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3040. Loss (train/val): 0.112 / 0.118. Val. acc: 95.0%, Val. checks: 3/15\n",
            "Epoch: 10/100, iter: 3050. Loss (train/val): 0.111 / 0.114. Val. acc: 95.6%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3060. Loss (train/val): 0.111 / 0.115. Val. acc: 95.6%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3070. Loss (train/val): 0.112 / 0.117. Val. acc: 95.6%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3080. Loss (train/val): 0.110 / 0.113. Val. acc: 95.5%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3090. Loss (train/val): 0.110 / 0.114. Val. acc: 95.2%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3100. Loss (train/val): 0.111 / 0.115. Val. acc: 95.3%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3110. Loss (train/val): 0.117 / 0.119. Val. acc: 95.6%, Val. checks: 3/15\n",
            "Epoch: 10/100, iter: 3120. Loss (train/val): 0.108 / 0.112. Val. acc: 95.3%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3130. Loss (train/val): 0.108 / 0.114. Val. acc: 95.1%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3140. Loss (train/val): 0.109 / 0.112. Val. acc: 95.8%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3150. Loss (train/val): 0.109 / 0.111. Val. acc: 95.7%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3160. Loss (train/val): 0.107 / 0.111. Val. acc: 95.5%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3170. Loss (train/val): 0.109 / 0.114. Val. acc: 95.3%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3180. Loss (train/val): 0.112 / 0.117. Val. acc: 95.7%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3190. Loss (train/val): 0.107 / 0.111. Val. acc: 95.3%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3200. Loss (train/val): 0.109 / 0.113. Val. acc: 95.7%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3210. Loss (train/val): 0.107 / 0.110. Val. acc: 95.5%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3220. Loss (train/val): 0.105 / 0.108. Val. acc: 95.6%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3230. Loss (train/val): 0.106 / 0.111. Val. acc: 95.6%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3240. Loss (train/val): 0.105 / 0.109. Val. acc: 95.7%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3250. Loss (train/val): 0.110 / 0.115. Val. acc: 95.7%, Val. checks: 3/15\n",
            "Epoch: 10/100, iter: 3260. Loss (train/val): 0.105 / 0.110. Val. acc: 95.5%, Val. checks: 4/15\n",
            "Epoch: 10/100, iter: 3270. Loss (train/val): 0.104 / 0.109. Val. acc: 95.7%, Val. checks: 5/15\n",
            "Epoch: 10/100, iter: 3280. Loss (train/val): 0.109 / 0.114. Val. acc: 95.5%, Val. checks: 6/15\n",
            "Epoch: 10/100, iter: 3290. Loss (train/val): 0.108 / 0.114. Val. acc: 95.5%, Val. checks: 7/15\n",
            "Epoch: 10/100, iter: 3300. Loss (train/val): 0.104 / 0.109. Val. acc: 95.7%, Val. checks: 8/15\n",
            "Epoch: 10/100, iter: 3310. Loss (train/val): 0.103 / 0.108. Val. acc: 95.9%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3320. Loss (train/val): 0.104 / 0.108. Val. acc: 96.0%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3330. Loss (train/val): 0.103 / 0.107. Val. acc: 96.0%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3340. Loss (train/val): 0.103 / 0.110. Val. acc: 95.5%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3350. Loss (train/val): 0.104 / 0.110. Val. acc: 95.9%, Val. checks: 2/15\n",
            "Epoch finished. Elapsed time 8.7061 [s]\n",
            "\n",
            "Epoch: 11/100, iter: 3360. Loss (train/val): 0.101 / 0.108. Val. acc: 96.1%, Val. checks: 3/15\n",
            "Epoch: 11/100, iter: 3370. Loss (train/val): 0.105 / 0.110. Val. acc: 95.7%, Val. checks: 4/15\n",
            "Epoch: 11/100, iter: 3380. Loss (train/val): 0.102 / 0.107. Val. acc: 96.0%, Val. checks: 5/15\n",
            "Epoch: 11/100, iter: 3390. Loss (train/val): 0.103 / 0.108. Val. acc: 95.7%, Val. checks: 6/15\n",
            "Epoch: 11/100, iter: 3400. Loss (train/val): 0.107 / 0.111. Val. acc: 95.7%, Val. checks: 7/15\n",
            "Epoch: 11/100, iter: 3410. Loss (train/val): 0.101 / 0.107. Val. acc: 95.9%, Val. checks: 8/15\n",
            "Epoch: 11/100, iter: 3420. Loss (train/val): 0.100 / 0.105. Val. acc: 96.1%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3430. Loss (train/val): 0.100 / 0.105. Val. acc: 96.2%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3440. Loss (train/val): 0.100 / 0.107. Val. acc: 96.2%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3450. Loss (train/val): 0.100 / 0.106. Val. acc: 95.8%, Val. checks: 3/15\n",
            "Epoch: 11/100, iter: 3460. Loss (train/val): 0.102 / 0.106. Val. acc: 95.9%, Val. checks: 4/15\n",
            "Epoch: 11/100, iter: 3470. Loss (train/val): 0.101 / 0.104. Val. acc: 96.2%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3480. Loss (train/val): 0.100 / 0.104. Val. acc: 96.1%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3490. Loss (train/val): 0.099 / 0.105. Val. acc: 96.1%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3500. Loss (train/val): 0.100 / 0.105. Val. acc: 95.8%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3510. Loss (train/val): 0.100 / 0.106. Val. acc: 95.9%, Val. checks: 3/15\n",
            "Epoch: 11/100, iter: 3520. Loss (train/val): 0.098 / 0.105. Val. acc: 96.1%, Val. checks: 4/15\n",
            "Epoch: 11/100, iter: 3530. Loss (train/val): 0.098 / 0.104. Val. acc: 96.2%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3540. Loss (train/val): 0.101 / 0.106. Val. acc: 95.8%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3550. Loss (train/val): 0.099 / 0.106. Val. acc: 96.2%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3560. Loss (train/val): 0.098 / 0.106. Val. acc: 96.1%, Val. checks: 3/15\n",
            "Epoch: 11/100, iter: 3570. Loss (train/val): 0.102 / 0.108. Val. acc: 96.2%, Val. checks: 4/15\n",
            "Epoch: 11/100, iter: 3580. Loss (train/val): 0.097 / 0.101. Val. acc: 96.0%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3590. Loss (train/val): 0.097 / 0.102. Val. acc: 96.2%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3600. Loss (train/val): 0.096 / 0.102. Val. acc: 96.2%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3610. Loss (train/val): 0.095 / 0.101. Val. acc: 96.1%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3620. Loss (train/val): 0.095 / 0.102. Val. acc: 96.2%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3630. Loss (train/val): 0.098 / 0.105. Val. acc: 96.1%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3640. Loss (train/val): 0.097 / 0.106. Val. acc: 96.0%, Val. checks: 3/15\n",
            "Epoch: 11/100, iter: 3650. Loss (train/val): 0.095 / 0.102. Val. acc: 96.1%, Val. checks: 4/15\n",
            "Epoch: 11/100, iter: 3660. Loss (train/val): 0.100 / 0.106. Val. acc: 95.9%, Val. checks: 5/15\n",
            "Epoch: 11/100, iter: 3670. Loss (train/val): 0.096 / 0.103. Val. acc: 96.1%, Val. checks: 6/15\n",
            "Epoch: 11/100, iter: 3680. Loss (train/val): 0.094 / 0.101. Val. acc: 96.2%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3690. Loss (train/val): 0.094 / 0.102. Val. acc: 96.1%, Val. checks: 1/15\n",
            "Epoch finished. Elapsed time 9.5724 [s]\n",
            "\n",
            "Epoch: 12/100, iter: 3700. Loss (train/val): 0.095 / 0.103. Val. acc: 96.1%, Val. checks: 2/15\n",
            "Epoch: 12/100, iter: 3710. Loss (train/val): 0.094 / 0.103. Val. acc: 96.0%, Val. checks: 3/15\n",
            "Epoch: 12/100, iter: 3720. Loss (train/val): 0.100 / 0.111. Val. acc: 96.1%, Val. checks: 4/15\n",
            "Epoch: 12/100, iter: 3730. Loss (train/val): 0.095 / 0.104. Val. acc: 96.3%, Val. checks: 5/15\n",
            "Epoch: 12/100, iter: 3740. Loss (train/val): 0.094 / 0.102. Val. acc: 96.0%, Val. checks: 6/15\n",
            "Epoch: 12/100, iter: 3750. Loss (train/val): 0.093 / 0.099. Val. acc: 96.2%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3760. Loss (train/val): 0.092 / 0.100. Val. acc: 96.2%, Val. checks: 1/15\n",
            "Epoch: 12/100, iter: 3770. Loss (train/val): 0.093 / 0.102. Val. acc: 95.8%, Val. checks: 2/15\n",
            "Epoch: 12/100, iter: 3780. Loss (train/val): 0.093 / 0.101. Val. acc: 96.2%, Val. checks: 3/15\n",
            "Epoch: 12/100, iter: 3790. Loss (train/val): 0.094 / 0.103. Val. acc: 95.8%, Val. checks: 4/15\n",
            "Epoch: 12/100, iter: 3800. Loss (train/val): 0.096 / 0.105. Val. acc: 96.2%, Val. checks: 5/15\n",
            "Epoch: 12/100, iter: 3810. Loss (train/val): 0.094 / 0.104. Val. acc: 96.0%, Val. checks: 6/15\n",
            "Epoch: 12/100, iter: 3820. Loss (train/val): 0.097 / 0.107. Val. acc: 95.6%, Val. checks: 7/15\n",
            "Epoch: 12/100, iter: 3830. Loss (train/val): 0.093 / 0.102. Val. acc: 96.2%, Val. checks: 8/15\n",
            "Epoch: 12/100, iter: 3840. Loss (train/val): 0.092 / 0.098. Val. acc: 96.1%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3850. Loss (train/val): 0.092 / 0.100. Val. acc: 96.2%, Val. checks: 1/15\n",
            "Epoch: 12/100, iter: 3860. Loss (train/val): 0.096 / 0.103. Val. acc: 96.0%, Val. checks: 2/15\n",
            "Epoch: 12/100, iter: 3870. Loss (train/val): 0.094 / 0.106. Val. acc: 96.2%, Val. checks: 3/15\n",
            "Epoch: 12/100, iter: 3880. Loss (train/val): 0.090 / 0.101. Val. acc: 96.3%, Val. checks: 4/15\n",
            "Epoch: 12/100, iter: 3890. Loss (train/val): 0.090 / 0.100. Val. acc: 96.4%, Val. checks: 5/15\n",
            "Epoch: 12/100, iter: 3900. Loss (train/val): 0.090 / 0.099. Val. acc: 96.4%, Val. checks: 6/15\n",
            "Epoch: 12/100, iter: 3910. Loss (train/val): 0.091 / 0.101. Val. acc: 96.2%, Val. checks: 7/15\n",
            "Epoch: 12/100, iter: 3920. Loss (train/val): 0.090 / 0.099. Val. acc: 96.1%, Val. checks: 8/15\n",
            "Epoch: 12/100, iter: 3930. Loss (train/val): 0.091 / 0.098. Val. acc: 96.3%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3940. Loss (train/val): 0.089 / 0.097. Val. acc: 96.2%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3950. Loss (train/val): 0.089 / 0.097. Val. acc: 96.1%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3960. Loss (train/val): 0.089 / 0.097. Val. acc: 96.3%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3970. Loss (train/val): 0.091 / 0.098. Val. acc: 96.3%, Val. checks: 1/15\n",
            "Epoch: 12/100, iter: 3980. Loss (train/val): 0.093 / 0.100. Val. acc: 96.0%, Val. checks: 2/15\n",
            "Epoch: 12/100, iter: 3990. Loss (train/val): 0.091 / 0.099. Val. acc: 96.3%, Val. checks: 3/15\n",
            "Epoch: 12/100, iter: 4000. Loss (train/val): 0.088 / 0.096. Val. acc: 96.3%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 4010. Loss (train/val): 0.088 / 0.097. Val. acc: 96.3%, Val. checks: 1/15\n",
            "Epoch: 12/100, iter: 4020. Loss (train/val): 0.088 / 0.097. Val. acc: 96.1%, Val. checks: 2/15\n",
            "Epoch: 12/100, iter: 4030. Loss (train/val): 0.087 / 0.096. Val. acc: 96.2%, Val. checks: 3/15\n",
            "Epoch finished. Elapsed time 10.4258 [s]\n",
            "\n",
            "Epoch: 13/100, iter: 4040. Loss (train/val): 0.088 / 0.094. Val. acc: 96.3%, Val. checks: 0/15\n",
            "Epoch: 13/100, iter: 4050. Loss (train/val): 0.091 / 0.100. Val. acc: 96.6%, Val. checks: 1/15\n",
            "Epoch: 13/100, iter: 4060. Loss (train/val): 0.087 / 0.094. Val. acc: 96.3%, Val. checks: 0/15\n",
            "Epoch: 13/100, iter: 4070. Loss (train/val): 0.087 / 0.093. Val. acc: 96.2%, Val. checks: 0/15\n",
            "Epoch: 13/100, iter: 4080. Loss (train/val): 0.093 / 0.099. Val. acc: 96.1%, Val. checks: 1/15\n",
            "Epoch: 13/100, iter: 4090. Loss (train/val): 0.087 / 0.095. Val. acc: 96.4%, Val. checks: 2/15\n",
            "Epoch: 13/100, iter: 4100. Loss (train/val): 0.086 / 0.093. Val. acc: 96.3%, Val. checks: 3/15\n",
            "Epoch: 13/100, iter: 4110. Loss (train/val): 0.089 / 0.096. Val. acc: 96.2%, Val. checks: 4/15\n",
            "Epoch: 13/100, iter: 4120. Loss (train/val): 0.086 / 0.094. Val. acc: 96.3%, Val. checks: 5/15\n",
            "Epoch: 13/100, iter: 4130. Loss (train/val): 0.091 / 0.100. Val. acc: 96.0%, Val. checks: 6/15\n",
            "Epoch: 13/100, iter: 4140. Loss (train/val): 0.086 / 0.094. Val. acc: 96.5%, Val. checks: 7/15\n",
            "Epoch: 13/100, iter: 4150. Loss (train/val): 0.088 / 0.097. Val. acc: 96.6%, Val. checks: 8/15\n",
            "Epoch: 13/100, iter: 4160. Loss (train/val): 0.086 / 0.093. Val. acc: 96.2%, Val. checks: 0/15\n",
            "Epoch: 13/100, iter: 4170. Loss (train/val): 0.085 / 0.094. Val. acc: 96.4%, Val. checks: 1/15\n",
            "Epoch: 13/100, iter: 4180. Loss (train/val): 0.085 / 0.095. Val. acc: 96.5%, Val. checks: 2/15\n",
            "Epoch: 13/100, iter: 4190. Loss (train/val): 0.085 / 0.095. Val. acc: 96.4%, Val. checks: 3/15\n",
            "Epoch: 13/100, iter: 4200. Loss (train/val): 0.084 / 0.093. Val. acc: 96.5%, Val. checks: 4/15\n",
            "Epoch: 13/100, iter: 4210. Loss (train/val): 0.084 / 0.093. Val. acc: 96.4%, Val. checks: 5/15\n",
            "Epoch: 13/100, iter: 4220. Loss (train/val): 0.084 / 0.094. Val. acc: 96.2%, Val. checks: 6/15\n",
            "Epoch: 13/100, iter: 4230. Loss (train/val): 0.086 / 0.096. Val. acc: 96.6%, Val. checks: 7/15\n",
            "Epoch: 13/100, iter: 4240. Loss (train/val): 0.084 / 0.093. Val. acc: 96.4%, Val. checks: 8/15\n",
            "Epoch: 13/100, iter: 4250. Loss (train/val): 0.085 / 0.093. Val. acc: 96.3%, Val. checks: 0/15\n",
            "Epoch: 13/100, iter: 4260. Loss (train/val): 0.088 / 0.095. Val. acc: 96.5%, Val. checks: 1/15\n",
            "Epoch: 13/100, iter: 4270. Loss (train/val): 0.083 / 0.093. Val. acc: 96.4%, Val. checks: 2/15\n",
            "Epoch: 13/100, iter: 4280. Loss (train/val): 0.085 / 0.097. Val. acc: 96.9%, Val. checks: 3/15\n",
            "Epoch: 13/100, iter: 4290. Loss (train/val): 0.084 / 0.096. Val. acc: 96.6%, Val. checks: 4/15\n",
            "Epoch: 13/100, iter: 4300. Loss (train/val): 0.084 / 0.096. Val. acc: 96.9%, Val. checks: 5/15\n",
            "Epoch: 13/100, iter: 4310. Loss (train/val): 0.084 / 0.095. Val. acc: 96.5%, Val. checks: 6/15\n",
            "Epoch: 13/100, iter: 4320. Loss (train/val): 0.083 / 0.094. Val. acc: 96.4%, Val. checks: 7/15\n",
            "Epoch: 13/100, iter: 4330. Loss (train/val): 0.086 / 0.096. Val. acc: 96.3%, Val. checks: 8/15\n",
            "Epoch: 13/100, iter: 4340. Loss (train/val): 0.083 / 0.095. Val. acc: 96.3%, Val. checks: 9/15\n",
            "Epoch: 13/100, iter: 4350. Loss (train/val): 0.082 / 0.092. Val. acc: 96.2%, Val. checks: 0/15\n",
            "Epoch: 13/100, iter: 4360. Loss (train/val): 0.089 / 0.102. Val. acc: 96.4%, Val. checks: 1/15\n",
            "Epoch finished. Elapsed time 11.2809 [s]\n",
            "\n",
            "Epoch: 14/100, iter: 4370. Loss (train/val): 0.081 / 0.093. Val. acc: 96.4%, Val. checks: 2/15\n",
            "Epoch: 14/100, iter: 4380. Loss (train/val): 0.082 / 0.095. Val. acc: 96.3%, Val. checks: 3/15\n",
            "Epoch: 14/100, iter: 4390. Loss (train/val): 0.082 / 0.094. Val. acc: 96.4%, Val. checks: 4/15\n",
            "Epoch: 14/100, iter: 4400. Loss (train/val): 0.082 / 0.092. Val. acc: 96.2%, Val. checks: 0/15\n",
            "Epoch: 14/100, iter: 4410. Loss (train/val): 0.082 / 0.092. Val. acc: 96.4%, Val. checks: 1/15\n",
            "Epoch: 14/100, iter: 4420. Loss (train/val): 0.082 / 0.095. Val. acc: 96.4%, Val. checks: 2/15\n",
            "Epoch: 14/100, iter: 4430. Loss (train/val): 0.081 / 0.091. Val. acc: 96.3%, Val. checks: 0/15\n",
            "Epoch: 14/100, iter: 4440. Loss (train/val): 0.082 / 0.093. Val. acc: 96.4%, Val. checks: 1/15\n",
            "Epoch: 14/100, iter: 4450. Loss (train/val): 0.081 / 0.092. Val. acc: 96.3%, Val. checks: 2/15\n",
            "Epoch: 14/100, iter: 4460. Loss (train/val): 0.081 / 0.093. Val. acc: 96.4%, Val. checks: 3/15\n",
            "Epoch: 14/100, iter: 4470. Loss (train/val): 0.080 / 0.091. Val. acc: 96.4%, Val. checks: 4/15\n",
            "Epoch: 14/100, iter: 4480. Loss (train/val): 0.085 / 0.099. Val. acc: 96.5%, Val. checks: 5/15\n",
            "Epoch: 14/100, iter: 4490. Loss (train/val): 0.082 / 0.094. Val. acc: 96.4%, Val. checks: 6/15\n",
            "Epoch: 14/100, iter: 4500. Loss (train/val): 0.080 / 0.092. Val. acc: 96.4%, Val. checks: 7/15\n",
            "Epoch: 14/100, iter: 4510. Loss (train/val): 0.080 / 0.092. Val. acc: 96.4%, Val. checks: 8/15\n",
            "Epoch: 14/100, iter: 4520. Loss (train/val): 0.079 / 0.090. Val. acc: 96.4%, Val. checks: 0/15\n",
            "Epoch: 14/100, iter: 4530. Loss (train/val): 0.079 / 0.090. Val. acc: 96.5%, Val. checks: 0/15\n",
            "Epoch: 14/100, iter: 4540. Loss (train/val): 0.079 / 0.091. Val. acc: 96.5%, Val. checks: 1/15\n",
            "Epoch: 14/100, iter: 4550. Loss (train/val): 0.079 / 0.093. Val. acc: 96.4%, Val. checks: 2/15\n",
            "Epoch: 14/100, iter: 4560. Loss (train/val): 0.079 / 0.091. Val. acc: 96.4%, Val. checks: 3/15\n",
            "Epoch: 14/100, iter: 4570. Loss (train/val): 0.085 / 0.096. Val. acc: 96.4%, Val. checks: 4/15\n",
            "Epoch: 14/100, iter: 4580. Loss (train/val): 0.080 / 0.092. Val. acc: 96.5%, Val. checks: 5/15\n",
            "Epoch: 14/100, iter: 4590. Loss (train/val): 0.079 / 0.089. Val. acc: 96.5%, Val. checks: 0/15\n",
            "Epoch: 14/100, iter: 4600. Loss (train/val): 0.080 / 0.091. Val. acc: 96.6%, Val. checks: 1/15\n",
            "Epoch: 14/100, iter: 4610. Loss (train/val): 0.078 / 0.089. Val. acc: 96.6%, Val. checks: 0/15\n",
            "Epoch: 14/100, iter: 4620. Loss (train/val): 0.077 / 0.088. Val. acc: 96.6%, Val. checks: 0/15\n",
            "Epoch: 14/100, iter: 4630. Loss (train/val): 0.077 / 0.088. Val. acc: 96.6%, Val. checks: 0/15\n",
            "Epoch: 14/100, iter: 4640. Loss (train/val): 0.080 / 0.090. Val. acc: 96.4%, Val. checks: 1/15\n",
            "Epoch: 14/100, iter: 4650. Loss (train/val): 0.078 / 0.087. Val. acc: 96.5%, Val. checks: 0/15\n",
            "Epoch: 14/100, iter: 4660. Loss (train/val): 0.078 / 0.088. Val. acc: 96.8%, Val. checks: 1/15\n",
            "Epoch: 14/100, iter: 4670. Loss (train/val): 0.077 / 0.088. Val. acc: 96.4%, Val. checks: 2/15\n",
            "Epoch: 14/100, iter: 4680. Loss (train/val): 0.077 / 0.089. Val. acc: 97.0%, Val. checks: 3/15\n",
            "Epoch: 14/100, iter: 4690. Loss (train/val): 0.076 / 0.089. Val. acc: 96.6%, Val. checks: 4/15\n",
            "Epoch: 14/100, iter: 4700. Loss (train/val): 0.077 / 0.090. Val. acc: 96.5%, Val. checks: 5/15\n",
            "Epoch finished. Elapsed time 12.1416 [s]\n",
            "\n",
            "Epoch: 15/100, iter: 4710. Loss (train/val): 0.077 / 0.090. Val. acc: 96.9%, Val. checks: 6/15\n",
            "Epoch: 15/100, iter: 4720. Loss (train/val): 0.076 / 0.089. Val. acc: 96.8%, Val. checks: 7/15\n",
            "Epoch: 15/100, iter: 4730. Loss (train/val): 0.085 / 0.096. Val. acc: 96.4%, Val. checks: 8/15\n",
            "Epoch: 15/100, iter: 4740. Loss (train/val): 0.076 / 0.088. Val. acc: 96.6%, Val. checks: 9/15\n",
            "Epoch: 15/100, iter: 4750. Loss (train/val): 0.077 / 0.088. Val. acc: 96.5%, Val. checks: 10/15\n",
            "Epoch: 15/100, iter: 4760. Loss (train/val): 0.076 / 0.088. Val. acc: 96.5%, Val. checks: 11/15\n",
            "Epoch: 15/100, iter: 4770. Loss (train/val): 0.076 / 0.089. Val. acc: 96.4%, Val. checks: 12/15\n",
            "Epoch: 15/100, iter: 4780. Loss (train/val): 0.077 / 0.090. Val. acc: 96.3%, Val. checks: 13/15\n",
            "Epoch: 15/100, iter: 4790. Loss (train/val): 0.076 / 0.088. Val. acc: 96.6%, Val. checks: 14/15\n",
            "Epoch: 15/100, iter: 4800. Loss (train/val): 0.076 / 0.087. Val. acc: 96.8%, Val. checks: 0/15\n",
            "Epoch: 15/100, iter: 4810. Loss (train/val): 0.075 / 0.087. Val. acc: 96.5%, Val. checks: 1/15\n",
            "Epoch: 15/100, iter: 4820. Loss (train/val): 0.077 / 0.091. Val. acc: 96.9%, Val. checks: 2/15\n",
            "Epoch: 15/100, iter: 4830. Loss (train/val): 0.075 / 0.088. Val. acc: 96.4%, Val. checks: 3/15\n",
            "Epoch: 15/100, iter: 4840. Loss (train/val): 0.077 / 0.091. Val. acc: 96.8%, Val. checks: 4/15\n",
            "Epoch: 15/100, iter: 4850. Loss (train/val): 0.075 / 0.086. Val. acc: 96.4%, Val. checks: 0/15\n",
            "Epoch: 15/100, iter: 4860. Loss (train/val): 0.075 / 0.086. Val. acc: 96.4%, Val. checks: 1/15\n",
            "Epoch: 15/100, iter: 4870. Loss (train/val): 0.075 / 0.085. Val. acc: 96.4%, Val. checks: 0/15\n",
            "Epoch: 15/100, iter: 4880. Loss (train/val): 0.075 / 0.085. Val. acc: 96.5%, Val. checks: 0/15\n",
            "Epoch: 15/100, iter: 4890. Loss (train/val): 0.075 / 0.083. Val. acc: 96.6%, Val. checks: 0/15\n",
            "Epoch: 15/100, iter: 4900. Loss (train/val): 0.075 / 0.087. Val. acc: 96.8%, Val. checks: 1/15\n",
            "Epoch: 15/100, iter: 4910. Loss (train/val): 0.074 / 0.085. Val. acc: 96.6%, Val. checks: 2/15\n",
            "Epoch: 15/100, iter: 4920. Loss (train/val): 0.075 / 0.086. Val. acc: 96.6%, Val. checks: 3/15\n",
            "Epoch: 15/100, iter: 4930. Loss (train/val): 0.074 / 0.085. Val. acc: 96.8%, Val. checks: 4/15\n",
            "Epoch: 15/100, iter: 4940. Loss (train/val): 0.075 / 0.087. Val. acc: 96.8%, Val. checks: 5/15\n",
            "Epoch: 15/100, iter: 4950. Loss (train/val): 0.073 / 0.086. Val. acc: 96.8%, Val. checks: 6/15\n",
            "Epoch: 15/100, iter: 4960. Loss (train/val): 0.073 / 0.086. Val. acc: 96.8%, Val. checks: 7/15\n",
            "Epoch: 15/100, iter: 4970. Loss (train/val): 0.073 / 0.087. Val. acc: 96.8%, Val. checks: 8/15\n",
            "Epoch: 15/100, iter: 4980. Loss (train/val): 0.073 / 0.088. Val. acc: 96.8%, Val. checks: 9/15\n",
            "Epoch: 15/100, iter: 4990. Loss (train/val): 0.073 / 0.087. Val. acc: 96.8%, Val. checks: 10/15\n",
            "Epoch: 15/100, iter: 5000. Loss (train/val): 0.073 / 0.087. Val. acc: 96.9%, Val. checks: 11/15\n",
            "Epoch: 15/100, iter: 5010. Loss (train/val): 0.073 / 0.087. Val. acc: 96.8%, Val. checks: 12/15\n",
            "Epoch: 15/100, iter: 5020. Loss (train/val): 0.072 / 0.086. Val. acc: 96.8%, Val. checks: 13/15\n",
            "Epoch: 15/100, iter: 5030. Loss (train/val): 0.072 / 0.086. Val. acc: 96.6%, Val. checks: 14/15\n",
            "Epoch finished. Elapsed time 13.0947 [s]\n",
            "\n",
            "Epoch: 16/100, iter: 5040. Loss (train/val): 0.073 / 0.087. Val. acc: 96.6%, Val. checks: 15/15\n",
            "Early stopping\n",
            "Epoch finished. Elapsed time 13.1192 [s]\n",
            "\n",
            "\n",
            "\n",
            "[Beginning training of MLP at logdir \"./tarea_1_logs/exp_xentropyrun_1\"]\n",
            "\n",
            "Epoch: 1/100, iter: 0. Loss (train/val): 0.710 / 0.710. Val. acc: 50.1%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 10. Loss (train/val): 0.661 / 0.663. Val. acc: 52.1%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 20. Loss (train/val): 0.600 / 0.602. Val. acc: 71.2%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 30. Loss (train/val): 0.572 / 0.575. Val. acc: 70.7%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 40. Loss (train/val): 0.538 / 0.541. Val. acc: 75.8%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 50. Loss (train/val): 0.503 / 0.507. Val. acc: 79.2%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 60. Loss (train/val): 0.471 / 0.475. Val. acc: 82.4%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 70. Loss (train/val): 0.450 / 0.449. Val. acc: 86.0%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 80. Loss (train/val): 0.435 / 0.433. Val. acc: 86.1%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 90. Loss (train/val): 0.412 / 0.411. Val. acc: 86.5%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 100. Loss (train/val): 0.398 / 0.402. Val. acc: 85.1%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 110. Loss (train/val): 0.375 / 0.376. Val. acc: 87.0%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 120. Loss (train/val): 0.363 / 0.363. Val. acc: 87.1%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 130. Loss (train/val): 0.355 / 0.356. Val. acc: 87.3%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 140. Loss (train/val): 0.345 / 0.343. Val. acc: 87.8%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 150. Loss (train/val): 0.339 / 0.336. Val. acc: 87.8%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 160. Loss (train/val): 0.330 / 0.329. Val. acc: 87.4%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 170. Loss (train/val): 0.325 / 0.321. Val. acc: 87.9%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 180. Loss (train/val): 0.316 / 0.312. Val. acc: 88.4%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 190. Loss (train/val): 0.316 / 0.318. Val. acc: 87.0%, Val. checks: 1/15\n",
            "Epoch: 1/100, iter: 200. Loss (train/val): 0.309 / 0.305. Val. acc: 88.4%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 210. Loss (train/val): 0.306 / 0.305. Val. acc: 87.9%, Val. checks: 1/15\n",
            "Epoch: 1/100, iter: 220. Loss (train/val): 0.298 / 0.295. Val. acc: 88.4%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 230. Loss (train/val): 0.302 / 0.302. Val. acc: 87.4%, Val. checks: 1/15\n",
            "Epoch: 1/100, iter: 240. Loss (train/val): 0.293 / 0.288. Val. acc: 88.7%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 250. Loss (train/val): 0.291 / 0.285. Val. acc: 88.5%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 260. Loss (train/val): 0.289 / 0.282. Val. acc: 89.0%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 270. Loss (train/val): 0.286 / 0.279. Val. acc: 89.3%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 280. Loss (train/val): 0.289 / 0.288. Val. acc: 87.9%, Val. checks: 1/15\n",
            "Epoch: 1/100, iter: 290. Loss (train/val): 0.306 / 0.306. Val. acc: 86.7%, Val. checks: 2/15\n",
            "Epoch: 1/100, iter: 300. Loss (train/val): 0.276 / 0.269. Val. acc: 90.3%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 310. Loss (train/val): 0.280 / 0.276. Val. acc: 89.0%, Val. checks: 1/15\n",
            "Epoch: 1/100, iter: 320. Loss (train/val): 0.275 / 0.270. Val. acc: 89.4%, Val. checks: 2/15\n",
            "Epoch: 1/100, iter: 330. Loss (train/val): 0.272 / 0.265. Val. acc: 89.7%, Val. checks: 0/15\n",
            "Epoch finished. Elapsed time 0.9386 [s]\n",
            "\n",
            "Epoch: 2/100, iter: 340. Loss (train/val): 0.271 / 0.267. Val. acc: 89.4%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 350. Loss (train/val): 0.269 / 0.262. Val. acc: 90.0%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 360. Loss (train/val): 0.266 / 0.262. Val. acc: 89.4%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 370. Loss (train/val): 0.269 / 0.267. Val. acc: 89.1%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 380. Loss (train/val): 0.263 / 0.259. Val. acc: 89.1%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 390. Loss (train/val): 0.266 / 0.257. Val. acc: 90.4%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 400. Loss (train/val): 0.260 / 0.254. Val. acc: 89.7%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 410. Loss (train/val): 0.258 / 0.250. Val. acc: 90.6%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 420. Loss (train/val): 0.260 / 0.255. Val. acc: 89.4%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 430. Loss (train/val): 0.257 / 0.247. Val. acc: 90.8%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 440. Loss (train/val): 0.259 / 0.247. Val. acc: 91.0%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 450. Loss (train/val): 0.253 / 0.243. Val. acc: 90.2%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 460. Loss (train/val): 0.254 / 0.245. Val. acc: 90.6%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 470. Loss (train/val): 0.255 / 0.249. Val. acc: 89.7%, Val. checks: 2/15\n",
            "Epoch: 2/100, iter: 480. Loss (train/val): 0.255 / 0.251. Val. acc: 89.3%, Val. checks: 3/15\n",
            "Epoch: 2/100, iter: 490. Loss (train/val): 0.250 / 0.243. Val. acc: 89.9%, Val. checks: 4/15\n",
            "Epoch: 2/100, iter: 500. Loss (train/val): 0.251 / 0.240. Val. acc: 90.9%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 510. Loss (train/val): 0.270 / 0.255. Val. acc: 91.1%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 520. Loss (train/val): 0.247 / 0.240. Val. acc: 90.4%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 530. Loss (train/val): 0.247 / 0.243. Val. acc: 89.7%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 540. Loss (train/val): 0.245 / 0.238. Val. acc: 89.9%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 550. Loss (train/val): 0.244 / 0.236. Val. acc: 90.3%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 560. Loss (train/val): 0.244 / 0.234. Val. acc: 91.5%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 570. Loss (train/val): 0.242 / 0.234. Val. acc: 90.8%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 580. Loss (train/val): 0.245 / 0.234. Val. acc: 91.2%, Val. checks: 2/15\n",
            "Epoch: 2/100, iter: 590. Loss (train/val): 0.243 / 0.235. Val. acc: 90.3%, Val. checks: 3/15\n",
            "Epoch: 2/100, iter: 600. Loss (train/val): 0.246 / 0.238. Val. acc: 91.5%, Val. checks: 4/15\n",
            "Epoch: 2/100, iter: 610. Loss (train/val): 0.240 / 0.236. Val. acc: 90.5%, Val. checks: 5/15\n",
            "Epoch: 2/100, iter: 620. Loss (train/val): 0.238 / 0.234. Val. acc: 90.4%, Val. checks: 6/15\n",
            "Epoch: 2/100, iter: 630. Loss (train/val): 0.238 / 0.229. Val. acc: 91.1%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 640. Loss (train/val): 0.255 / 0.246. Val. acc: 89.3%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 650. Loss (train/val): 0.240 / 0.229. Val. acc: 91.3%, Val. checks: 2/15\n",
            "Epoch: 2/100, iter: 660. Loss (train/val): 0.237 / 0.225. Val. acc: 90.8%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 670. Loss (train/val): 0.245 / 0.237. Val. acc: 89.8%, Val. checks: 1/15\n",
            "Epoch finished. Elapsed time 1.8326 [s]\n",
            "\n",
            "Epoch: 3/100, iter: 680. Loss (train/val): 0.254 / 0.241. Val. acc: 92.0%, Val. checks: 2/15\n",
            "Epoch: 3/100, iter: 690. Loss (train/val): 0.234 / 0.228. Val. acc: 90.9%, Val. checks: 3/15\n",
            "Epoch: 3/100, iter: 700. Loss (train/val): 0.239 / 0.232. Val. acc: 90.3%, Val. checks: 4/15\n",
            "Epoch: 3/100, iter: 710. Loss (train/val): 0.237 / 0.231. Val. acc: 90.0%, Val. checks: 5/15\n",
            "Epoch: 3/100, iter: 720. Loss (train/val): 0.232 / 0.226. Val. acc: 90.6%, Val. checks: 6/15\n",
            "Epoch: 3/100, iter: 730. Loss (train/val): 0.248 / 0.239. Val. acc: 92.4%, Val. checks: 7/15\n",
            "Epoch: 3/100, iter: 740. Loss (train/val): 0.231 / 0.224. Val. acc: 90.7%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 750. Loss (train/val): 0.231 / 0.223. Val. acc: 90.9%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 760. Loss (train/val): 0.232 / 0.222. Val. acc: 90.9%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 770. Loss (train/val): 0.231 / 0.222. Val. acc: 90.8%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 780. Loss (train/val): 0.232 / 0.223. Val. acc: 92.0%, Val. checks: 2/15\n",
            "Epoch: 3/100, iter: 790. Loss (train/val): 0.228 / 0.220. Val. acc: 92.1%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 800. Loss (train/val): 0.230 / 0.222. Val. acc: 92.1%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 810. Loss (train/val): 0.226 / 0.219. Val. acc: 91.3%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 820. Loss (train/val): 0.228 / 0.220. Val. acc: 91.6%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 830. Loss (train/val): 0.225 / 0.217. Val. acc: 90.9%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 840. Loss (train/val): 0.225 / 0.219. Val. acc: 91.5%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 850. Loss (train/val): 0.230 / 0.222. Val. acc: 92.4%, Val. checks: 2/15\n",
            "Epoch: 3/100, iter: 860. Loss (train/val): 0.227 / 0.219. Val. acc: 92.3%, Val. checks: 3/15\n",
            "Epoch: 3/100, iter: 870. Loss (train/val): 0.224 / 0.217. Val. acc: 92.1%, Val. checks: 4/15\n",
            "Epoch: 3/100, iter: 880. Loss (train/val): 0.222 / 0.216. Val. acc: 91.3%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 890. Loss (train/val): 0.221 / 0.213. Val. acc: 91.8%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 900. Loss (train/val): 0.220 / 0.212. Val. acc: 92.0%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 910. Loss (train/val): 0.220 / 0.214. Val. acc: 91.3%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 920. Loss (train/val): 0.219 / 0.215. Val. acc: 91.8%, Val. checks: 2/15\n",
            "Epoch: 3/100, iter: 930. Loss (train/val): 0.218 / 0.211. Val. acc: 91.7%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 940. Loss (train/val): 0.217 / 0.210. Val. acc: 92.1%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 950. Loss (train/val): 0.217 / 0.209. Val. acc: 92.1%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 960. Loss (train/val): 0.224 / 0.220. Val. acc: 90.2%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 970. Loss (train/val): 0.216 / 0.208. Val. acc: 91.8%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 980. Loss (train/val): 0.217 / 0.207. Val. acc: 91.7%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 990. Loss (train/val): 0.217 / 0.209. Val. acc: 91.3%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 1000. Loss (train/val): 0.214 / 0.207. Val. acc: 92.1%, Val. checks: 2/15\n",
            "Epoch finished. Elapsed time 2.6892 [s]\n",
            "\n",
            "Epoch: 4/100, iter: 1010. Loss (train/val): 0.223 / 0.214. Val. acc: 93.0%, Val. checks: 3/15\n",
            "Epoch: 4/100, iter: 1020. Loss (train/val): 0.217 / 0.209. Val. acc: 92.5%, Val. checks: 4/15\n",
            "Epoch: 4/100, iter: 1030. Loss (train/val): 0.214 / 0.207. Val. acc: 92.5%, Val. checks: 5/15\n",
            "Epoch: 4/100, iter: 1040. Loss (train/val): 0.213 / 0.207. Val. acc: 92.1%, Val. checks: 6/15\n",
            "Epoch: 4/100, iter: 1050. Loss (train/val): 0.219 / 0.214. Val. acc: 91.1%, Val. checks: 7/15\n",
            "Epoch: 4/100, iter: 1060. Loss (train/val): 0.211 / 0.204. Val. acc: 92.2%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1070. Loss (train/val): 0.211 / 0.203. Val. acc: 92.4%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1080. Loss (train/val): 0.210 / 0.201. Val. acc: 92.4%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1090. Loss (train/val): 0.210 / 0.200. Val. acc: 92.6%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1100. Loss (train/val): 0.209 / 0.201. Val. acc: 92.5%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1110. Loss (train/val): 0.210 / 0.201. Val. acc: 92.7%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1120. Loss (train/val): 0.221 / 0.212. Val. acc: 92.9%, Val. checks: 3/15\n",
            "Epoch: 4/100, iter: 1130. Loss (train/val): 0.208 / 0.201. Val. acc: 93.0%, Val. checks: 4/15\n",
            "Epoch: 4/100, iter: 1140. Loss (train/val): 0.207 / 0.199. Val. acc: 92.9%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1150. Loss (train/val): 0.210 / 0.202. Val. acc: 91.6%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1160. Loss (train/val): 0.213 / 0.206. Val. acc: 93.4%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1170. Loss (train/val): 0.208 / 0.202. Val. acc: 92.0%, Val. checks: 3/15\n",
            "Epoch: 4/100, iter: 1180. Loss (train/val): 0.206 / 0.203. Val. acc: 92.7%, Val. checks: 4/15\n",
            "Epoch: 4/100, iter: 1190. Loss (train/val): 0.207 / 0.202. Val. acc: 92.6%, Val. checks: 5/15\n",
            "Epoch: 4/100, iter: 1200. Loss (train/val): 0.205 / 0.198. Val. acc: 92.7%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1210. Loss (train/val): 0.203 / 0.197. Val. acc: 92.4%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1220. Loss (train/val): 0.209 / 0.204. Val. acc: 90.9%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1230. Loss (train/val): 0.203 / 0.197. Val. acc: 92.3%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1240. Loss (train/val): 0.202 / 0.195. Val. acc: 92.9%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1250. Loss (train/val): 0.210 / 0.203. Val. acc: 93.2%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1260. Loss (train/val): 0.220 / 0.217. Val. acc: 90.2%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1270. Loss (train/val): 0.200 / 0.193. Val. acc: 92.4%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1280. Loss (train/val): 0.200 / 0.194. Val. acc: 92.3%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1290. Loss (train/val): 0.208 / 0.199. Val. acc: 93.1%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1300. Loss (train/val): 0.201 / 0.195. Val. acc: 92.3%, Val. checks: 3/15\n",
            "Epoch: 4/100, iter: 1310. Loss (train/val): 0.203 / 0.198. Val. acc: 91.8%, Val. checks: 4/15\n",
            "Epoch: 4/100, iter: 1320. Loss (train/val): 0.196 / 0.190. Val. acc: 93.0%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1330. Loss (train/val): 0.198 / 0.192. Val. acc: 93.1%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1340. Loss (train/val): 0.196 / 0.192. Val. acc: 93.0%, Val. checks: 2/15\n",
            "Epoch finished. Elapsed time 3.5660 [s]\n",
            "\n",
            "Epoch: 5/100, iter: 1350. Loss (train/val): 0.196 / 0.189. Val. acc: 93.1%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1360. Loss (train/val): 0.208 / 0.201. Val. acc: 91.6%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1370. Loss (train/val): 0.194 / 0.188. Val. acc: 93.1%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1380. Loss (train/val): 0.193 / 0.188. Val. acc: 93.1%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1390. Loss (train/val): 0.193 / 0.190. Val. acc: 93.0%, Val. checks: 2/15\n",
            "Epoch: 5/100, iter: 1400. Loss (train/val): 0.193 / 0.188. Val. acc: 92.6%, Val. checks: 3/15\n",
            "Epoch: 5/100, iter: 1410. Loss (train/val): 0.192 / 0.188. Val. acc: 93.1%, Val. checks: 4/15\n",
            "Epoch: 5/100, iter: 1420. Loss (train/val): 0.194 / 0.191. Val. acc: 92.7%, Val. checks: 5/15\n",
            "Epoch: 5/100, iter: 1430. Loss (train/val): 0.192 / 0.190. Val. acc: 93.4%, Val. checks: 6/15\n",
            "Epoch: 5/100, iter: 1440. Loss (train/val): 0.190 / 0.189. Val. acc: 93.3%, Val. checks: 7/15\n",
            "Epoch: 5/100, iter: 1450. Loss (train/val): 0.190 / 0.188. Val. acc: 93.3%, Val. checks: 8/15\n",
            "Epoch: 5/100, iter: 1460. Loss (train/val): 0.192 / 0.188. Val. acc: 93.5%, Val. checks: 9/15\n",
            "Epoch: 5/100, iter: 1470. Loss (train/val): 0.201 / 0.195. Val. acc: 91.6%, Val. checks: 10/15\n",
            "Epoch: 5/100, iter: 1480. Loss (train/val): 0.191 / 0.187. Val. acc: 92.7%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1490. Loss (train/val): 0.187 / 0.185. Val. acc: 92.9%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1500. Loss (train/val): 0.187 / 0.184. Val. acc: 92.9%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1510. Loss (train/val): 0.189 / 0.186. Val. acc: 92.9%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1520. Loss (train/val): 0.185 / 0.183. Val. acc: 93.0%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1530. Loss (train/val): 0.184 / 0.182. Val. acc: 93.2%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1540. Loss (train/val): 0.185 / 0.184. Val. acc: 92.5%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1550. Loss (train/val): 0.184 / 0.183. Val. acc: 93.0%, Val. checks: 2/15\n",
            "Epoch: 5/100, iter: 1560. Loss (train/val): 0.183 / 0.182. Val. acc: 93.1%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1570. Loss (train/val): 0.182 / 0.179. Val. acc: 93.4%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1580. Loss (train/val): 0.184 / 0.179. Val. acc: 93.6%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1590. Loss (train/val): 0.182 / 0.178. Val. acc: 93.0%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1600. Loss (train/val): 0.191 / 0.189. Val. acc: 93.1%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1610. Loss (train/val): 0.180 / 0.180. Val. acc: 93.0%, Val. checks: 2/15\n",
            "Epoch: 5/100, iter: 1620. Loss (train/val): 0.192 / 0.190. Val. acc: 93.0%, Val. checks: 3/15\n",
            "Epoch: 5/100, iter: 1630. Loss (train/val): 0.180 / 0.179. Val. acc: 93.3%, Val. checks: 4/15\n",
            "Epoch: 5/100, iter: 1640. Loss (train/val): 0.180 / 0.178. Val. acc: 93.0%, Val. checks: 5/15\n",
            "Epoch: 5/100, iter: 1650. Loss (train/val): 0.180 / 0.179. Val. acc: 92.5%, Val. checks: 6/15\n",
            "Epoch: 5/100, iter: 1660. Loss (train/val): 0.177 / 0.175. Val. acc: 93.6%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1670. Loss (train/val): 0.177 / 0.176. Val. acc: 93.5%, Val. checks: 1/15\n",
            "Epoch finished. Elapsed time 4.4435 [s]\n",
            "\n",
            "Epoch: 6/100, iter: 1680. Loss (train/val): 0.177 / 0.174. Val. acc: 93.5%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1690. Loss (train/val): 0.177 / 0.175. Val. acc: 93.3%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1700. Loss (train/val): 0.175 / 0.173. Val. acc: 93.4%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1710. Loss (train/val): 0.182 / 0.180. Val. acc: 93.7%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1720. Loss (train/val): 0.178 / 0.175. Val. acc: 93.6%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 1730. Loss (train/val): 0.179 / 0.177. Val. acc: 93.4%, Val. checks: 3/15\n",
            "Epoch: 6/100, iter: 1740. Loss (train/val): 0.173 / 0.171. Val. acc: 93.5%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1750. Loss (train/val): 0.183 / 0.181. Val. acc: 92.3%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1760. Loss (train/val): 0.173 / 0.171. Val. acc: 94.2%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1770. Loss (train/val): 0.172 / 0.170. Val. acc: 93.7%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1780. Loss (train/val): 0.172 / 0.169. Val. acc: 93.8%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1790. Loss (train/val): 0.176 / 0.174. Val. acc: 93.4%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1800. Loss (train/val): 0.172 / 0.169. Val. acc: 93.9%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 1810. Loss (train/val): 0.171 / 0.170. Val. acc: 93.5%, Val. checks: 3/15\n",
            "Epoch: 6/100, iter: 1820. Loss (train/val): 0.171 / 0.170. Val. acc: 93.5%, Val. checks: 4/15\n",
            "Epoch: 6/100, iter: 1830. Loss (train/val): 0.169 / 0.166. Val. acc: 93.8%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1840. Loss (train/val): 0.168 / 0.165. Val. acc: 94.3%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1850. Loss (train/val): 0.169 / 0.166. Val. acc: 94.5%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1860. Loss (train/val): 0.181 / 0.178. Val. acc: 92.3%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 1870. Loss (train/val): 0.168 / 0.168. Val. acc: 93.4%, Val. checks: 3/15\n",
            "Epoch: 6/100, iter: 1880. Loss (train/val): 0.167 / 0.166. Val. acc: 93.5%, Val. checks: 4/15\n",
            "Epoch: 6/100, iter: 1890. Loss (train/val): 0.165 / 0.163. Val. acc: 93.9%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1900. Loss (train/val): 0.167 / 0.166. Val. acc: 94.0%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1910. Loss (train/val): 0.165 / 0.163. Val. acc: 93.9%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 1920. Loss (train/val): 0.167 / 0.165. Val. acc: 94.4%, Val. checks: 3/15\n",
            "Epoch: 6/100, iter: 1930. Loss (train/val): 0.165 / 0.162. Val. acc: 93.8%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1940. Loss (train/val): 0.174 / 0.170. Val. acc: 92.4%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1950. Loss (train/val): 0.162 / 0.162. Val. acc: 94.0%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1960. Loss (train/val): 0.165 / 0.163. Val. acc: 93.3%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1970. Loss (train/val): 0.161 / 0.161. Val. acc: 93.9%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1980. Loss (train/val): 0.167 / 0.167. Val. acc: 94.2%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1990. Loss (train/val): 0.160 / 0.161. Val. acc: 93.9%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 2000. Loss (train/val): 0.161 / 0.161. Val. acc: 93.8%, Val. checks: 3/15\n",
            "Epoch: 6/100, iter: 2010. Loss (train/val): 0.159 / 0.159. Val. acc: 94.0%, Val. checks: 0/15\n",
            "Epoch finished. Elapsed time 5.4359 [s]\n",
            "\n",
            "Epoch: 7/100, iter: 2020. Loss (train/val): 0.169 / 0.170. Val. acc: 94.4%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2030. Loss (train/val): 0.159 / 0.158. Val. acc: 93.7%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2040. Loss (train/val): 0.159 / 0.158. Val. acc: 94.4%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2050. Loss (train/val): 0.161 / 0.160. Val. acc: 93.5%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2060. Loss (train/val): 0.157 / 0.156. Val. acc: 94.2%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2070. Loss (train/val): 0.158 / 0.157. Val. acc: 94.0%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2080. Loss (train/val): 0.156 / 0.157. Val. acc: 93.9%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2090. Loss (train/val): 0.157 / 0.158. Val. acc: 94.3%, Val. checks: 3/15\n",
            "Epoch: 7/100, iter: 2100. Loss (train/val): 0.155 / 0.157. Val. acc: 94.5%, Val. checks: 4/15\n",
            "Epoch: 7/100, iter: 2110. Loss (train/val): 0.155 / 0.156. Val. acc: 93.9%, Val. checks: 5/15\n",
            "Epoch: 7/100, iter: 2120. Loss (train/val): 0.158 / 0.159. Val. acc: 93.5%, Val. checks: 6/15\n",
            "Epoch: 7/100, iter: 2130. Loss (train/val): 0.155 / 0.155. Val. acc: 93.8%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2140. Loss (train/val): 0.158 / 0.158. Val. acc: 93.6%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2150. Loss (train/val): 0.153 / 0.151. Val. acc: 93.9%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2160. Loss (train/val): 0.152 / 0.151. Val. acc: 93.8%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2170. Loss (train/val): 0.155 / 0.155. Val. acc: 94.7%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2180. Loss (train/val): 0.151 / 0.152. Val. acc: 94.5%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2190. Loss (train/val): 0.151 / 0.151. Val. acc: 94.5%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2200. Loss (train/val): 0.150 / 0.151. Val. acc: 94.5%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2210. Loss (train/val): 0.151 / 0.152. Val. acc: 94.7%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2220. Loss (train/val): 0.155 / 0.156. Val. acc: 94.7%, Val. checks: 3/15\n",
            "Epoch: 7/100, iter: 2230. Loss (train/val): 0.150 / 0.153. Val. acc: 94.0%, Val. checks: 4/15\n",
            "Epoch: 7/100, iter: 2240. Loss (train/val): 0.158 / 0.160. Val. acc: 94.7%, Val. checks: 5/15\n",
            "Epoch: 7/100, iter: 2250. Loss (train/val): 0.148 / 0.148. Val. acc: 94.5%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2260. Loss (train/val): 0.148 / 0.147. Val. acc: 93.9%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2270. Loss (train/val): 0.156 / 0.157. Val. acc: 94.7%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2280. Loss (train/val): 0.147 / 0.148. Val. acc: 94.5%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2290. Loss (train/val): 0.146 / 0.147. Val. acc: 93.8%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2300. Loss (train/val): 0.145 / 0.146. Val. acc: 94.0%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2310. Loss (train/val): 0.148 / 0.148. Val. acc: 94.6%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2320. Loss (train/val): 0.145 / 0.146. Val. acc: 94.4%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2330. Loss (train/val): 0.149 / 0.149. Val. acc: 93.5%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2340. Loss (train/val): 0.149 / 0.150. Val. acc: 93.6%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2350. Loss (train/val): 0.144 / 0.146. Val. acc: 94.2%, Val. checks: 3/15\n",
            "Epoch finished. Elapsed time 6.2987 [s]\n",
            "\n",
            "Epoch: 8/100, iter: 2360. Loss (train/val): 0.143 / 0.144. Val. acc: 94.2%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2370. Loss (train/val): 0.144 / 0.145. Val. acc: 93.9%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2380. Loss (train/val): 0.143 / 0.143. Val. acc: 93.9%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2390. Loss (train/val): 0.149 / 0.151. Val. acc: 94.7%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2400. Loss (train/val): 0.149 / 0.152. Val. acc: 94.4%, Val. checks: 2/15\n",
            "Epoch: 8/100, iter: 2410. Loss (train/val): 0.142 / 0.144. Val. acc: 94.2%, Val. checks: 3/15\n",
            "Epoch: 8/100, iter: 2420. Loss (train/val): 0.142 / 0.143. Val. acc: 94.6%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2430. Loss (train/val): 0.141 / 0.145. Val. acc: 94.3%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2440. Loss (train/val): 0.140 / 0.142. Val. acc: 94.7%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2450. Loss (train/val): 0.140 / 0.143. Val. acc: 94.7%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2460. Loss (train/val): 0.140 / 0.143. Val. acc: 94.8%, Val. checks: 2/15\n",
            "Epoch: 8/100, iter: 2470. Loss (train/val): 0.139 / 0.142. Val. acc: 94.6%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2480. Loss (train/val): 0.140 / 0.143. Val. acc: 93.8%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2490. Loss (train/val): 0.139 / 0.143. Val. acc: 94.8%, Val. checks: 2/15\n",
            "Epoch: 8/100, iter: 2500. Loss (train/val): 0.142 / 0.145. Val. acc: 93.5%, Val. checks: 3/15\n",
            "Epoch: 8/100, iter: 2510. Loss (train/val): 0.137 / 0.141. Val. acc: 93.9%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2520. Loss (train/val): 0.138 / 0.141. Val. acc: 94.9%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2530. Loss (train/val): 0.145 / 0.147. Val. acc: 94.9%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2540. Loss (train/val): 0.138 / 0.140. Val. acc: 93.8%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2550. Loss (train/val): 0.137 / 0.138. Val. acc: 94.2%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2560. Loss (train/val): 0.137 / 0.138. Val. acc: 94.2%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2570. Loss (train/val): 0.135 / 0.136. Val. acc: 94.7%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2580. Loss (train/val): 0.137 / 0.139. Val. acc: 94.8%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2590. Loss (train/val): 0.133 / 0.135. Val. acc: 94.5%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2600. Loss (train/val): 0.136 / 0.137. Val. acc: 94.7%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2610. Loss (train/val): 0.135 / 0.137. Val. acc: 93.9%, Val. checks: 2/15\n",
            "Epoch: 8/100, iter: 2620. Loss (train/val): 0.135 / 0.135. Val. acc: 94.7%, Val. checks: 3/15\n",
            "Epoch: 8/100, iter: 2630. Loss (train/val): 0.134 / 0.134. Val. acc: 94.8%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2640. Loss (train/val): 0.133 / 0.134. Val. acc: 94.5%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2650. Loss (train/val): 0.133 / 0.132. Val. acc: 94.7%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2660. Loss (train/val): 0.133 / 0.135. Val. acc: 94.6%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2670. Loss (train/val): 0.131 / 0.133. Val. acc: 94.5%, Val. checks: 2/15\n",
            "Epoch: 8/100, iter: 2680. Loss (train/val): 0.130 / 0.132. Val. acc: 94.6%, Val. checks: 3/15\n",
            "Epoch finished. Elapsed time 7.1667 [s]\n",
            "\n",
            "Epoch: 9/100, iter: 2690. Loss (train/val): 0.130 / 0.133. Val. acc: 94.5%, Val. checks: 4/15\n",
            "Epoch: 9/100, iter: 2700. Loss (train/val): 0.130 / 0.132. Val. acc: 94.4%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2710. Loss (train/val): 0.130 / 0.132. Val. acc: 94.6%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2720. Loss (train/val): 0.131 / 0.134. Val. acc: 94.8%, Val. checks: 2/15\n",
            "Epoch: 9/100, iter: 2730. Loss (train/val): 0.130 / 0.133. Val. acc: 94.3%, Val. checks: 3/15\n",
            "Epoch: 9/100, iter: 2740. Loss (train/val): 0.129 / 0.132. Val. acc: 94.4%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2750. Loss (train/val): 0.129 / 0.131. Val. acc: 94.4%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2760. Loss (train/val): 0.129 / 0.130. Val. acc: 95.0%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2770. Loss (train/val): 0.129 / 0.130. Val. acc: 94.7%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2780. Loss (train/val): 0.128 / 0.131. Val. acc: 94.3%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2790. Loss (train/val): 0.126 / 0.129. Val. acc: 94.5%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2800. Loss (train/val): 0.127 / 0.130. Val. acc: 94.9%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2810. Loss (train/val): 0.129 / 0.133. Val. acc: 94.7%, Val. checks: 2/15\n",
            "Epoch: 9/100, iter: 2820. Loss (train/val): 0.129 / 0.134. Val. acc: 94.6%, Val. checks: 3/15\n",
            "Epoch: 9/100, iter: 2830. Loss (train/val): 0.127 / 0.131. Val. acc: 94.7%, Val. checks: 4/15\n",
            "Epoch: 9/100, iter: 2840. Loss (train/val): 0.127 / 0.132. Val. acc: 94.7%, Val. checks: 5/15\n",
            "Epoch: 9/100, iter: 2850. Loss (train/val): 0.126 / 0.130. Val. acc: 94.4%, Val. checks: 6/15\n",
            "Epoch: 9/100, iter: 2860. Loss (train/val): 0.126 / 0.131. Val. acc: 94.4%, Val. checks: 7/15\n",
            "Epoch: 9/100, iter: 2870. Loss (train/val): 0.124 / 0.129. Val. acc: 94.9%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2880. Loss (train/val): 0.125 / 0.129. Val. acc: 95.0%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2890. Loss (train/val): 0.124 / 0.127. Val. acc: 95.1%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2900. Loss (train/val): 0.123 / 0.127. Val. acc: 94.8%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2910. Loss (train/val): 0.128 / 0.134. Val. acc: 94.9%, Val. checks: 2/15\n",
            "Epoch: 9/100, iter: 2920. Loss (train/val): 0.132 / 0.136. Val. acc: 94.4%, Val. checks: 3/15\n",
            "Epoch: 9/100, iter: 2930. Loss (train/val): 0.122 / 0.126. Val. acc: 94.7%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2940. Loss (train/val): 0.122 / 0.128. Val. acc: 94.4%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2950. Loss (train/val): 0.122 / 0.126. Val. acc: 94.9%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2960. Loss (train/val): 0.121 / 0.126. Val. acc: 94.9%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2970. Loss (train/val): 0.120 / 0.124. Val. acc: 94.8%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2980. Loss (train/val): 0.120 / 0.124. Val. acc: 94.6%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2990. Loss (train/val): 0.120 / 0.125. Val. acc: 94.5%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 3000. Loss (train/val): 0.119 / 0.125. Val. acc: 94.3%, Val. checks: 2/15\n",
            "Epoch: 9/100, iter: 3010. Loss (train/val): 0.120 / 0.125. Val. acc: 94.4%, Val. checks: 3/15\n",
            "Epoch: 9/100, iter: 3020. Loss (train/val): 0.118 / 0.124. Val. acc: 94.5%, Val. checks: 0/15\n",
            "Epoch finished. Elapsed time 8.0444 [s]\n",
            "\n",
            "Epoch: 10/100, iter: 3030. Loss (train/val): 0.119 / 0.124. Val. acc: 94.6%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3040. Loss (train/val): 0.120 / 0.124. Val. acc: 94.5%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3050. Loss (train/val): 0.119 / 0.123. Val. acc: 94.9%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3060. Loss (train/val): 0.124 / 0.128. Val. acc: 95.0%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3070. Loss (train/val): 0.117 / 0.121. Val. acc: 94.8%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3080. Loss (train/val): 0.117 / 0.121. Val. acc: 94.6%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3090. Loss (train/val): 0.121 / 0.125. Val. acc: 95.0%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3100. Loss (train/val): 0.117 / 0.120. Val. acc: 95.2%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3110. Loss (train/val): 0.117 / 0.120. Val. acc: 95.1%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3120. Loss (train/val): 0.117 / 0.120. Val. acc: 95.2%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3130. Loss (train/val): 0.124 / 0.128. Val. acc: 95.5%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3140. Loss (train/val): 0.116 / 0.120. Val. acc: 95.1%, Val. checks: 3/15\n",
            "Epoch: 10/100, iter: 3150. Loss (train/val): 0.116 / 0.120. Val. acc: 94.6%, Val. checks: 4/15\n",
            "Epoch: 10/100, iter: 3160. Loss (train/val): 0.116 / 0.121. Val. acc: 95.1%, Val. checks: 5/15\n",
            "Epoch: 10/100, iter: 3170. Loss (train/val): 0.121 / 0.126. Val. acc: 94.7%, Val. checks: 6/15\n",
            "Epoch: 10/100, iter: 3180. Loss (train/val): 0.114 / 0.119. Val. acc: 95.2%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3190. Loss (train/val): 0.116 / 0.120. Val. acc: 95.0%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3200. Loss (train/val): 0.115 / 0.120. Val. acc: 94.9%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3210. Loss (train/val): 0.114 / 0.119. Val. acc: 94.4%, Val. checks: 3/15\n",
            "Epoch: 10/100, iter: 3220. Loss (train/val): 0.122 / 0.129. Val. acc: 95.1%, Val. checks: 4/15\n",
            "Epoch: 10/100, iter: 3230. Loss (train/val): 0.113 / 0.119. Val. acc: 94.9%, Val. checks: 5/15\n",
            "Epoch: 10/100, iter: 3240. Loss (train/val): 0.112 / 0.119. Val. acc: 94.4%, Val. checks: 6/15\n",
            "Epoch: 10/100, iter: 3250. Loss (train/val): 0.115 / 0.123. Val. acc: 94.8%, Val. checks: 7/15\n",
            "Epoch: 10/100, iter: 3260. Loss (train/val): 0.112 / 0.119. Val. acc: 94.8%, Val. checks: 8/15\n",
            "Epoch: 10/100, iter: 3270. Loss (train/val): 0.111 / 0.119. Val. acc: 94.6%, Val. checks: 9/15\n",
            "Epoch: 10/100, iter: 3280. Loss (train/val): 0.114 / 0.121. Val. acc: 95.0%, Val. checks: 10/15\n",
            "Epoch: 10/100, iter: 3290. Loss (train/val): 0.112 / 0.119. Val. acc: 95.0%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3300. Loss (train/val): 0.111 / 0.117. Val. acc: 94.7%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3310. Loss (train/val): 0.111 / 0.117. Val. acc: 95.0%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3320. Loss (train/val): 0.111 / 0.115. Val. acc: 94.8%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3330. Loss (train/val): 0.117 / 0.123. Val. acc: 95.3%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3340. Loss (train/val): 0.110 / 0.117. Val. acc: 95.0%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3350. Loss (train/val): 0.113 / 0.120. Val. acc: 95.3%, Val. checks: 3/15\n",
            "Epoch finished. Elapsed time 8.9088 [s]\n",
            "\n",
            "Epoch: 11/100, iter: 3360. Loss (train/val): 0.109 / 0.115. Val. acc: 94.7%, Val. checks: 4/15\n",
            "Epoch: 11/100, iter: 3370. Loss (train/val): 0.108 / 0.116. Val. acc: 94.8%, Val. checks: 5/15\n",
            "Epoch: 11/100, iter: 3380. Loss (train/val): 0.109 / 0.115. Val. acc: 94.8%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3390. Loss (train/val): 0.109 / 0.115. Val. acc: 94.8%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3400. Loss (train/val): 0.110 / 0.115. Val. acc: 95.1%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3410. Loss (train/val): 0.108 / 0.115. Val. acc: 94.9%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3420. Loss (train/val): 0.107 / 0.115. Val. acc: 95.1%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3430. Loss (train/val): 0.108 / 0.114. Val. acc: 95.0%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3440. Loss (train/val): 0.107 / 0.113. Val. acc: 94.8%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3450. Loss (train/val): 0.106 / 0.114. Val. acc: 95.1%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3460. Loss (train/val): 0.107 / 0.115. Val. acc: 94.8%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3470. Loss (train/val): 0.109 / 0.117. Val. acc: 95.3%, Val. checks: 3/15\n",
            "Epoch: 11/100, iter: 3480. Loss (train/val): 0.106 / 0.113. Val. acc: 95.1%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3490. Loss (train/val): 0.106 / 0.113. Val. acc: 95.1%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3500. Loss (train/val): 0.105 / 0.113. Val. acc: 95.0%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3510. Loss (train/val): 0.106 / 0.113. Val. acc: 95.0%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3520. Loss (train/val): 0.107 / 0.114. Val. acc: 95.5%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3530. Loss (train/val): 0.107 / 0.115. Val. acc: 95.2%, Val. checks: 3/15\n",
            "Epoch: 11/100, iter: 3540. Loss (train/val): 0.109 / 0.117. Val. acc: 95.6%, Val. checks: 4/15\n",
            "Epoch: 11/100, iter: 3550. Loss (train/val): 0.105 / 0.112. Val. acc: 94.9%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3560. Loss (train/val): 0.105 / 0.112. Val. acc: 95.0%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3570. Loss (train/val): 0.105 / 0.112. Val. acc: 95.6%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3580. Loss (train/val): 0.103 / 0.111. Val. acc: 95.2%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3590. Loss (train/val): 0.103 / 0.112. Val. acc: 95.0%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3600. Loss (train/val): 0.103 / 0.110. Val. acc: 94.8%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3610. Loss (train/val): 0.103 / 0.110. Val. acc: 95.0%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3620. Loss (train/val): 0.103 / 0.110. Val. acc: 95.5%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3630. Loss (train/val): 0.102 / 0.108. Val. acc: 95.1%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3640. Loss (train/val): 0.102 / 0.110. Val. acc: 95.0%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3650. Loss (train/val): 0.102 / 0.110. Val. acc: 95.1%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3660. Loss (train/val): 0.107 / 0.116. Val. acc: 95.7%, Val. checks: 3/15\n",
            "Epoch: 11/100, iter: 3670. Loss (train/val): 0.102 / 0.110. Val. acc: 95.0%, Val. checks: 4/15\n",
            "Epoch: 11/100, iter: 3680. Loss (train/val): 0.115 / 0.124. Val. acc: 95.5%, Val. checks: 5/15\n",
            "Epoch: 11/100, iter: 3690. Loss (train/val): 0.101 / 0.109. Val. acc: 95.1%, Val. checks: 6/15\n",
            "Epoch finished. Elapsed time 9.7879 [s]\n",
            "\n",
            "Epoch: 12/100, iter: 3700. Loss (train/val): 0.101 / 0.109. Val. acc: 95.1%, Val. checks: 7/15\n",
            "Epoch: 12/100, iter: 3710. Loss (train/val): 0.102 / 0.110. Val. acc: 95.5%, Val. checks: 8/15\n",
            "Epoch: 12/100, iter: 3720. Loss (train/val): 0.101 / 0.109. Val. acc: 95.0%, Val. checks: 9/15\n",
            "Epoch: 12/100, iter: 3730. Loss (train/val): 0.101 / 0.109. Val. acc: 95.0%, Val. checks: 10/15\n",
            "Epoch: 12/100, iter: 3740. Loss (train/val): 0.104 / 0.114. Val. acc: 95.6%, Val. checks: 11/15\n",
            "Epoch: 12/100, iter: 3750. Loss (train/val): 0.100 / 0.110. Val. acc: 95.1%, Val. checks: 12/15\n",
            "Epoch: 12/100, iter: 3760. Loss (train/val): 0.107 / 0.117. Val. acc: 95.7%, Val. checks: 13/15\n",
            "Epoch: 12/100, iter: 3770. Loss (train/val): 0.099 / 0.109. Val. acc: 95.3%, Val. checks: 14/15\n",
            "Epoch: 12/100, iter: 3780. Loss (train/val): 0.099 / 0.109. Val. acc: 95.0%, Val. checks: 15/15\n",
            "Early stopping\n",
            "Epoch finished. Elapsed time 10.0172 [s]\n",
            "\n",
            "\n",
            "\n",
            "[Beginning training of MLP at logdir \"./tarea_1_logs/exp_xentropyrun_2\"]\n",
            "\n",
            "Epoch: 1/100, iter: 0. Loss (train/val): 0.822 / 0.825. Val. acc: 50.0%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 10. Loss (train/val): 0.617 / 0.618. Val. acc: 72.1%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 20. Loss (train/val): 0.556 / 0.558. Val. acc: 81.8%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 30. Loss (train/val): 0.523 / 0.523. Val. acc: 81.8%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 40. Loss (train/val): 0.489 / 0.493. Val. acc: 80.5%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 50. Loss (train/val): 0.452 / 0.452. Val. acc: 84.2%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 60. Loss (train/val): 0.417 / 0.420. Val. acc: 85.9%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 70. Loss (train/val): 0.401 / 0.406. Val. acc: 84.6%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 80. Loss (train/val): 0.390 / 0.390. Val. acc: 85.6%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 90. Loss (train/val): 0.369 / 0.370. Val. acc: 87.0%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 100. Loss (train/val): 0.357 / 0.358. Val. acc: 87.8%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 110. Loss (train/val): 0.352 / 0.354. Val. acc: 86.1%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 120. Loss (train/val): 0.341 / 0.339. Val. acc: 88.4%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 130. Loss (train/val): 0.331 / 0.328. Val. acc: 88.5%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 140. Loss (train/val): 0.324 / 0.321. Val. acc: 87.7%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 150. Loss (train/val): 0.319 / 0.319. Val. acc: 87.3%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 160. Loss (train/val): 0.310 / 0.308. Val. acc: 88.1%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 170. Loss (train/val): 0.321 / 0.323. Val. acc: 86.6%, Val. checks: 1/15\n",
            "Epoch: 1/100, iter: 180. Loss (train/val): 0.302 / 0.302. Val. acc: 88.0%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 190. Loss (train/val): 0.322 / 0.325. Val. acc: 85.9%, Val. checks: 1/15\n",
            "Epoch: 1/100, iter: 200. Loss (train/val): 0.299 / 0.299. Val. acc: 87.3%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 210. Loss (train/val): 0.292 / 0.287. Val. acc: 88.3%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 220. Loss (train/val): 0.288 / 0.282. Val. acc: 88.9%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 230. Loss (train/val): 0.285 / 0.277. Val. acc: 89.2%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 240. Loss (train/val): 0.281 / 0.273. Val. acc: 89.7%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 250. Loss (train/val): 0.280 / 0.274. Val. acc: 89.3%, Val. checks: 1/15\n",
            "Epoch: 1/100, iter: 260. Loss (train/val): 0.279 / 0.273. Val. acc: 88.9%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 270. Loss (train/val): 0.277 / 0.266. Val. acc: 89.9%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 280. Loss (train/val): 0.272 / 0.262. Val. acc: 90.2%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 290. Loss (train/val): 0.271 / 0.261. Val. acc: 89.7%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 300. Loss (train/val): 0.268 / 0.260. Val. acc: 89.4%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 310. Loss (train/val): 0.282 / 0.278. Val. acc: 88.2%, Val. checks: 1/15\n",
            "Epoch: 1/100, iter: 320. Loss (train/val): 0.267 / 0.261. Val. acc: 89.4%, Val. checks: 2/15\n",
            "Epoch: 1/100, iter: 330. Loss (train/val): 0.265 / 0.258. Val. acc: 89.4%, Val. checks: 0/15\n",
            "Epoch finished. Elapsed time 1.0346 [s]\n",
            "\n",
            "Epoch: 2/100, iter: 340. Loss (train/val): 0.264 / 0.259. Val. acc: 89.6%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 350. Loss (train/val): 0.267 / 0.257. Val. acc: 90.0%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 360. Loss (train/val): 0.263 / 0.254. Val. acc: 90.4%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 370. Loss (train/val): 0.262 / 0.259. Val. acc: 89.6%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 380. Loss (train/val): 0.272 / 0.261. Val. acc: 90.5%, Val. checks: 2/15\n",
            "Epoch: 2/100, iter: 390. Loss (train/val): 0.262 / 0.260. Val. acc: 89.1%, Val. checks: 3/15\n",
            "Epoch: 2/100, iter: 400. Loss (train/val): 0.256 / 0.249. Val. acc: 89.5%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 410. Loss (train/val): 0.255 / 0.247. Val. acc: 90.2%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 420. Loss (train/val): 0.253 / 0.247. Val. acc: 89.5%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 430. Loss (train/val): 0.264 / 0.262. Val. acc: 88.2%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 440. Loss (train/val): 0.266 / 0.253. Val. acc: 91.0%, Val. checks: 2/15\n",
            "Epoch: 2/100, iter: 450. Loss (train/val): 0.250 / 0.243. Val. acc: 90.0%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 460. Loss (train/val): 0.252 / 0.244. Val. acc: 90.0%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 470. Loss (train/val): 0.253 / 0.241. Val. acc: 90.6%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 480. Loss (train/val): 0.249 / 0.239. Val. acc: 90.5%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 490. Loss (train/val): 0.260 / 0.246. Val. acc: 90.7%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 500. Loss (train/val): 0.246 / 0.236. Val. acc: 90.5%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 510. Loss (train/val): 0.245 / 0.235. Val. acc: 90.7%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 520. Loss (train/val): 0.255 / 0.249. Val. acc: 89.4%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 530. Loss (train/val): 0.249 / 0.241. Val. acc: 90.2%, Val. checks: 2/15\n",
            "Epoch: 2/100, iter: 540. Loss (train/val): 0.243 / 0.232. Val. acc: 91.1%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 550. Loss (train/val): 0.254 / 0.239. Val. acc: 91.8%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 560. Loss (train/val): 0.253 / 0.238. Val. acc: 91.9%, Val. checks: 2/15\n",
            "Epoch: 2/100, iter: 570. Loss (train/val): 0.283 / 0.263. Val. acc: 90.4%, Val. checks: 3/15\n",
            "Epoch: 2/100, iter: 580. Loss (train/val): 0.251 / 0.240. Val. acc: 90.0%, Val. checks: 4/15\n",
            "Epoch: 2/100, iter: 590. Loss (train/val): 0.250 / 0.241. Val. acc: 89.5%, Val. checks: 5/15\n",
            "Epoch: 2/100, iter: 600. Loss (train/val): 0.241 / 0.229. Val. acc: 91.5%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 610. Loss (train/val): 0.246 / 0.237. Val. acc: 89.6%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 620. Loss (train/val): 0.238 / 0.227. Val. acc: 91.3%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 630. Loss (train/val): 0.246 / 0.232. Val. acc: 92.2%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 640. Loss (train/val): 0.242 / 0.234. Val. acc: 89.8%, Val. checks: 2/15\n",
            "Epoch: 2/100, iter: 650. Loss (train/val): 0.236 / 0.226. Val. acc: 90.7%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 660. Loss (train/val): 0.237 / 0.230. Val. acc: 90.0%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 670. Loss (train/val): 0.235 / 0.225. Val. acc: 91.2%, Val. checks: 0/15\n",
            "Epoch finished. Elapsed time 1.9064 [s]\n",
            "\n",
            "Epoch: 3/100, iter: 680. Loss (train/val): 0.234 / 0.224. Val. acc: 91.8%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 690. Loss (train/val): 0.234 / 0.227. Val. acc: 90.9%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 700. Loss (train/val): 0.232 / 0.223. Val. acc: 91.5%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 710. Loss (train/val): 0.259 / 0.247. Val. acc: 91.3%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 720. Loss (train/val): 0.232 / 0.223. Val. acc: 91.9%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 730. Loss (train/val): 0.231 / 0.225. Val. acc: 91.2%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 740. Loss (train/val): 0.238 / 0.230. Val. acc: 92.2%, Val. checks: 2/15\n",
            "Epoch: 3/100, iter: 750. Loss (train/val): 0.232 / 0.227. Val. acc: 91.3%, Val. checks: 3/15\n",
            "Epoch: 3/100, iter: 760. Loss (train/val): 0.230 / 0.227. Val. acc: 91.1%, Val. checks: 4/15\n",
            "Epoch: 3/100, iter: 770. Loss (train/val): 0.234 / 0.231. Val. acc: 92.4%, Val. checks: 5/15\n",
            "Epoch: 3/100, iter: 780. Loss (train/val): 0.230 / 0.229. Val. acc: 90.4%, Val. checks: 6/15\n",
            "Epoch: 3/100, iter: 790. Loss (train/val): 0.235 / 0.231. Val. acc: 92.1%, Val. checks: 7/15\n",
            "Epoch: 3/100, iter: 800. Loss (train/val): 0.229 / 0.223. Val. acc: 91.8%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 810. Loss (train/val): 0.230 / 0.227. Val. acc: 90.4%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 820. Loss (train/val): 0.228 / 0.222. Val. acc: 92.4%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 830. Loss (train/val): 0.224 / 0.220. Val. acc: 91.0%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 840. Loss (train/val): 0.224 / 0.218. Val. acc: 91.0%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 850. Loss (train/val): 0.228 / 0.225. Val. acc: 90.3%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 860. Loss (train/val): 0.224 / 0.220. Val. acc: 90.7%, Val. checks: 2/15\n",
            "Epoch: 3/100, iter: 870. Loss (train/val): 0.225 / 0.220. Val. acc: 90.5%, Val. checks: 3/15\n",
            "Epoch: 3/100, iter: 880. Loss (train/val): 0.221 / 0.215. Val. acc: 91.7%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 890. Loss (train/val): 0.219 / 0.213. Val. acc: 91.8%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 900. Loss (train/val): 0.225 / 0.221. Val. acc: 90.4%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 910. Loss (train/val): 0.218 / 0.211. Val. acc: 91.6%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 920. Loss (train/val): 0.221 / 0.214. Val. acc: 92.5%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 930. Loss (train/val): 0.217 / 0.210. Val. acc: 91.8%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 940. Loss (train/val): 0.216 / 0.210. Val. acc: 92.0%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 950. Loss (train/val): 0.216 / 0.208. Val. acc: 92.0%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 960. Loss (train/val): 0.214 / 0.208. Val. acc: 91.8%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 970. Loss (train/val): 0.222 / 0.215. Val. acc: 92.4%, Val. checks: 2/15\n",
            "Epoch: 3/100, iter: 980. Loss (train/val): 0.217 / 0.209. Val. acc: 91.7%, Val. checks: 3/15\n",
            "Epoch: 3/100, iter: 990. Loss (train/val): 0.216 / 0.210. Val. acc: 91.9%, Val. checks: 4/15\n",
            "Epoch: 3/100, iter: 1000. Loss (train/val): 0.213 / 0.207. Val. acc: 92.3%, Val. checks: 0/15\n",
            "Epoch finished. Elapsed time 2.7610 [s]\n",
            "\n",
            "Epoch: 4/100, iter: 1010. Loss (train/val): 0.212 / 0.205. Val. acc: 92.1%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1020. Loss (train/val): 0.212 / 0.204. Val. acc: 92.3%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1030. Loss (train/val): 0.221 / 0.214. Val. acc: 92.6%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1040. Loss (train/val): 0.240 / 0.231. Val. acc: 92.6%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1050. Loss (train/val): 0.213 / 0.208. Val. acc: 92.5%, Val. checks: 3/15\n",
            "Epoch: 4/100, iter: 1060. Loss (train/val): 0.216 / 0.211. Val. acc: 91.1%, Val. checks: 4/15\n",
            "Epoch: 4/100, iter: 1070. Loss (train/val): 0.208 / 0.204. Val. acc: 92.1%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1080. Loss (train/val): 0.216 / 0.209. Val. acc: 93.2%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1090. Loss (train/val): 0.208 / 0.201. Val. acc: 92.0%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1100. Loss (train/val): 0.207 / 0.201. Val. acc: 92.1%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1110. Loss (train/val): 0.213 / 0.207. Val. acc: 91.3%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1120. Loss (train/val): 0.212 / 0.204. Val. acc: 91.8%, Val. checks: 3/15\n",
            "Epoch: 4/100, iter: 1130. Loss (train/val): 0.203 / 0.199. Val. acc: 92.4%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1140. Loss (train/val): 0.204 / 0.199. Val. acc: 92.6%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1150. Loss (train/val): 0.206 / 0.201. Val. acc: 91.8%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1160. Loss (train/val): 0.202 / 0.196. Val. acc: 91.9%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1170. Loss (train/val): 0.202 / 0.195. Val. acc: 92.5%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1180. Loss (train/val): 0.201 / 0.194. Val. acc: 92.0%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1190. Loss (train/val): 0.201 / 0.196. Val. acc: 92.7%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1200. Loss (train/val): 0.198 / 0.193. Val. acc: 92.6%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1210. Loss (train/val): 0.208 / 0.202. Val. acc: 91.9%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1220. Loss (train/val): 0.200 / 0.195. Val. acc: 92.9%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1230. Loss (train/val): 0.198 / 0.193. Val. acc: 92.9%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1240. Loss (train/val): 0.199 / 0.194. Val. acc: 92.3%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1250. Loss (train/val): 0.198 / 0.193. Val. acc: 92.1%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1260. Loss (train/val): 0.194 / 0.190. Val. acc: 92.3%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1270. Loss (train/val): 0.194 / 0.189. Val. acc: 92.5%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1280. Loss (train/val): 0.194 / 0.188. Val. acc: 92.7%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1290. Loss (train/val): 0.194 / 0.189. Val. acc: 92.6%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1300. Loss (train/val): 0.192 / 0.188. Val. acc: 93.3%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1310. Loss (train/val): 0.191 / 0.187. Val. acc: 93.0%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1320. Loss (train/val): 0.196 / 0.193. Val. acc: 93.1%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1330. Loss (train/val): 0.190 / 0.186. Val. acc: 92.6%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1340. Loss (train/val): 0.190 / 0.188. Val. acc: 93.4%, Val. checks: 1/15\n",
            "Epoch finished. Elapsed time 3.6480 [s]\n",
            "\n",
            "Epoch: 5/100, iter: 1350. Loss (train/val): 0.194 / 0.193. Val. acc: 93.0%, Val. checks: 2/15\n",
            "Epoch: 5/100, iter: 1360. Loss (train/val): 0.188 / 0.185. Val. acc: 92.7%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1370. Loss (train/val): 0.191 / 0.186. Val. acc: 92.5%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1380. Loss (train/val): 0.194 / 0.191. Val. acc: 93.6%, Val. checks: 2/15\n",
            "Epoch: 5/100, iter: 1390. Loss (train/val): 0.188 / 0.186. Val. acc: 93.4%, Val. checks: 3/15\n",
            "Epoch: 5/100, iter: 1400. Loss (train/val): 0.190 / 0.186. Val. acc: 92.3%, Val. checks: 4/15\n",
            "Epoch: 5/100, iter: 1410. Loss (train/val): 0.189 / 0.188. Val. acc: 93.4%, Val. checks: 5/15\n",
            "Epoch: 5/100, iter: 1420. Loss (train/val): 0.186 / 0.187. Val. acc: 93.4%, Val. checks: 6/15\n",
            "Epoch: 5/100, iter: 1430. Loss (train/val): 0.184 / 0.185. Val. acc: 93.6%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1440. Loss (train/val): 0.184 / 0.183. Val. acc: 92.3%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1450. Loss (train/val): 0.184 / 0.183. Val. acc: 93.4%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1460. Loss (train/val): 0.185 / 0.185. Val. acc: 93.1%, Val. checks: 2/15\n",
            "Epoch: 5/100, iter: 1470. Loss (train/val): 0.180 / 0.179. Val. acc: 92.7%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1480. Loss (train/val): 0.180 / 0.179. Val. acc: 93.2%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1490. Loss (train/val): 0.181 / 0.178. Val. acc: 93.0%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1500. Loss (train/val): 0.182 / 0.178. Val. acc: 92.9%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1510. Loss (train/val): 0.179 / 0.175. Val. acc: 93.4%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1520. Loss (train/val): 0.177 / 0.175. Val. acc: 93.3%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1530. Loss (train/val): 0.180 / 0.179. Val. acc: 93.1%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1540. Loss (train/val): 0.177 / 0.175. Val. acc: 93.4%, Val. checks: 2/15\n",
            "Epoch: 5/100, iter: 1550. Loss (train/val): 0.177 / 0.176. Val. acc: 93.7%, Val. checks: 3/15\n",
            "Epoch: 5/100, iter: 1560. Loss (train/val): 0.175 / 0.173. Val. acc: 93.4%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1570. Loss (train/val): 0.174 / 0.172. Val. acc: 93.6%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1580. Loss (train/val): 0.175 / 0.173. Val. acc: 93.6%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1590. Loss (train/val): 0.176 / 0.176. Val. acc: 93.2%, Val. checks: 2/15\n",
            "Epoch: 5/100, iter: 1600. Loss (train/val): 0.173 / 0.173. Val. acc: 93.4%, Val. checks: 3/15\n",
            "Epoch: 5/100, iter: 1610. Loss (train/val): 0.180 / 0.178. Val. acc: 92.9%, Val. checks: 4/15\n",
            "Epoch: 5/100, iter: 1620. Loss (train/val): 0.173 / 0.172. Val. acc: 93.5%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1630. Loss (train/val): 0.173 / 0.172. Val. acc: 93.6%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1640. Loss (train/val): 0.172 / 0.171. Val. acc: 93.2%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1650. Loss (train/val): 0.173 / 0.169. Val. acc: 93.3%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1660. Loss (train/val): 0.169 / 0.166. Val. acc: 93.7%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1670. Loss (train/val): 0.170 / 0.168. Val. acc: 94.2%, Val. checks: 1/15\n",
            "Epoch finished. Elapsed time 4.4922 [s]\n",
            "\n",
            "Epoch: 6/100, iter: 1680. Loss (train/val): 0.168 / 0.167. Val. acc: 94.0%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 1690. Loss (train/val): 0.167 / 0.165. Val. acc: 93.5%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1700. Loss (train/val): 0.169 / 0.166. Val. acc: 93.5%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1710. Loss (train/val): 0.167 / 0.165. Val. acc: 93.4%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1720. Loss (train/val): 0.171 / 0.168. Val. acc: 93.5%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1730. Loss (train/val): 0.168 / 0.168. Val. acc: 94.2%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 1740. Loss (train/val): 0.164 / 0.161. Val. acc: 93.9%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1750. Loss (train/val): 0.164 / 0.160. Val. acc: 93.9%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1760. Loss (train/val): 0.165 / 0.161. Val. acc: 93.6%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1770. Loss (train/val): 0.164 / 0.160. Val. acc: 93.8%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1780. Loss (train/val): 0.171 / 0.169. Val. acc: 94.0%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1790. Loss (train/val): 0.163 / 0.160. Val. acc: 93.6%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 1800. Loss (train/val): 0.163 / 0.160. Val. acc: 93.7%, Val. checks: 3/15\n",
            "Epoch: 6/100, iter: 1810. Loss (train/val): 0.162 / 0.159. Val. acc: 93.5%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1820. Loss (train/val): 0.163 / 0.159. Val. acc: 94.2%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1830. Loss (train/val): 0.162 / 0.159. Val. acc: 93.6%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1840. Loss (train/val): 0.160 / 0.158. Val. acc: 93.6%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1850. Loss (train/val): 0.159 / 0.158. Val. acc: 93.8%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1860. Loss (train/val): 0.163 / 0.161. Val. acc: 93.4%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 1870. Loss (train/val): 0.165 / 0.165. Val. acc: 94.3%, Val. checks: 3/15\n",
            "Epoch: 6/100, iter: 1880. Loss (train/val): 0.159 / 0.156. Val. acc: 93.6%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1890. Loss (train/val): 0.157 / 0.155. Val. acc: 93.9%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1900. Loss (train/val): 0.157 / 0.154. Val. acc: 93.8%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1910. Loss (train/val): 0.157 / 0.157. Val. acc: 93.7%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1920. Loss (train/val): 0.156 / 0.157. Val. acc: 94.2%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 1930. Loss (train/val): 0.170 / 0.166. Val. acc: 93.2%, Val. checks: 3/15\n",
            "Epoch: 6/100, iter: 1940. Loss (train/val): 0.156 / 0.155. Val. acc: 94.6%, Val. checks: 4/15\n",
            "Epoch: 6/100, iter: 1950. Loss (train/val): 0.160 / 0.161. Val. acc: 94.6%, Val. checks: 5/15\n",
            "Epoch: 6/100, iter: 1960. Loss (train/val): 0.156 / 0.157. Val. acc: 94.8%, Val. checks: 6/15\n",
            "Epoch: 6/100, iter: 1970. Loss (train/val): 0.154 / 0.154. Val. acc: 94.6%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1980. Loss (train/val): 0.154 / 0.152. Val. acc: 93.6%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1990. Loss (train/val): 0.156 / 0.159. Val. acc: 94.3%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 2000. Loss (train/val): 0.153 / 0.153. Val. acc: 93.9%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 2010. Loss (train/val): 0.152 / 0.153. Val. acc: 94.2%, Val. checks: 3/15\n",
            "Epoch finished. Elapsed time 5.3730 [s]\n",
            "\n",
            "Epoch: 7/100, iter: 2020. Loss (train/val): 0.151 / 0.149. Val. acc: 93.8%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2030. Loss (train/val): 0.151 / 0.151. Val. acc: 94.6%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2040. Loss (train/val): 0.149 / 0.149. Val. acc: 94.2%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2050. Loss (train/val): 0.149 / 0.148. Val. acc: 94.0%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2060. Loss (train/val): 0.148 / 0.149. Val. acc: 94.5%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2070. Loss (train/val): 0.147 / 0.147. Val. acc: 93.9%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2080. Loss (train/val): 0.147 / 0.148. Val. acc: 94.3%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2090. Loss (train/val): 0.148 / 0.150. Val. acc: 94.6%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2100. Loss (train/val): 0.149 / 0.152. Val. acc: 94.6%, Val. checks: 3/15\n",
            "Epoch: 7/100, iter: 2110. Loss (train/val): 0.149 / 0.149. Val. acc: 93.4%, Val. checks: 4/15\n",
            "Epoch: 7/100, iter: 2120. Loss (train/val): 0.147 / 0.147. Val. acc: 93.7%, Val. checks: 5/15\n",
            "Epoch: 7/100, iter: 2130. Loss (train/val): 0.146 / 0.148. Val. acc: 93.6%, Val. checks: 6/15\n",
            "Epoch: 7/100, iter: 2140. Loss (train/val): 0.146 / 0.148. Val. acc: 94.6%, Val. checks: 7/15\n",
            "Epoch: 7/100, iter: 2150. Loss (train/val): 0.145 / 0.148. Val. acc: 94.5%, Val. checks: 8/15\n",
            "Epoch: 7/100, iter: 2160. Loss (train/val): 0.147 / 0.147. Val. acc: 93.9%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2170. Loss (train/val): 0.143 / 0.143. Val. acc: 94.2%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2180. Loss (train/val): 0.146 / 0.145. Val. acc: 94.0%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2190. Loss (train/val): 0.143 / 0.142. Val. acc: 94.5%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2200. Loss (train/val): 0.142 / 0.141. Val. acc: 94.5%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2210. Loss (train/val): 0.142 / 0.143. Val. acc: 94.5%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2220. Loss (train/val): 0.142 / 0.142. Val. acc: 94.5%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2230. Loss (train/val): 0.140 / 0.139. Val. acc: 94.6%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2240. Loss (train/val): 0.140 / 0.138. Val. acc: 94.3%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2250. Loss (train/val): 0.139 / 0.138. Val. acc: 94.7%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2260. Loss (train/val): 0.149 / 0.146. Val. acc: 93.7%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2270. Loss (train/val): 0.140 / 0.137. Val. acc: 94.6%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2280. Loss (train/val): 0.139 / 0.137. Val. acc: 94.4%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2290. Loss (train/val): 0.138 / 0.139. Val. acc: 95.1%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2300. Loss (train/val): 0.137 / 0.139. Val. acc: 94.6%, Val. checks: 3/15\n",
            "Epoch: 7/100, iter: 2310. Loss (train/val): 0.140 / 0.142. Val. acc: 94.6%, Val. checks: 4/15\n",
            "Epoch: 7/100, iter: 2320. Loss (train/val): 0.136 / 0.136. Val. acc: 94.8%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2330. Loss (train/val): 0.136 / 0.137. Val. acc: 94.5%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2340. Loss (train/val): 0.139 / 0.142. Val. acc: 95.0%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2350. Loss (train/val): 0.135 / 0.134. Val. acc: 94.8%, Val. checks: 0/15\n",
            "Epoch finished. Elapsed time 6.3396 [s]\n",
            "\n",
            "Epoch: 8/100, iter: 2360. Loss (train/val): 0.144 / 0.140. Val. acc: 94.2%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2370. Loss (train/val): 0.137 / 0.137. Val. acc: 95.0%, Val. checks: 2/15\n",
            "Epoch: 8/100, iter: 2380. Loss (train/val): 0.134 / 0.132. Val. acc: 95.0%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2390. Loss (train/val): 0.138 / 0.138. Val. acc: 95.1%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2400. Loss (train/val): 0.133 / 0.131. Val. acc: 94.7%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2410. Loss (train/val): 0.133 / 0.132. Val. acc: 94.8%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2420. Loss (train/val): 0.133 / 0.131. Val. acc: 95.0%, Val. checks: 2/15\n",
            "Epoch: 8/100, iter: 2430. Loss (train/val): 0.134 / 0.133. Val. acc: 94.7%, Val. checks: 3/15\n",
            "Epoch: 8/100, iter: 2440. Loss (train/val): 0.133 / 0.131. Val. acc: 94.7%, Val. checks: 4/15\n",
            "Epoch: 8/100, iter: 2450. Loss (train/val): 0.133 / 0.131. Val. acc: 94.8%, Val. checks: 5/15\n",
            "Epoch: 8/100, iter: 2460. Loss (train/val): 0.132 / 0.133. Val. acc: 95.0%, Val. checks: 6/15\n",
            "Epoch: 8/100, iter: 2470. Loss (train/val): 0.139 / 0.137. Val. acc: 94.7%, Val. checks: 7/15\n",
            "Epoch: 8/100, iter: 2480. Loss (train/val): 0.134 / 0.135. Val. acc: 94.7%, Val. checks: 8/15\n",
            "Epoch: 8/100, iter: 2490. Loss (train/val): 0.138 / 0.134. Val. acc: 94.8%, Val. checks: 9/15\n",
            "Epoch: 8/100, iter: 2500. Loss (train/val): 0.130 / 0.128. Val. acc: 94.9%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2510. Loss (train/val): 0.133 / 0.134. Val. acc: 95.3%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2520. Loss (train/val): 0.128 / 0.127. Val. acc: 95.1%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2530. Loss (train/val): 0.131 / 0.127. Val. acc: 94.9%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2540. Loss (train/val): 0.127 / 0.126. Val. acc: 94.6%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2550. Loss (train/val): 0.128 / 0.128. Val. acc: 95.1%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2560. Loss (train/val): 0.127 / 0.126. Val. acc: 94.9%, Val. checks: 2/15\n",
            "Epoch: 8/100, iter: 2570. Loss (train/val): 0.129 / 0.131. Val. acc: 94.9%, Val. checks: 3/15\n",
            "Epoch: 8/100, iter: 2580. Loss (train/val): 0.135 / 0.137. Val. acc: 95.6%, Val. checks: 4/15\n",
            "Epoch: 8/100, iter: 2590. Loss (train/val): 0.127 / 0.126. Val. acc: 94.8%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2600. Loss (train/val): 0.126 / 0.127. Val. acc: 94.8%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2610. Loss (train/val): 0.125 / 0.125. Val. acc: 95.0%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2620. Loss (train/val): 0.124 / 0.124. Val. acc: 95.1%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2630. Loss (train/val): 0.126 / 0.125. Val. acc: 94.5%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2640. Loss (train/val): 0.125 / 0.125. Val. acc: 94.9%, Val. checks: 2/15\n",
            "Epoch: 8/100, iter: 2650. Loss (train/val): 0.125 / 0.125. Val. acc: 94.8%, Val. checks: 3/15\n",
            "Epoch: 8/100, iter: 2660. Loss (train/val): 0.124 / 0.122. Val. acc: 95.0%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2670. Loss (train/val): 0.129 / 0.125. Val. acc: 94.8%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2680. Loss (train/val): 0.122 / 0.121. Val. acc: 95.1%, Val. checks: 0/15\n",
            "Epoch finished. Elapsed time 7.2056 [s]\n",
            "\n",
            "Epoch: 9/100, iter: 2690. Loss (train/val): 0.126 / 0.124. Val. acc: 94.8%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2700. Loss (train/val): 0.123 / 0.125. Val. acc: 95.1%, Val. checks: 2/15\n",
            "Epoch: 9/100, iter: 2710. Loss (train/val): 0.122 / 0.124. Val. acc: 95.0%, Val. checks: 3/15\n",
            "Epoch: 9/100, iter: 2720. Loss (train/val): 0.130 / 0.129. Val. acc: 94.6%, Val. checks: 4/15\n",
            "Epoch: 9/100, iter: 2730. Loss (train/val): 0.125 / 0.127. Val. acc: 95.3%, Val. checks: 5/15\n",
            "Epoch: 9/100, iter: 2740. Loss (train/val): 0.123 / 0.123. Val. acc: 95.5%, Val. checks: 6/15\n",
            "Epoch: 9/100, iter: 2750. Loss (train/val): 0.121 / 0.120. Val. acc: 95.1%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2760. Loss (train/val): 0.120 / 0.121. Val. acc: 95.0%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2770. Loss (train/val): 0.123 / 0.123. Val. acc: 94.9%, Val. checks: 2/15\n",
            "Epoch: 9/100, iter: 2780. Loss (train/val): 0.120 / 0.121. Val. acc: 95.2%, Val. checks: 3/15\n",
            "Epoch: 9/100, iter: 2790. Loss (train/val): 0.125 / 0.129. Val. acc: 95.0%, Val. checks: 4/15\n",
            "Epoch: 9/100, iter: 2800. Loss (train/val): 0.119 / 0.119. Val. acc: 95.1%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2810. Loss (train/val): 0.119 / 0.118. Val. acc: 95.5%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2820. Loss (train/val): 0.119 / 0.119. Val. acc: 95.5%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2830. Loss (train/val): 0.119 / 0.121. Val. acc: 95.6%, Val. checks: 2/15\n",
            "Epoch: 9/100, iter: 2840. Loss (train/val): 0.121 / 0.123. Val. acc: 95.3%, Val. checks: 3/15\n",
            "Epoch: 9/100, iter: 2850. Loss (train/val): 0.120 / 0.120. Val. acc: 94.9%, Val. checks: 4/15\n",
            "Epoch: 9/100, iter: 2860. Loss (train/val): 0.118 / 0.119. Val. acc: 95.5%, Val. checks: 5/15\n",
            "Epoch: 9/100, iter: 2870. Loss (train/val): 0.119 / 0.118. Val. acc: 94.9%, Val. checks: 6/15\n",
            "Epoch: 9/100, iter: 2880. Loss (train/val): 0.118 / 0.117. Val. acc: 95.5%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2890. Loss (train/val): 0.117 / 0.117. Val. acc: 95.2%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2900. Loss (train/val): 0.119 / 0.118. Val. acc: 94.8%, Val. checks: 2/15\n",
            "Epoch: 9/100, iter: 2910. Loss (train/val): 0.119 / 0.121. Val. acc: 95.3%, Val. checks: 3/15\n",
            "Epoch: 9/100, iter: 2920. Loss (train/val): 0.121 / 0.123. Val. acc: 95.3%, Val. checks: 4/15\n",
            "Epoch: 9/100, iter: 2930. Loss (train/val): 0.118 / 0.118. Val. acc: 94.8%, Val. checks: 5/15\n",
            "Epoch: 9/100, iter: 2940. Loss (train/val): 0.115 / 0.117. Val. acc: 95.2%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2950. Loss (train/val): 0.114 / 0.114. Val. acc: 95.3%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2960. Loss (train/val): 0.114 / 0.115. Val. acc: 95.5%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2970. Loss (train/val): 0.117 / 0.118. Val. acc: 95.5%, Val. checks: 2/15\n",
            "Epoch: 9/100, iter: 2980. Loss (train/val): 0.114 / 0.113. Val. acc: 95.5%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2990. Loss (train/val): 0.114 / 0.113. Val. acc: 95.6%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 3000. Loss (train/val): 0.114 / 0.115. Val. acc: 95.7%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 3010. Loss (train/val): 0.113 / 0.113. Val. acc: 95.6%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 3020. Loss (train/val): 0.113 / 0.112. Val. acc: 95.6%, Val. checks: 0/15\n",
            "Epoch finished. Elapsed time 8.0880 [s]\n",
            "\n",
            "Epoch: 10/100, iter: 3030. Loss (train/val): 0.113 / 0.114. Val. acc: 95.8%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3040. Loss (train/val): 0.113 / 0.115. Val. acc: 95.7%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3050. Loss (train/val): 0.112 / 0.111. Val. acc: 95.7%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3060. Loss (train/val): 0.113 / 0.114. Val. acc: 95.6%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3070. Loss (train/val): 0.114 / 0.117. Val. acc: 95.2%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3080. Loss (train/val): 0.111 / 0.113. Val. acc: 95.5%, Val. checks: 3/15\n",
            "Epoch: 10/100, iter: 3090. Loss (train/val): 0.112 / 0.117. Val. acc: 95.5%, Val. checks: 4/15\n",
            "Epoch: 10/100, iter: 3100. Loss (train/val): 0.111 / 0.114. Val. acc: 95.6%, Val. checks: 5/15\n",
            "Epoch: 10/100, iter: 3110. Loss (train/val): 0.110 / 0.111. Val. acc: 95.3%, Val. checks: 6/15\n",
            "Epoch: 10/100, iter: 3120. Loss (train/val): 0.110 / 0.110. Val. acc: 95.6%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3130. Loss (train/val): 0.109 / 0.111. Val. acc: 95.6%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3140. Loss (train/val): 0.109 / 0.112. Val. acc: 95.5%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3150. Loss (train/val): 0.111 / 0.115. Val. acc: 95.6%, Val. checks: 3/15\n",
            "Epoch: 10/100, iter: 3160. Loss (train/val): 0.109 / 0.112. Val. acc: 95.3%, Val. checks: 4/15\n",
            "Epoch: 10/100, iter: 3170. Loss (train/val): 0.109 / 0.113. Val. acc: 95.2%, Val. checks: 5/15\n",
            "Epoch: 10/100, iter: 3180. Loss (train/val): 0.110 / 0.112. Val. acc: 95.6%, Val. checks: 6/15\n",
            "Epoch: 10/100, iter: 3190. Loss (train/val): 0.108 / 0.110. Val. acc: 95.3%, Val. checks: 7/15\n",
            "Epoch: 10/100, iter: 3200. Loss (train/val): 0.108 / 0.109. Val. acc: 95.6%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3210. Loss (train/val): 0.109 / 0.110. Val. acc: 95.5%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3220. Loss (train/val): 0.110 / 0.110. Val. acc: 95.5%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3230. Loss (train/val): 0.108 / 0.109. Val. acc: 95.6%, Val. checks: 3/15\n",
            "Epoch: 10/100, iter: 3240. Loss (train/val): 0.124 / 0.132. Val. acc: 95.5%, Val. checks: 4/15\n",
            "Epoch: 10/100, iter: 3250. Loss (train/val): 0.107 / 0.111. Val. acc: 95.5%, Val. checks: 5/15\n",
            "Epoch: 10/100, iter: 3260. Loss (train/val): 0.108 / 0.109. Val. acc: 95.8%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3270. Loss (train/val): 0.107 / 0.108. Val. acc: 95.7%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3280. Loss (train/val): 0.109 / 0.111. Val. acc: 95.6%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3290. Loss (train/val): 0.106 / 0.109. Val. acc: 95.7%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3300. Loss (train/val): 0.105 / 0.107. Val. acc: 95.8%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3310. Loss (train/val): 0.110 / 0.113. Val. acc: 96.0%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3320. Loss (train/val): 0.108 / 0.111. Val. acc: 95.8%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3330. Loss (train/val): 0.107 / 0.109. Val. acc: 96.0%, Val. checks: 3/15\n",
            "Epoch: 10/100, iter: 3340. Loss (train/val): 0.104 / 0.105. Val. acc: 95.8%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3350. Loss (train/val): 0.114 / 0.114. Val. acc: 95.3%, Val. checks: 1/15\n",
            "Epoch finished. Elapsed time 8.9525 [s]\n",
            "\n",
            "Epoch: 11/100, iter: 3360. Loss (train/val): 0.104 / 0.107. Val. acc: 95.9%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3370. Loss (train/val): 0.103 / 0.105. Val. acc: 96.0%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3380. Loss (train/val): 0.103 / 0.105. Val. acc: 96.1%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3390. Loss (train/val): 0.103 / 0.106. Val. acc: 95.9%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3400. Loss (train/val): 0.103 / 0.106. Val. acc: 95.8%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3410. Loss (train/val): 0.102 / 0.104. Val. acc: 95.9%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3420. Loss (train/val): 0.104 / 0.107. Val. acc: 95.8%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3430. Loss (train/val): 0.103 / 0.104. Val. acc: 95.8%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3440. Loss (train/val): 0.104 / 0.104. Val. acc: 96.0%, Val. checks: 3/15\n",
            "Epoch: 11/100, iter: 3450. Loss (train/val): 0.102 / 0.103. Val. acc: 95.9%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3460. Loss (train/val): 0.102 / 0.103. Val. acc: 96.1%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3470. Loss (train/val): 0.104 / 0.105. Val. acc: 95.9%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3480. Loss (train/val): 0.102 / 0.107. Val. acc: 95.8%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3490. Loss (train/val): 0.102 / 0.105. Val. acc: 95.8%, Val. checks: 3/15\n",
            "Epoch: 11/100, iter: 3500. Loss (train/val): 0.100 / 0.102. Val. acc: 95.9%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3510. Loss (train/val): 0.102 / 0.101. Val. acc: 95.9%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3520. Loss (train/val): 0.099 / 0.101. Val. acc: 96.2%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3530. Loss (train/val): 0.103 / 0.106. Val. acc: 95.9%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3540. Loss (train/val): 0.099 / 0.099. Val. acc: 95.8%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3550. Loss (train/val): 0.100 / 0.100. Val. acc: 95.7%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3560. Loss (train/val): 0.099 / 0.102. Val. acc: 96.0%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3570. Loss (train/val): 0.100 / 0.104. Val. acc: 96.1%, Val. checks: 3/15\n",
            "Epoch: 11/100, iter: 3580. Loss (train/val): 0.099 / 0.102. Val. acc: 96.1%, Val. checks: 4/15\n",
            "Epoch: 11/100, iter: 3590. Loss (train/val): 0.099 / 0.101. Val. acc: 95.8%, Val. checks: 5/15\n",
            "Epoch: 11/100, iter: 3600. Loss (train/val): 0.098 / 0.100. Val. acc: 96.2%, Val. checks: 6/15\n",
            "Epoch: 11/100, iter: 3610. Loss (train/val): 0.097 / 0.101. Val. acc: 96.0%, Val. checks: 7/15\n",
            "Epoch: 11/100, iter: 3620. Loss (train/val): 0.097 / 0.100. Val. acc: 95.9%, Val. checks: 8/15\n",
            "Epoch: 11/100, iter: 3630. Loss (train/val): 0.097 / 0.101. Val. acc: 96.2%, Val. checks: 9/15\n",
            "Epoch: 11/100, iter: 3640. Loss (train/val): 0.098 / 0.102. Val. acc: 96.3%, Val. checks: 10/15\n",
            "Epoch: 11/100, iter: 3650. Loss (train/val): 0.106 / 0.108. Val. acc: 95.7%, Val. checks: 11/15\n",
            "Epoch: 11/100, iter: 3660. Loss (train/val): 0.097 / 0.102. Val. acc: 95.9%, Val. checks: 12/15\n",
            "Epoch: 11/100, iter: 3670. Loss (train/val): 0.101 / 0.103. Val. acc: 96.3%, Val. checks: 13/15\n",
            "Epoch: 11/100, iter: 3680. Loss (train/val): 0.096 / 0.099. Val. acc: 95.9%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3690. Loss (train/val): 0.096 / 0.100. Val. acc: 95.9%, Val. checks: 1/15\n",
            "Epoch finished. Elapsed time 9.8272 [s]\n",
            "\n",
            "Epoch: 12/100, iter: 3700. Loss (train/val): 0.104 / 0.111. Val. acc: 95.8%, Val. checks: 2/15\n",
            "Epoch: 12/100, iter: 3710. Loss (train/val): 0.096 / 0.101. Val. acc: 96.0%, Val. checks: 3/15\n",
            "Epoch: 12/100, iter: 3720. Loss (train/val): 0.096 / 0.098. Val. acc: 96.1%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3730. Loss (train/val): 0.095 / 0.098. Val. acc: 96.2%, Val. checks: 1/15\n",
            "Epoch: 12/100, iter: 3740. Loss (train/val): 0.096 / 0.101. Val. acc: 95.7%, Val. checks: 2/15\n",
            "Epoch: 12/100, iter: 3750. Loss (train/val): 0.094 / 0.098. Val. acc: 96.1%, Val. checks: 3/15\n",
            "Epoch: 12/100, iter: 3760. Loss (train/val): 0.094 / 0.098. Val. acc: 96.2%, Val. checks: 4/15\n",
            "Epoch: 12/100, iter: 3770. Loss (train/val): 0.094 / 0.097. Val. acc: 96.1%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3780. Loss (train/val): 0.095 / 0.099. Val. acc: 96.0%, Val. checks: 1/15\n",
            "Epoch: 12/100, iter: 3790. Loss (train/val): 0.101 / 0.107. Val. acc: 96.3%, Val. checks: 2/15\n",
            "Epoch: 12/100, iter: 3800. Loss (train/val): 0.095 / 0.100. Val. acc: 95.9%, Val. checks: 3/15\n",
            "Epoch: 12/100, iter: 3810. Loss (train/val): 0.094 / 0.098. Val. acc: 96.2%, Val. checks: 4/15\n",
            "Epoch: 12/100, iter: 3820. Loss (train/val): 0.094 / 0.099. Val. acc: 95.9%, Val. checks: 5/15\n",
            "Epoch: 12/100, iter: 3830. Loss (train/val): 0.093 / 0.098. Val. acc: 96.1%, Val. checks: 6/15\n",
            "Epoch: 12/100, iter: 3840. Loss (train/val): 0.093 / 0.096. Val. acc: 96.4%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3850. Loss (train/val): 0.094 / 0.096. Val. acc: 96.2%, Val. checks: 1/15\n",
            "Epoch: 12/100, iter: 3860. Loss (train/val): 0.093 / 0.096. Val. acc: 96.2%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3870. Loss (train/val): 0.093 / 0.095. Val. acc: 96.2%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3880. Loss (train/val): 0.092 / 0.096. Val. acc: 96.0%, Val. checks: 1/15\n",
            "Epoch: 12/100, iter: 3890. Loss (train/val): 0.092 / 0.095. Val. acc: 96.3%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3900. Loss (train/val): 0.094 / 0.095. Val. acc: 96.5%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3910. Loss (train/val): 0.094 / 0.096. Val. acc: 96.4%, Val. checks: 1/15\n",
            "Epoch: 12/100, iter: 3920. Loss (train/val): 0.093 / 0.095. Val. acc: 96.5%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3930. Loss (train/val): 0.093 / 0.098. Val. acc: 96.3%, Val. checks: 1/15\n",
            "Epoch: 12/100, iter: 3940. Loss (train/val): 0.091 / 0.094. Val. acc: 96.4%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3950. Loss (train/val): 0.091 / 0.095. Val. acc: 96.2%, Val. checks: 1/15\n",
            "Epoch: 12/100, iter: 3960. Loss (train/val): 0.091 / 0.094. Val. acc: 96.3%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3970. Loss (train/val): 0.090 / 0.093. Val. acc: 96.1%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3980. Loss (train/val): 0.091 / 0.096. Val. acc: 96.1%, Val. checks: 1/15\n",
            "Epoch: 12/100, iter: 3990. Loss (train/val): 0.090 / 0.096. Val. acc: 95.9%, Val. checks: 2/15\n",
            "Epoch: 12/100, iter: 4000. Loss (train/val): 0.099 / 0.101. Val. acc: 96.1%, Val. checks: 3/15\n",
            "Epoch: 12/100, iter: 4010. Loss (train/val): 0.109 / 0.109. Val. acc: 96.0%, Val. checks: 4/15\n",
            "Epoch: 12/100, iter: 4020. Loss (train/val): 0.091 / 0.093. Val. acc: 96.5%, Val. checks: 5/15\n",
            "Epoch: 12/100, iter: 4030. Loss (train/val): 0.091 / 0.094. Val. acc: 96.6%, Val. checks: 6/15\n",
            "Epoch finished. Elapsed time 10.7031 [s]\n",
            "\n",
            "Epoch: 13/100, iter: 4040. Loss (train/val): 0.090 / 0.094. Val. acc: 96.5%, Val. checks: 7/15\n",
            "Epoch: 13/100, iter: 4050. Loss (train/val): 0.099 / 0.107. Val. acc: 95.7%, Val. checks: 8/15\n",
            "Epoch: 13/100, iter: 4060. Loss (train/val): 0.089 / 0.093. Val. acc: 96.2%, Val. checks: 0/15\n",
            "Epoch: 13/100, iter: 4070. Loss (train/val): 0.092 / 0.094. Val. acc: 96.2%, Val. checks: 1/15\n",
            "Epoch: 13/100, iter: 4080. Loss (train/val): 0.088 / 0.092. Val. acc: 96.6%, Val. checks: 0/15\n",
            "Epoch: 13/100, iter: 4090. Loss (train/val): 0.090 / 0.096. Val. acc: 96.1%, Val. checks: 1/15\n",
            "Epoch: 13/100, iter: 4100. Loss (train/val): 0.088 / 0.092. Val. acc: 96.4%, Val. checks: 2/15\n",
            "Epoch: 13/100, iter: 4110. Loss (train/val): 0.088 / 0.092. Val. acc: 96.4%, Val. checks: 3/15\n",
            "Epoch: 13/100, iter: 4120. Loss (train/val): 0.088 / 0.091. Val. acc: 96.3%, Val. checks: 0/15\n",
            "Epoch: 13/100, iter: 4130. Loss (train/val): 0.089 / 0.095. Val. acc: 96.1%, Val. checks: 1/15\n",
            "Epoch: 13/100, iter: 4140. Loss (train/val): 0.087 / 0.092. Val. acc: 96.4%, Val. checks: 2/15\n",
            "Epoch: 13/100, iter: 4150. Loss (train/val): 0.089 / 0.097. Val. acc: 96.1%, Val. checks: 3/15\n",
            "Epoch: 13/100, iter: 4160. Loss (train/val): 0.087 / 0.092. Val. acc: 96.2%, Val. checks: 4/15\n",
            "Epoch: 13/100, iter: 4170. Loss (train/val): 0.087 / 0.090. Val. acc: 96.5%, Val. checks: 0/15\n",
            "Epoch: 13/100, iter: 4180. Loss (train/val): 0.088 / 0.096. Val. acc: 96.3%, Val. checks: 1/15\n",
            "Epoch: 13/100, iter: 4190. Loss (train/val): 0.086 / 0.093. Val. acc: 96.3%, Val. checks: 2/15\n",
            "Epoch: 13/100, iter: 4200. Loss (train/val): 0.088 / 0.092. Val. acc: 96.2%, Val. checks: 3/15\n",
            "Epoch: 13/100, iter: 4210. Loss (train/val): 0.086 / 0.091. Val. acc: 96.3%, Val. checks: 4/15\n",
            "Epoch: 13/100, iter: 4220. Loss (train/val): 0.088 / 0.095. Val. acc: 96.2%, Val. checks: 5/15\n",
            "Epoch: 13/100, iter: 4230. Loss (train/val): 0.087 / 0.091. Val. acc: 96.4%, Val. checks: 6/15\n",
            "Epoch: 13/100, iter: 4240. Loss (train/val): 0.089 / 0.094. Val. acc: 96.4%, Val. checks: 7/15\n",
            "Epoch: 13/100, iter: 4250. Loss (train/val): 0.088 / 0.096. Val. acc: 96.3%, Val. checks: 8/15\n",
            "Epoch: 13/100, iter: 4260. Loss (train/val): 0.085 / 0.091. Val. acc: 96.4%, Val. checks: 9/15\n",
            "Epoch: 13/100, iter: 4270. Loss (train/val): 0.086 / 0.091. Val. acc: 96.5%, Val. checks: 10/15\n",
            "Epoch: 13/100, iter: 4280. Loss (train/val): 0.087 / 0.095. Val. acc: 96.5%, Val. checks: 11/15\n",
            "Epoch: 13/100, iter: 4290. Loss (train/val): 0.086 / 0.093. Val. acc: 96.2%, Val. checks: 12/15\n",
            "Epoch: 13/100, iter: 4300. Loss (train/val): 0.084 / 0.090. Val. acc: 96.2%, Val. checks: 0/15\n",
            "Epoch: 13/100, iter: 4310. Loss (train/val): 0.084 / 0.091. Val. acc: 96.1%, Val. checks: 1/15\n",
            "Epoch: 13/100, iter: 4320. Loss (train/val): 0.086 / 0.090. Val. acc: 96.5%, Val. checks: 0/15\n",
            "Epoch: 13/100, iter: 4330. Loss (train/val): 0.084 / 0.089. Val. acc: 96.5%, Val. checks: 0/15\n",
            "Epoch: 13/100, iter: 4340. Loss (train/val): 0.085 / 0.089. Val. acc: 96.8%, Val. checks: 0/15\n",
            "Epoch: 13/100, iter: 4350. Loss (train/val): 0.085 / 0.093. Val. acc: 96.4%, Val. checks: 1/15\n",
            "Epoch: 13/100, iter: 4360. Loss (train/val): 0.083 / 0.089. Val. acc: 96.2%, Val. checks: 0/15\n",
            "Epoch finished. Elapsed time 11.6664 [s]\n",
            "\n",
            "Epoch: 14/100, iter: 4370. Loss (train/val): 0.083 / 0.089. Val. acc: 96.3%, Val. checks: 0/15\n",
            "Epoch: 14/100, iter: 4380. Loss (train/val): 0.085 / 0.091. Val. acc: 96.5%, Val. checks: 1/15\n",
            "Epoch: 14/100, iter: 4390. Loss (train/val): 0.083 / 0.089. Val. acc: 96.5%, Val. checks: 2/15\n",
            "Epoch: 14/100, iter: 4400. Loss (train/val): 0.085 / 0.090. Val. acc: 96.5%, Val. checks: 3/15\n",
            "Epoch: 14/100, iter: 4410. Loss (train/val): 0.082 / 0.088. Val. acc: 96.4%, Val. checks: 0/15\n",
            "Epoch: 14/100, iter: 4420. Loss (train/val): 0.086 / 0.091. Val. acc: 96.5%, Val. checks: 1/15\n",
            "Epoch: 14/100, iter: 4430. Loss (train/val): 0.083 / 0.091. Val. acc: 96.4%, Val. checks: 2/15\n",
            "Epoch: 14/100, iter: 4440. Loss (train/val): 0.089 / 0.100. Val. acc: 96.3%, Val. checks: 3/15\n",
            "Epoch: 14/100, iter: 4450. Loss (train/val): 0.084 / 0.093. Val. acc: 96.2%, Val. checks: 4/15\n",
            "Epoch: 14/100, iter: 4460. Loss (train/val): 0.082 / 0.090. Val. acc: 96.4%, Val. checks: 5/15\n",
            "Epoch: 14/100, iter: 4470. Loss (train/val): 0.082 / 0.088. Val. acc: 96.8%, Val. checks: 0/15\n",
            "Epoch: 14/100, iter: 4480. Loss (train/val): 0.086 / 0.095. Val. acc: 96.3%, Val. checks: 1/15\n",
            "Epoch: 14/100, iter: 4490. Loss (train/val): 0.082 / 0.089. Val. acc: 96.5%, Val. checks: 2/15\n",
            "Epoch: 14/100, iter: 4500. Loss (train/val): 0.084 / 0.093. Val. acc: 96.4%, Val. checks: 3/15\n",
            "Epoch: 14/100, iter: 4510. Loss (train/val): 0.082 / 0.090. Val. acc: 96.4%, Val. checks: 4/15\n",
            "Epoch: 14/100, iter: 4520. Loss (train/val): 0.084 / 0.090. Val. acc: 96.4%, Val. checks: 5/15\n",
            "Epoch: 14/100, iter: 4530. Loss (train/val): 0.081 / 0.088. Val. acc: 96.5%, Val. checks: 6/15\n",
            "Epoch: 14/100, iter: 4540. Loss (train/val): 0.082 / 0.088. Val. acc: 96.6%, Val. checks: 7/15\n",
            "Epoch: 14/100, iter: 4550. Loss (train/val): 0.082 / 0.088. Val. acc: 96.5%, Val. checks: 0/15\n",
            "Epoch: 14/100, iter: 4560. Loss (train/val): 0.080 / 0.087. Val. acc: 96.4%, Val. checks: 0/15\n",
            "Epoch: 14/100, iter: 4570. Loss (train/val): 0.080 / 0.087. Val. acc: 96.3%, Val. checks: 1/15\n",
            "Epoch: 14/100, iter: 4580. Loss (train/val): 0.080 / 0.087. Val. acc: 96.6%, Val. checks: 0/15\n",
            "Epoch: 14/100, iter: 4590. Loss (train/val): 0.081 / 0.089. Val. acc: 96.4%, Val. checks: 1/15\n",
            "Epoch: 14/100, iter: 4600. Loss (train/val): 0.080 / 0.089. Val. acc: 96.2%, Val. checks: 2/15\n",
            "Epoch: 14/100, iter: 4610. Loss (train/val): 0.079 / 0.088. Val. acc: 96.2%, Val. checks: 3/15\n",
            "Epoch: 14/100, iter: 4620. Loss (train/val): 0.079 / 0.087. Val. acc: 96.2%, Val. checks: 0/15\n",
            "Epoch: 14/100, iter: 4630. Loss (train/val): 0.079 / 0.086. Val. acc: 96.6%, Val. checks: 0/15\n",
            "Epoch: 14/100, iter: 4640. Loss (train/val): 0.081 / 0.090. Val. acc: 96.4%, Val. checks: 1/15\n",
            "Epoch: 14/100, iter: 4650. Loss (train/val): 0.079 / 0.086. Val. acc: 96.6%, Val. checks: 0/15\n",
            "Epoch: 14/100, iter: 4660. Loss (train/val): 0.080 / 0.088. Val. acc: 96.4%, Val. checks: 1/15\n",
            "Epoch: 14/100, iter: 4670. Loss (train/val): 0.079 / 0.085. Val. acc: 96.6%, Val. checks: 0/15\n",
            "Epoch: 14/100, iter: 4680. Loss (train/val): 0.080 / 0.086. Val. acc: 96.8%, Val. checks: 1/15\n",
            "Epoch: 14/100, iter: 4690. Loss (train/val): 0.079 / 0.086. Val. acc: 96.6%, Val. checks: 2/15\n",
            "Epoch: 14/100, iter: 4700. Loss (train/val): 0.084 / 0.088. Val. acc: 96.8%, Val. checks: 3/15\n",
            "Epoch finished. Elapsed time 12.5608 [s]\n",
            "\n",
            "Epoch: 15/100, iter: 4710. Loss (train/val): 0.078 / 0.085. Val. acc: 96.8%, Val. checks: 0/15\n",
            "Epoch: 15/100, iter: 4720. Loss (train/val): 0.078 / 0.085. Val. acc: 96.5%, Val. checks: 1/15\n",
            "Epoch: 15/100, iter: 4730. Loss (train/val): 0.078 / 0.084. Val. acc: 96.6%, Val. checks: 0/15\n",
            "Epoch: 15/100, iter: 4740. Loss (train/val): 0.077 / 0.085. Val. acc: 96.4%, Val. checks: 1/15\n",
            "Epoch: 15/100, iter: 4750. Loss (train/val): 0.079 / 0.085. Val. acc: 96.8%, Val. checks: 2/15\n",
            "Epoch: 15/100, iter: 4760. Loss (train/val): 0.077 / 0.085. Val. acc: 96.3%, Val. checks: 3/15\n",
            "Epoch: 15/100, iter: 4770. Loss (train/val): 0.077 / 0.084. Val. acc: 96.4%, Val. checks: 0/15\n",
            "Epoch: 15/100, iter: 4780. Loss (train/val): 0.084 / 0.089. Val. acc: 96.8%, Val. checks: 1/15\n",
            "Epoch: 15/100, iter: 4790. Loss (train/val): 0.077 / 0.086. Val. acc: 96.8%, Val. checks: 2/15\n",
            "Epoch: 15/100, iter: 4800. Loss (train/val): 0.077 / 0.084. Val. acc: 96.8%, Val. checks: 3/15\n",
            "Epoch: 15/100, iter: 4810. Loss (train/val): 0.076 / 0.084. Val. acc: 96.9%, Val. checks: 4/15\n",
            "Epoch: 15/100, iter: 4820. Loss (train/val): 0.077 / 0.086. Val. acc: 96.8%, Val. checks: 5/15\n",
            "Epoch: 15/100, iter: 4830. Loss (train/val): 0.078 / 0.087. Val. acc: 96.4%, Val. checks: 6/15\n",
            "Epoch: 15/100, iter: 4840. Loss (train/val): 0.076 / 0.084. Val. acc: 96.8%, Val. checks: 0/15\n",
            "Epoch: 15/100, iter: 4850. Loss (train/val): 0.076 / 0.084. Val. acc: 97.0%, Val. checks: 1/15\n",
            "Epoch: 15/100, iter: 4860. Loss (train/val): 0.076 / 0.084. Val. acc: 97.0%, Val. checks: 2/15\n",
            "Epoch: 15/100, iter: 4870. Loss (train/val): 0.076 / 0.084. Val. acc: 96.5%, Val. checks: 3/15\n",
            "Epoch: 15/100, iter: 4880. Loss (train/val): 0.075 / 0.083. Val. acc: 96.6%, Val. checks: 0/15\n",
            "Epoch: 15/100, iter: 4890. Loss (train/val): 0.076 / 0.084. Val. acc: 96.6%, Val. checks: 1/15\n",
            "Epoch: 15/100, iter: 4900. Loss (train/val): 0.075 / 0.083. Val. acc: 96.9%, Val. checks: 0/15\n",
            "Epoch: 15/100, iter: 4910. Loss (train/val): 0.075 / 0.084. Val. acc: 96.5%, Val. checks: 1/15\n",
            "Epoch: 15/100, iter: 4920. Loss (train/val): 0.075 / 0.083. Val. acc: 96.4%, Val. checks: 2/15\n",
            "Epoch: 15/100, iter: 4930. Loss (train/val): 0.075 / 0.083. Val. acc: 96.4%, Val. checks: 3/15\n",
            "Epoch: 15/100, iter: 4940. Loss (train/val): 0.075 / 0.083. Val. acc: 96.4%, Val. checks: 4/15\n",
            "Epoch: 15/100, iter: 4950. Loss (train/val): 0.075 / 0.084. Val. acc: 96.5%, Val. checks: 5/15\n",
            "Epoch: 15/100, iter: 4960. Loss (train/val): 0.074 / 0.081. Val. acc: 96.8%, Val. checks: 0/15\n",
            "Epoch: 15/100, iter: 4970. Loss (train/val): 0.077 / 0.087. Val. acc: 96.3%, Val. checks: 1/15\n",
            "Epoch: 15/100, iter: 4980. Loss (train/val): 0.075 / 0.081. Val. acc: 96.8%, Val. checks: 0/15\n",
            "Epoch: 15/100, iter: 4990. Loss (train/val): 0.079 / 0.091. Val. acc: 96.3%, Val. checks: 1/15\n",
            "Epoch: 15/100, iter: 5000. Loss (train/val): 0.074 / 0.083. Val. acc: 96.5%, Val. checks: 2/15\n",
            "Epoch: 15/100, iter: 5010. Loss (train/val): 0.075 / 0.084. Val. acc: 96.6%, Val. checks: 3/15\n",
            "Epoch: 15/100, iter: 5020. Loss (train/val): 0.075 / 0.084. Val. acc: 96.4%, Val. checks: 4/15\n",
            "Epoch: 15/100, iter: 5030. Loss (train/val): 0.074 / 0.081. Val. acc: 97.1%, Val. checks: 5/15\n",
            "Epoch finished. Elapsed time 13.4416 [s]\n",
            "\n",
            "Epoch: 16/100, iter: 5040. Loss (train/val): 0.073 / 0.082. Val. acc: 96.8%, Val. checks: 6/15\n",
            "Epoch: 16/100, iter: 5050. Loss (train/val): 0.074 / 0.082. Val. acc: 96.8%, Val. checks: 7/15\n",
            "Epoch: 16/100, iter: 5060. Loss (train/val): 0.074 / 0.083. Val. acc: 96.8%, Val. checks: 8/15\n",
            "Epoch: 16/100, iter: 5070. Loss (train/val): 0.074 / 0.082. Val. acc: 96.6%, Val. checks: 9/15\n",
            "Epoch: 16/100, iter: 5080. Loss (train/val): 0.074 / 0.083. Val. acc: 96.8%, Val. checks: 10/15\n",
            "Epoch: 16/100, iter: 5090. Loss (train/val): 0.073 / 0.080. Val. acc: 96.5%, Val. checks: 0/15\n",
            "Epoch: 16/100, iter: 5100. Loss (train/val): 0.074 / 0.082. Val. acc: 96.9%, Val. checks: 1/15\n",
            "Epoch: 16/100, iter: 5110. Loss (train/val): 0.073 / 0.081. Val. acc: 96.9%, Val. checks: 2/15\n",
            "Epoch: 16/100, iter: 5120. Loss (train/val): 0.072 / 0.080. Val. acc: 96.9%, Val. checks: 0/15\n",
            "Epoch: 16/100, iter: 5130. Loss (train/val): 0.076 / 0.085. Val. acc: 96.6%, Val. checks: 1/15\n",
            "Epoch: 16/100, iter: 5140. Loss (train/val): 0.073 / 0.080. Val. acc: 96.8%, Val. checks: 0/15\n",
            "Epoch: 16/100, iter: 5150. Loss (train/val): 0.072 / 0.079. Val. acc: 97.1%, Val. checks: 0/15\n",
            "Epoch: 16/100, iter: 5160. Loss (train/val): 0.073 / 0.080. Val. acc: 96.8%, Val. checks: 1/15\n",
            "Epoch: 16/100, iter: 5170. Loss (train/val): 0.072 / 0.079. Val. acc: 96.8%, Val. checks: 0/15\n",
            "Epoch: 16/100, iter: 5180. Loss (train/val): 0.073 / 0.082. Val. acc: 96.8%, Val. checks: 1/15\n",
            "Epoch: 16/100, iter: 5190. Loss (train/val): 0.071 / 0.080. Val. acc: 97.2%, Val. checks: 2/15\n",
            "Epoch: 16/100, iter: 5200. Loss (train/val): 0.071 / 0.080. Val. acc: 96.9%, Val. checks: 3/15\n",
            "Epoch: 16/100, iter: 5210. Loss (train/val): 0.071 / 0.080. Val. acc: 97.1%, Val. checks: 4/15\n",
            "Epoch: 16/100, iter: 5220. Loss (train/val): 0.072 / 0.081. Val. acc: 96.6%, Val. checks: 5/15\n",
            "Epoch: 16/100, iter: 5230. Loss (train/val): 0.074 / 0.083. Val. acc: 96.4%, Val. checks: 6/15\n",
            "Epoch: 16/100, iter: 5240. Loss (train/val): 0.072 / 0.080. Val. acc: 96.9%, Val. checks: 7/15\n",
            "Epoch: 16/100, iter: 5250. Loss (train/val): 0.072 / 0.079. Val. acc: 97.1%, Val. checks: 0/15\n",
            "Epoch: 16/100, iter: 5260. Loss (train/val): 0.070 / 0.080. Val. acc: 96.9%, Val. checks: 1/15\n",
            "Epoch: 16/100, iter: 5270. Loss (train/val): 0.075 / 0.083. Val. acc: 96.6%, Val. checks: 2/15\n",
            "Epoch: 16/100, iter: 5280. Loss (train/val): 0.071 / 0.082. Val. acc: 96.4%, Val. checks: 3/15\n",
            "Epoch: 16/100, iter: 5290. Loss (train/val): 0.070 / 0.079. Val. acc: 97.0%, Val. checks: 4/15\n",
            "Epoch: 16/100, iter: 5300. Loss (train/val): 0.071 / 0.079. Val. acc: 97.0%, Val. checks: 5/15\n",
            "Epoch: 16/100, iter: 5310. Loss (train/val): 0.070 / 0.080. Val. acc: 96.9%, Val. checks: 6/15\n",
            "Epoch: 16/100, iter: 5320. Loss (train/val): 0.071 / 0.081. Val. acc: 97.0%, Val. checks: 7/15\n",
            "Epoch: 16/100, iter: 5330. Loss (train/val): 0.070 / 0.079. Val. acc: 97.2%, Val. checks: 8/15\n",
            "Epoch: 16/100, iter: 5340. Loss (train/val): 0.069 / 0.079. Val. acc: 97.3%, Val. checks: 0/15\n",
            "Epoch: 16/100, iter: 5350. Loss (train/val): 0.069 / 0.079. Val. acc: 97.0%, Val. checks: 1/15\n",
            "Epoch: 16/100, iter: 5360. Loss (train/val): 0.069 / 0.079. Val. acc: 97.1%, Val. checks: 2/15\n",
            "Epoch: 16/100, iter: 5370. Loss (train/val): 0.069 / 0.078. Val. acc: 97.2%, Val. checks: 0/15\n",
            "Epoch finished. Elapsed time 14.3327 [s]\n",
            "\n",
            "Epoch: 17/100, iter: 5380. Loss (train/val): 0.069 / 0.079. Val. acc: 97.0%, Val. checks: 1/15\n",
            "Epoch: 17/100, iter: 5390. Loss (train/val): 0.070 / 0.079. Val. acc: 97.3%, Val. checks: 2/15\n",
            "Epoch: 17/100, iter: 5400. Loss (train/val): 0.069 / 0.080. Val. acc: 96.9%, Val. checks: 3/15\n",
            "Epoch: 17/100, iter: 5410. Loss (train/val): 0.071 / 0.081. Val. acc: 97.1%, Val. checks: 4/15\n",
            "Epoch: 17/100, iter: 5420. Loss (train/val): 0.069 / 0.081. Val. acc: 97.0%, Val. checks: 5/15\n",
            "Epoch: 17/100, iter: 5430. Loss (train/val): 0.069 / 0.080. Val. acc: 97.1%, Val. checks: 6/15\n",
            "Epoch: 17/100, iter: 5440. Loss (train/val): 0.068 / 0.080. Val. acc: 97.0%, Val. checks: 7/15\n",
            "Epoch: 17/100, iter: 5450. Loss (train/val): 0.068 / 0.079. Val. acc: 97.1%, Val. checks: 8/15\n",
            "Epoch: 17/100, iter: 5460. Loss (train/val): 0.068 / 0.079. Val. acc: 97.1%, Val. checks: 9/15\n",
            "Epoch: 17/100, iter: 5470. Loss (train/val): 0.068 / 0.080. Val. acc: 97.1%, Val. checks: 10/15\n",
            "Epoch: 17/100, iter: 5480. Loss (train/val): 0.068 / 0.079. Val. acc: 97.4%, Val. checks: 11/15\n",
            "Epoch: 17/100, iter: 5490. Loss (train/val): 0.070 / 0.083. Val. acc: 96.9%, Val. checks: 12/15\n",
            "Epoch: 17/100, iter: 5500. Loss (train/val): 0.068 / 0.080. Val. acc: 97.2%, Val. checks: 13/15\n",
            "Epoch: 17/100, iter: 5510. Loss (train/val): 0.073 / 0.086. Val. acc: 96.6%, Val. checks: 14/15\n",
            "Epoch: 17/100, iter: 5520. Loss (train/val): 0.071 / 0.084. Val. acc: 96.9%, Val. checks: 15/15\n",
            "Early stopping\n",
            "Epoch finished. Elapsed time 14.7261 [s]\n",
            "\n",
            "\n",
            "\n",
            "[Beginning training of MLP at logdir \"./tarea_1_logs/exp_xentropyrun_3\"]\n",
            "\n",
            "Epoch: 1/100, iter: 0. Loss (train/val): 0.676 / 0.677. Val. acc: 59.3%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 10. Loss (train/val): 0.604 / 0.607. Val. acc: 78.4%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 20. Loss (train/val): 0.545 / 0.550. Val. acc: 84.2%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 30. Loss (train/val): 0.505 / 0.509. Val. acc: 86.3%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 40. Loss (train/val): 0.469 / 0.474. Val. acc: 86.4%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 50. Loss (train/val): 0.449 / 0.456. Val. acc: 83.2%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 60. Loss (train/val): 0.418 / 0.422. Val. acc: 87.0%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 70. Loss (train/val): 0.395 / 0.399. Val. acc: 87.6%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 80. Loss (train/val): 0.380 / 0.383. Val. acc: 87.3%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 90. Loss (train/val): 0.365 / 0.369. Val. acc: 87.6%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 100. Loss (train/val): 0.355 / 0.356. Val. acc: 88.0%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 110. Loss (train/val): 0.346 / 0.349. Val. acc: 87.7%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 120. Loss (train/val): 0.334 / 0.337. Val. acc: 87.3%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 130. Loss (train/val): 0.327 / 0.327. Val. acc: 88.5%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 140. Loss (train/val): 0.320 / 0.320. Val. acc: 87.7%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 150. Loss (train/val): 0.313 / 0.313. Val. acc: 88.6%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 160. Loss (train/val): 0.308 / 0.306. Val. acc: 88.6%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 170. Loss (train/val): 0.303 / 0.301. Val. acc: 89.2%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 180. Loss (train/val): 0.301 / 0.298. Val. acc: 88.5%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 190. Loss (train/val): 0.295 / 0.292. Val. acc: 88.7%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 200. Loss (train/val): 0.291 / 0.286. Val. acc: 89.4%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 210. Loss (train/val): 0.292 / 0.283. Val. acc: 89.2%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 220. Loss (train/val): 0.285 / 0.279. Val. acc: 89.4%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 230. Loss (train/val): 0.282 / 0.276. Val. acc: 89.7%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 240. Loss (train/val): 0.290 / 0.286. Val. acc: 88.5%, Val. checks: 1/15\n",
            "Epoch: 1/100, iter: 250. Loss (train/val): 0.277 / 0.270. Val. acc: 89.7%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 260. Loss (train/val): 0.283 / 0.270. Val. acc: 90.2%, Val. checks: 1/15\n",
            "Epoch: 1/100, iter: 270. Loss (train/val): 0.274 / 0.262. Val. acc: 90.5%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 280. Loss (train/val): 0.272 / 0.262. Val. acc: 89.9%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 290. Loss (train/val): 0.270 / 0.262. Val. acc: 89.6%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 300. Loss (train/val): 0.268 / 0.258. Val. acc: 90.2%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 310. Loss (train/val): 0.267 / 0.260. Val. acc: 89.6%, Val. checks: 1/15\n",
            "Epoch: 1/100, iter: 320. Loss (train/val): 0.265 / 0.257. Val. acc: 89.9%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 330. Loss (train/val): 0.263 / 0.256. Val. acc: 89.8%, Val. checks: 0/15\n",
            "Epoch finished. Elapsed time 0.9278 [s]\n",
            "\n",
            "Epoch: 2/100, iter: 340. Loss (train/val): 0.262 / 0.255. Val. acc: 89.9%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 350. Loss (train/val): 0.262 / 0.257. Val. acc: 89.3%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 360. Loss (train/val): 0.260 / 0.254. Val. acc: 89.1%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 370. Loss (train/val): 0.258 / 0.248. Val. acc: 90.5%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 380. Loss (train/val): 0.266 / 0.254. Val. acc: 90.6%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 390. Loss (train/val): 0.258 / 0.250. Val. acc: 90.3%, Val. checks: 2/15\n",
            "Epoch: 2/100, iter: 400. Loss (train/val): 0.263 / 0.253. Val. acc: 89.5%, Val. checks: 3/15\n",
            "Epoch: 2/100, iter: 410. Loss (train/val): 0.253 / 0.246. Val. acc: 90.4%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 420. Loss (train/val): 0.255 / 0.245. Val. acc: 90.8%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 430. Loss (train/val): 0.254 / 0.245. Val. acc: 90.8%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 440. Loss (train/val): 0.250 / 0.242. Val. acc: 90.4%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 450. Loss (train/val): 0.262 / 0.257. Val. acc: 88.9%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 460. Loss (train/val): 0.248 / 0.239. Val. acc: 90.5%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 470. Loss (train/val): 0.251 / 0.244. Val. acc: 89.7%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 480. Loss (train/val): 0.255 / 0.241. Val. acc: 91.6%, Val. checks: 2/15\n",
            "Epoch: 2/100, iter: 490. Loss (train/val): 0.262 / 0.257. Val. acc: 88.5%, Val. checks: 3/15\n",
            "Epoch: 2/100, iter: 500. Loss (train/val): 0.245 / 0.236. Val. acc: 90.5%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 510. Loss (train/val): 0.246 / 0.237. Val. acc: 89.9%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 520. Loss (train/val): 0.245 / 0.234. Val. acc: 90.8%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 530. Loss (train/val): 0.249 / 0.239. Val. acc: 89.7%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 540. Loss (train/val): 0.246 / 0.238. Val. acc: 89.9%, Val. checks: 2/15\n",
            "Epoch: 2/100, iter: 550. Loss (train/val): 0.247 / 0.235. Val. acc: 91.5%, Val. checks: 3/15\n",
            "Epoch: 2/100, iter: 560. Loss (train/val): 0.242 / 0.231. Val. acc: 91.1%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 570. Loss (train/val): 0.245 / 0.237. Val. acc: 90.2%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 580. Loss (train/val): 0.241 / 0.232. Val. acc: 90.7%, Val. checks: 2/15\n",
            "Epoch: 2/100, iter: 590. Loss (train/val): 0.242 / 0.232. Val. acc: 91.8%, Val. checks: 3/15\n",
            "Epoch: 2/100, iter: 600. Loss (train/val): 0.247 / 0.234. Val. acc: 91.9%, Val. checks: 4/15\n",
            "Epoch: 2/100, iter: 610. Loss (train/val): 0.241 / 0.233. Val. acc: 90.0%, Val. checks: 5/15\n",
            "Epoch: 2/100, iter: 620. Loss (train/val): 0.238 / 0.230. Val. acc: 91.1%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 630. Loss (train/val): 0.237 / 0.229. Val. acc: 90.9%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 640. Loss (train/val): 0.237 / 0.229. Val. acc: 90.8%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 650. Loss (train/val): 0.247 / 0.236. Val. acc: 92.0%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 660. Loss (train/val): 0.243 / 0.233. Val. acc: 91.3%, Val. checks: 2/15\n",
            "Epoch: 2/100, iter: 670. Loss (train/val): 0.235 / 0.226. Val. acc: 91.3%, Val. checks: 0/15\n",
            "Epoch finished. Elapsed time 1.8932 [s]\n",
            "\n",
            "Epoch: 3/100, iter: 680. Loss (train/val): 0.239 / 0.228. Val. acc: 92.1%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 690. Loss (train/val): 0.239 / 0.228. Val. acc: 92.3%, Val. checks: 2/15\n",
            "Epoch: 3/100, iter: 700. Loss (train/val): 0.233 / 0.223. Val. acc: 91.5%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 710. Loss (train/val): 0.232 / 0.224. Val. acc: 91.2%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 720. Loss (train/val): 0.234 / 0.228. Val. acc: 90.7%, Val. checks: 2/15\n",
            "Epoch: 3/100, iter: 730. Loss (train/val): 0.242 / 0.229. Val. acc: 92.6%, Val. checks: 3/15\n",
            "Epoch: 3/100, iter: 740. Loss (train/val): 0.234 / 0.223. Val. acc: 92.2%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 750. Loss (train/val): 0.230 / 0.221. Val. acc: 90.9%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 760. Loss (train/val): 0.229 / 0.217. Val. acc: 91.5%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 770. Loss (train/val): 0.228 / 0.218. Val. acc: 91.7%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 780. Loss (train/val): 0.231 / 0.217. Val. acc: 92.5%, Val. checks: 2/15\n",
            "Epoch: 3/100, iter: 790. Loss (train/val): 0.238 / 0.225. Val. acc: 90.2%, Val. checks: 3/15\n",
            "Epoch: 3/100, iter: 800. Loss (train/val): 0.227 / 0.215. Val. acc: 91.3%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 810. Loss (train/val): 0.229 / 0.217. Val. acc: 91.0%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 820. Loss (train/val): 0.233 / 0.223. Val. acc: 90.8%, Val. checks: 2/15\n",
            "Epoch: 3/100, iter: 830. Loss (train/val): 0.227 / 0.214. Val. acc: 92.3%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 840. Loss (train/val): 0.239 / 0.224. Val. acc: 92.1%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 850. Loss (train/val): 0.226 / 0.215. Val. acc: 92.3%, Val. checks: 2/15\n",
            "Epoch: 3/100, iter: 860. Loss (train/val): 0.247 / 0.239. Val. acc: 88.7%, Val. checks: 3/15\n",
            "Epoch: 3/100, iter: 870. Loss (train/val): 0.222 / 0.213. Val. acc: 91.5%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 880. Loss (train/val): 0.222 / 0.212. Val. acc: 91.3%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 890. Loss (train/val): 0.256 / 0.249. Val. acc: 88.5%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 900. Loss (train/val): 0.224 / 0.218. Val. acc: 91.0%, Val. checks: 2/15\n",
            "Epoch: 3/100, iter: 910. Loss (train/val): 0.220 / 0.211. Val. acc: 91.7%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 920. Loss (train/val): 0.226 / 0.220. Val. acc: 90.7%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 930. Loss (train/val): 0.220 / 0.211. Val. acc: 92.1%, Val. checks: 2/15\n",
            "Epoch: 3/100, iter: 940. Loss (train/val): 0.219 / 0.210. Val. acc: 92.2%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 950. Loss (train/val): 0.218 / 0.208. Val. acc: 92.4%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 960. Loss (train/val): 0.224 / 0.213. Val. acc: 92.9%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 970. Loss (train/val): 0.223 / 0.214. Val. acc: 91.5%, Val. checks: 2/15\n",
            "Epoch: 3/100, iter: 980. Loss (train/val): 0.216 / 0.208. Val. acc: 91.8%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 990. Loss (train/val): 0.215 / 0.209. Val. acc: 92.1%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 1000. Loss (train/val): 0.219 / 0.212. Val. acc: 92.6%, Val. checks: 2/15\n",
            "Epoch finished. Elapsed time 2.7982 [s]\n",
            "\n",
            "Epoch: 4/100, iter: 1010. Loss (train/val): 0.222 / 0.214. Val. acc: 93.0%, Val. checks: 3/15\n",
            "Epoch: 4/100, iter: 1020. Loss (train/val): 0.216 / 0.208. Val. acc: 91.5%, Val. checks: 4/15\n",
            "Epoch: 4/100, iter: 1030. Loss (train/val): 0.214 / 0.208. Val. acc: 91.6%, Val. checks: 5/15\n",
            "Epoch: 4/100, iter: 1040. Loss (train/val): 0.217 / 0.210. Val. acc: 91.0%, Val. checks: 6/15\n",
            "Epoch: 4/100, iter: 1050. Loss (train/val): 0.212 / 0.203. Val. acc: 92.6%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1060. Loss (train/val): 0.211 / 0.203. Val. acc: 92.3%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1070. Loss (train/val): 0.213 / 0.208. Val. acc: 92.1%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1080. Loss (train/val): 0.216 / 0.208. Val. acc: 93.0%, Val. checks: 3/15\n",
            "Epoch: 4/100, iter: 1090. Loss (train/val): 0.215 / 0.205. Val. acc: 93.2%, Val. checks: 4/15\n",
            "Epoch: 4/100, iter: 1100. Loss (train/val): 0.209 / 0.197. Val. acc: 92.9%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1110. Loss (train/val): 0.215 / 0.203. Val. acc: 93.2%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1120. Loss (train/val): 0.210 / 0.196. Val. acc: 93.0%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1130. Loss (train/val): 0.209 / 0.195. Val. acc: 92.9%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1140. Loss (train/val): 0.209 / 0.197. Val. acc: 93.3%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1150. Loss (train/val): 0.206 / 0.194. Val. acc: 92.4%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1160. Loss (train/val): 0.205 / 0.196. Val. acc: 92.3%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1170. Loss (train/val): 0.207 / 0.198. Val. acc: 92.3%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1180. Loss (train/val): 0.210 / 0.203. Val. acc: 93.2%, Val. checks: 3/15\n",
            "Epoch: 4/100, iter: 1190. Loss (train/val): 0.202 / 0.196. Val. acc: 92.5%, Val. checks: 4/15\n",
            "Epoch: 4/100, iter: 1200. Loss (train/val): 0.202 / 0.195. Val. acc: 92.6%, Val. checks: 5/15\n",
            "Epoch: 4/100, iter: 1210. Loss (train/val): 0.204 / 0.199. Val. acc: 92.2%, Val. checks: 6/15\n",
            "Epoch: 4/100, iter: 1220. Loss (train/val): 0.201 / 0.194. Val. acc: 92.6%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1230. Loss (train/val): 0.213 / 0.206. Val. acc: 93.4%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1240. Loss (train/val): 0.206 / 0.200. Val. acc: 91.6%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1250. Loss (train/val): 0.205 / 0.199. Val. acc: 93.5%, Val. checks: 3/15\n",
            "Epoch: 4/100, iter: 1260. Loss (train/val): 0.198 / 0.192. Val. acc: 92.3%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1270. Loss (train/val): 0.199 / 0.192. Val. acc: 93.2%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1280. Loss (train/val): 0.197 / 0.190. Val. acc: 92.6%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1290. Loss (train/val): 0.196 / 0.189. Val. acc: 92.9%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1300. Loss (train/val): 0.195 / 0.190. Val. acc: 92.7%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1310. Loss (train/val): 0.198 / 0.194. Val. acc: 93.2%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1320. Loss (train/val): 0.192 / 0.188. Val. acc: 93.2%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1330. Loss (train/val): 0.202 / 0.197. Val. acc: 93.4%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1340. Loss (train/val): 0.191 / 0.186. Val. acc: 93.3%, Val. checks: 0/15\n",
            "Epoch finished. Elapsed time 3.6894 [s]\n",
            "\n",
            "Epoch: 5/100, iter: 1350. Loss (train/val): 0.191 / 0.188. Val. acc: 93.1%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1360. Loss (train/val): 0.202 / 0.199. Val. acc: 91.9%, Val. checks: 2/15\n",
            "Epoch: 5/100, iter: 1370. Loss (train/val): 0.193 / 0.189. Val. acc: 92.3%, Val. checks: 3/15\n",
            "Epoch: 5/100, iter: 1380. Loss (train/val): 0.194 / 0.191. Val. acc: 93.8%, Val. checks: 4/15\n",
            "Epoch: 5/100, iter: 1390. Loss (train/val): 0.198 / 0.197. Val. acc: 91.5%, Val. checks: 5/15\n",
            "Epoch: 5/100, iter: 1400. Loss (train/val): 0.191 / 0.191. Val. acc: 92.6%, Val. checks: 6/15\n",
            "Epoch: 5/100, iter: 1410. Loss (train/val): 0.191 / 0.189. Val. acc: 92.2%, Val. checks: 7/15\n",
            "Epoch: 5/100, iter: 1420. Loss (train/val): 0.188 / 0.184. Val. acc: 93.2%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1430. Loss (train/val): 0.187 / 0.184. Val. acc: 93.3%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1440. Loss (train/val): 0.187 / 0.185. Val. acc: 93.6%, Val. checks: 2/15\n",
            "Epoch: 5/100, iter: 1450. Loss (train/val): 0.185 / 0.182. Val. acc: 92.9%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1460. Loss (train/val): 0.203 / 0.198. Val. acc: 91.5%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1470. Loss (train/val): 0.185 / 0.182. Val. acc: 92.4%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1480. Loss (train/val): 0.186 / 0.182. Val. acc: 93.8%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1490. Loss (train/val): 0.186 / 0.181. Val. acc: 93.9%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1500. Loss (train/val): 0.189 / 0.184. Val. acc: 92.6%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1510. Loss (train/val): 0.181 / 0.175. Val. acc: 93.4%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1520. Loss (train/val): 0.188 / 0.180. Val. acc: 93.2%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1530. Loss (train/val): 0.187 / 0.177. Val. acc: 93.2%, Val. checks: 2/15\n",
            "Epoch: 5/100, iter: 1540. Loss (train/val): 0.181 / 0.172. Val. acc: 93.7%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1550. Loss (train/val): 0.183 / 0.177. Val. acc: 94.4%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1560. Loss (train/val): 0.177 / 0.171. Val. acc: 93.4%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1570. Loss (train/val): 0.180 / 0.173. Val. acc: 94.0%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1580. Loss (train/val): 0.180 / 0.173. Val. acc: 93.3%, Val. checks: 2/15\n",
            "Epoch: 5/100, iter: 1590. Loss (train/val): 0.178 / 0.172. Val. acc: 94.0%, Val. checks: 3/15\n",
            "Epoch: 5/100, iter: 1600. Loss (train/val): 0.176 / 0.171. Val. acc: 94.3%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1610. Loss (train/val): 0.174 / 0.170. Val. acc: 94.2%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1620. Loss (train/val): 0.174 / 0.169. Val. acc: 94.0%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1630. Loss (train/val): 0.174 / 0.170. Val. acc: 94.2%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1640. Loss (train/val): 0.177 / 0.174. Val. acc: 94.4%, Val. checks: 2/15\n",
            "Epoch: 5/100, iter: 1650. Loss (train/val): 0.173 / 0.171. Val. acc: 94.2%, Val. checks: 3/15\n",
            "Epoch: 5/100, iter: 1660. Loss (train/val): 0.175 / 0.171. Val. acc: 93.6%, Val. checks: 4/15\n",
            "Epoch: 5/100, iter: 1670. Loss (train/val): 0.170 / 0.167. Val. acc: 93.9%, Val. checks: 0/15\n",
            "Epoch finished. Elapsed time 4.5742 [s]\n",
            "\n",
            "Epoch: 6/100, iter: 1680. Loss (train/val): 0.172 / 0.169. Val. acc: 93.2%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1690. Loss (train/val): 0.169 / 0.165. Val. acc: 93.6%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1700. Loss (train/val): 0.170 / 0.168. Val. acc: 93.1%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1710. Loss (train/val): 0.168 / 0.164. Val. acc: 93.9%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1720. Loss (train/val): 0.167 / 0.162. Val. acc: 93.9%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1730. Loss (train/val): 0.170 / 0.165. Val. acc: 93.2%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1740. Loss (train/val): 0.167 / 0.162. Val. acc: 94.0%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1750. Loss (train/val): 0.168 / 0.165. Val. acc: 93.4%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1760. Loss (train/val): 0.167 / 0.163. Val. acc: 94.3%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 1770. Loss (train/val): 0.169 / 0.164. Val. acc: 94.6%, Val. checks: 3/15\n",
            "Epoch: 6/100, iter: 1780. Loss (train/val): 0.164 / 0.159. Val. acc: 94.0%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1790. Loss (train/val): 0.164 / 0.159. Val. acc: 94.3%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1800. Loss (train/val): 0.163 / 0.158. Val. acc: 94.0%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1810. Loss (train/val): 0.167 / 0.161. Val. acc: 93.6%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1820. Loss (train/val): 0.162 / 0.158. Val. acc: 94.3%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 1830. Loss (train/val): 0.161 / 0.156. Val. acc: 94.3%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1840. Loss (train/val): 0.171 / 0.166. Val. acc: 93.4%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1850. Loss (train/val): 0.180 / 0.176. Val. acc: 94.3%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 1860. Loss (train/val): 0.167 / 0.163. Val. acc: 94.8%, Val. checks: 3/15\n",
            "Epoch: 6/100, iter: 1870. Loss (train/val): 0.159 / 0.157. Val. acc: 94.4%, Val. checks: 4/15\n",
            "Epoch: 6/100, iter: 1880. Loss (train/val): 0.159 / 0.156. Val. acc: 94.7%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1890. Loss (train/val): 0.160 / 0.157. Val. acc: 95.0%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1900. Loss (train/val): 0.158 / 0.155. Val. acc: 93.9%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1910. Loss (train/val): 0.163 / 0.160. Val. acc: 93.9%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1920. Loss (train/val): 0.160 / 0.159. Val. acc: 94.7%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 1930. Loss (train/val): 0.160 / 0.159. Val. acc: 93.6%, Val. checks: 3/15\n",
            "Epoch: 6/100, iter: 1940. Loss (train/val): 0.157 / 0.156. Val. acc: 94.7%, Val. checks: 4/15\n",
            "Epoch: 6/100, iter: 1950. Loss (train/val): 0.161 / 0.161. Val. acc: 94.6%, Val. checks: 5/15\n",
            "Epoch: 6/100, iter: 1960. Loss (train/val): 0.156 / 0.156. Val. acc: 94.4%, Val. checks: 6/15\n",
            "Epoch: 6/100, iter: 1970. Loss (train/val): 0.153 / 0.152. Val. acc: 93.5%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1980. Loss (train/val): 0.153 / 0.149. Val. acc: 94.0%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1990. Loss (train/val): 0.152 / 0.148. Val. acc: 94.3%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 2000. Loss (train/val): 0.151 / 0.149. Val. acc: 94.3%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 2010. Loss (train/val): 0.151 / 0.149. Val. acc: 94.2%, Val. checks: 2/15\n",
            "Epoch finished. Elapsed time 5.4621 [s]\n",
            "\n",
            "Epoch: 7/100, iter: 2020. Loss (train/val): 0.150 / 0.148. Val. acc: 94.4%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2030. Loss (train/val): 0.154 / 0.151. Val. acc: 94.7%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2040. Loss (train/val): 0.149 / 0.147. Val. acc: 94.5%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2050. Loss (train/val): 0.149 / 0.148. Val. acc: 94.7%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2060. Loss (train/val): 0.150 / 0.147. Val. acc: 94.5%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2070. Loss (train/val): 0.156 / 0.155. Val. acc: 94.3%, Val. checks: 3/15\n",
            "Epoch: 7/100, iter: 2080. Loss (train/val): 0.149 / 0.149. Val. acc: 94.7%, Val. checks: 4/15\n",
            "Epoch: 7/100, iter: 2090. Loss (train/val): 0.151 / 0.152. Val. acc: 95.0%, Val. checks: 5/15\n",
            "Epoch: 7/100, iter: 2100. Loss (train/val): 0.149 / 0.149. Val. acc: 94.2%, Val. checks: 6/15\n",
            "Epoch: 7/100, iter: 2110. Loss (train/val): 0.146 / 0.146. Val. acc: 93.9%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2120. Loss (train/val): 0.146 / 0.146. Val. acc: 93.8%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2130. Loss (train/val): 0.145 / 0.144. Val. acc: 94.0%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2140. Loss (train/val): 0.144 / 0.144. Val. acc: 93.9%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2150. Loss (train/val): 0.144 / 0.143. Val. acc: 94.4%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2160. Loss (train/val): 0.143 / 0.141. Val. acc: 94.6%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2170. Loss (train/val): 0.145 / 0.142. Val. acc: 94.5%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2180. Loss (train/val): 0.149 / 0.146. Val. acc: 94.2%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2190. Loss (train/val): 0.142 / 0.139. Val. acc: 94.4%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2200. Loss (train/val): 0.141 / 0.138. Val. acc: 94.2%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2210. Loss (train/val): 0.145 / 0.141. Val. acc: 95.1%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2220. Loss (train/val): 0.145 / 0.143. Val. acc: 94.8%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2230. Loss (train/val): 0.141 / 0.139. Val. acc: 94.8%, Val. checks: 3/15\n",
            "Epoch: 7/100, iter: 2240. Loss (train/val): 0.145 / 0.143. Val. acc: 94.8%, Val. checks: 4/15\n",
            "Epoch: 7/100, iter: 2250. Loss (train/val): 0.142 / 0.139. Val. acc: 94.7%, Val. checks: 5/15\n",
            "Epoch: 7/100, iter: 2260. Loss (train/val): 0.160 / 0.156. Val. acc: 93.6%, Val. checks: 6/15\n",
            "Epoch: 7/100, iter: 2270. Loss (train/val): 0.139 / 0.137. Val. acc: 94.5%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2280. Loss (train/val): 0.138 / 0.136. Val. acc: 94.5%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2290. Loss (train/val): 0.143 / 0.141. Val. acc: 95.0%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2300. Loss (train/val): 0.139 / 0.136. Val. acc: 94.5%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2310. Loss (train/val): 0.139 / 0.135. Val. acc: 94.6%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2320. Loss (train/val): 0.137 / 0.133. Val. acc: 94.6%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2330. Loss (train/val): 0.140 / 0.136. Val. acc: 94.9%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2340. Loss (train/val): 0.136 / 0.134. Val. acc: 94.8%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2350. Loss (train/val): 0.137 / 0.134. Val. acc: 94.8%, Val. checks: 3/15\n",
            "Epoch finished. Elapsed time 6.3545 [s]\n",
            "\n",
            "Epoch: 8/100, iter: 2360. Loss (train/val): 0.135 / 0.132. Val. acc: 94.5%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2370. Loss (train/val): 0.134 / 0.131. Val. acc: 94.2%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2380. Loss (train/val): 0.134 / 0.131. Val. acc: 94.5%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2390. Loss (train/val): 0.136 / 0.131. Val. acc: 95.1%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2400. Loss (train/val): 0.135 / 0.130. Val. acc: 95.0%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2410. Loss (train/val): 0.133 / 0.130. Val. acc: 94.6%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2420. Loss (train/val): 0.133 / 0.129. Val. acc: 95.1%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2430. Loss (train/val): 0.133 / 0.128. Val. acc: 94.8%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2440. Loss (train/val): 0.135 / 0.132. Val. acc: 94.7%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2450. Loss (train/val): 0.137 / 0.136. Val. acc: 95.1%, Val. checks: 2/15\n",
            "Epoch: 8/100, iter: 2460. Loss (train/val): 0.133 / 0.130. Val. acc: 94.5%, Val. checks: 3/15\n",
            "Epoch: 8/100, iter: 2470. Loss (train/val): 0.131 / 0.129. Val. acc: 94.6%, Val. checks: 4/15\n",
            "Epoch: 8/100, iter: 2480. Loss (train/val): 0.131 / 0.128. Val. acc: 95.0%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2490. Loss (train/val): 0.130 / 0.126. Val. acc: 94.7%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2500. Loss (train/val): 0.129 / 0.126. Val. acc: 94.8%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2510. Loss (train/val): 0.132 / 0.129. Val. acc: 95.0%, Val. checks: 2/15\n",
            "Epoch: 8/100, iter: 2520. Loss (train/val): 0.129 / 0.125. Val. acc: 94.7%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2530. Loss (train/val): 0.131 / 0.130. Val. acc: 95.0%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2540. Loss (train/val): 0.129 / 0.127. Val. acc: 95.2%, Val. checks: 2/15\n",
            "Epoch: 8/100, iter: 2550. Loss (train/val): 0.128 / 0.123. Val. acc: 95.0%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2560. Loss (train/val): 0.127 / 0.124. Val. acc: 94.6%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2570. Loss (train/val): 0.127 / 0.126. Val. acc: 94.5%, Val. checks: 2/15\n",
            "Epoch: 8/100, iter: 2580. Loss (train/val): 0.126 / 0.124. Val. acc: 94.7%, Val. checks: 3/15\n",
            "Epoch: 8/100, iter: 2590. Loss (train/val): 0.137 / 0.136. Val. acc: 94.7%, Val. checks: 4/15\n",
            "Epoch: 8/100, iter: 2600. Loss (train/val): 0.125 / 0.124. Val. acc: 94.9%, Val. checks: 5/15\n",
            "Epoch: 8/100, iter: 2610. Loss (train/val): 0.125 / 0.125. Val. acc: 95.0%, Val. checks: 6/15\n",
            "Epoch: 8/100, iter: 2620. Loss (train/val): 0.135 / 0.136. Val. acc: 95.3%, Val. checks: 7/15\n",
            "Epoch: 8/100, iter: 2630. Loss (train/val): 0.124 / 0.123. Val. acc: 94.9%, Val. checks: 8/15\n",
            "Epoch: 8/100, iter: 2640. Loss (train/val): 0.124 / 0.124. Val. acc: 95.1%, Val. checks: 9/15\n",
            "Epoch: 8/100, iter: 2650. Loss (train/val): 0.124 / 0.125. Val. acc: 94.6%, Val. checks: 10/15\n",
            "Epoch: 8/100, iter: 2660. Loss (train/val): 0.123 / 0.124. Val. acc: 95.1%, Val. checks: 11/15\n",
            "Epoch: 8/100, iter: 2670. Loss (train/val): 0.126 / 0.126. Val. acc: 94.8%, Val. checks: 12/15\n",
            "Epoch: 8/100, iter: 2680. Loss (train/val): 0.128 / 0.128. Val. acc: 95.3%, Val. checks: 13/15\n",
            "Epoch finished. Elapsed time 7.2656 [s]\n",
            "\n",
            "Epoch: 9/100, iter: 2690. Loss (train/val): 0.126 / 0.127. Val. acc: 95.5%, Val. checks: 14/15\n",
            "Epoch: 9/100, iter: 2700. Loss (train/val): 0.122 / 0.122. Val. acc: 95.1%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2710. Loss (train/val): 0.121 / 0.121. Val. acc: 95.1%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2720. Loss (train/val): 0.123 / 0.121. Val. acc: 95.3%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2730. Loss (train/val): 0.122 / 0.122. Val. acc: 95.1%, Val. checks: 2/15\n",
            "Epoch: 9/100, iter: 2740. Loss (train/val): 0.124 / 0.123. Val. acc: 95.2%, Val. checks: 3/15\n",
            "Epoch: 9/100, iter: 2750. Loss (train/val): 0.121 / 0.121. Val. acc: 94.8%, Val. checks: 4/15\n",
            "Epoch: 9/100, iter: 2760. Loss (train/val): 0.120 / 0.121. Val. acc: 95.2%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2770. Loss (train/val): 0.120 / 0.118. Val. acc: 95.0%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2780. Loss (train/val): 0.120 / 0.118. Val. acc: 95.1%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2790. Loss (train/val): 0.119 / 0.118. Val. acc: 95.3%, Val. checks: 2/15\n",
            "Epoch: 9/100, iter: 2800. Loss (train/val): 0.121 / 0.122. Val. acc: 95.3%, Val. checks: 3/15\n",
            "Epoch: 9/100, iter: 2810. Loss (train/val): 0.124 / 0.125. Val. acc: 95.7%, Val. checks: 4/15\n",
            "Epoch: 9/100, iter: 2820. Loss (train/val): 0.121 / 0.120. Val. acc: 95.6%, Val. checks: 5/15\n",
            "Epoch: 9/100, iter: 2830. Loss (train/val): 0.118 / 0.118. Val. acc: 95.2%, Val. checks: 6/15\n",
            "Epoch: 9/100, iter: 2840. Loss (train/val): 0.120 / 0.120. Val. acc: 95.5%, Val. checks: 7/15\n",
            "Epoch: 9/100, iter: 2850. Loss (train/val): 0.117 / 0.117. Val. acc: 95.3%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2860. Loss (train/val): 0.117 / 0.115. Val. acc: 95.2%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2870. Loss (train/val): 0.118 / 0.117. Val. acc: 95.5%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2880. Loss (train/val): 0.117 / 0.116. Val. acc: 95.6%, Val. checks: 2/15\n",
            "Epoch: 9/100, iter: 2890. Loss (train/val): 0.116 / 0.115. Val. acc: 95.3%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2900. Loss (train/val): 0.118 / 0.118. Val. acc: 95.2%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2910. Loss (train/val): 0.117 / 0.118. Val. acc: 95.8%, Val. checks: 2/15\n",
            "Epoch: 9/100, iter: 2920. Loss (train/val): 0.117 / 0.118. Val. acc: 95.5%, Val. checks: 3/15\n",
            "Epoch: 9/100, iter: 2930. Loss (train/val): 0.127 / 0.129. Val. acc: 94.8%, Val. checks: 4/15\n",
            "Epoch: 9/100, iter: 2940. Loss (train/val): 0.115 / 0.119. Val. acc: 94.7%, Val. checks: 5/15\n",
            "Epoch: 9/100, iter: 2950. Loss (train/val): 0.114 / 0.116. Val. acc: 95.1%, Val. checks: 6/15\n",
            "Epoch: 9/100, iter: 2960. Loss (train/val): 0.114 / 0.115. Val. acc: 95.2%, Val. checks: 7/15\n",
            "Epoch: 9/100, iter: 2970. Loss (train/val): 0.115 / 0.116. Val. acc: 95.3%, Val. checks: 8/15\n",
            "Epoch: 9/100, iter: 2980. Loss (train/val): 0.112 / 0.113. Val. acc: 95.5%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2990. Loss (train/val): 0.113 / 0.114. Val. acc: 95.2%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 3000. Loss (train/val): 0.113 / 0.116. Val. acc: 95.2%, Val. checks: 2/15\n",
            "Epoch: 9/100, iter: 3010. Loss (train/val): 0.114 / 0.116. Val. acc: 95.3%, Val. checks: 3/15\n",
            "Epoch: 9/100, iter: 3020. Loss (train/val): 0.111 / 0.114. Val. acc: 95.3%, Val. checks: 4/15\n",
            "Epoch finished. Elapsed time 8.2020 [s]\n",
            "\n",
            "Epoch: 10/100, iter: 3030. Loss (train/val): 0.115 / 0.117. Val. acc: 95.9%, Val. checks: 5/15\n",
            "Epoch: 10/100, iter: 3040. Loss (train/val): 0.115 / 0.118. Val. acc: 95.6%, Val. checks: 6/15\n",
            "Epoch: 10/100, iter: 3050. Loss (train/val): 0.111 / 0.112. Val. acc: 95.1%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3060. Loss (train/val): 0.113 / 0.112. Val. acc: 95.8%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3070. Loss (train/val): 0.112 / 0.114. Val. acc: 95.5%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3080. Loss (train/val): 0.110 / 0.112. Val. acc: 95.1%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3090. Loss (train/val): 0.114 / 0.116. Val. acc: 95.9%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3100. Loss (train/val): 0.113 / 0.115. Val. acc: 95.6%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3110. Loss (train/val): 0.113 / 0.116. Val. acc: 95.7%, Val. checks: 3/15\n",
            "Epoch: 10/100, iter: 3120. Loss (train/val): 0.110 / 0.111. Val. acc: 95.7%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3130. Loss (train/val): 0.109 / 0.110. Val. acc: 95.5%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3140. Loss (train/val): 0.109 / 0.110. Val. acc: 95.8%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3150. Loss (train/val): 0.112 / 0.114. Val. acc: 95.8%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3160. Loss (train/val): 0.108 / 0.111. Val. acc: 95.7%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3170. Loss (train/val): 0.108 / 0.110. Val. acc: 95.6%, Val. checks: 3/15\n",
            "Epoch: 10/100, iter: 3180. Loss (train/val): 0.107 / 0.110. Val. acc: 95.7%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3190. Loss (train/val): 0.108 / 0.111. Val. acc: 95.7%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3200. Loss (train/val): 0.106 / 0.110. Val. acc: 95.8%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3210. Loss (train/val): 0.107 / 0.111. Val. acc: 95.7%, Val. checks: 3/15\n",
            "Epoch: 10/100, iter: 3220. Loss (train/val): 0.106 / 0.109. Val. acc: 95.8%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3230. Loss (train/val): 0.108 / 0.113. Val. acc: 95.6%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3240. Loss (train/val): 0.106 / 0.111. Val. acc: 95.5%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3250. Loss (train/val): 0.110 / 0.115. Val. acc: 95.1%, Val. checks: 3/15\n",
            "Epoch: 10/100, iter: 3260. Loss (train/val): 0.105 / 0.111. Val. acc: 95.7%, Val. checks: 4/15\n",
            "Epoch: 10/100, iter: 3270. Loss (train/val): 0.104 / 0.108. Val. acc: 95.7%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3280. Loss (train/val): 0.105 / 0.110. Val. acc: 95.7%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3290. Loss (train/val): 0.105 / 0.110. Val. acc: 95.6%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3300. Loss (train/val): 0.106 / 0.110. Val. acc: 95.5%, Val. checks: 3/15\n",
            "Epoch: 10/100, iter: 3310. Loss (train/val): 0.104 / 0.107. Val. acc: 95.6%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3320. Loss (train/val): 0.105 / 0.108. Val. acc: 95.9%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3330. Loss (train/val): 0.104 / 0.107. Val. acc: 95.8%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3340. Loss (train/val): 0.103 / 0.106. Val. acc: 95.8%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3350. Loss (train/val): 0.103 / 0.107. Val. acc: 95.6%, Val. checks: 1/15\n",
            "Epoch finished. Elapsed time 9.0634 [s]\n",
            "\n",
            "Epoch: 11/100, iter: 3360. Loss (train/val): 0.102 / 0.107. Val. acc: 95.6%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3370. Loss (train/val): 0.102 / 0.105. Val. acc: 95.8%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3380. Loss (train/val): 0.104 / 0.106. Val. acc: 95.9%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3390. Loss (train/val): 0.109 / 0.112. Val. acc: 96.0%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3400. Loss (train/val): 0.102 / 0.105. Val. acc: 95.5%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3410. Loss (train/val): 0.103 / 0.107. Val. acc: 95.8%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3420. Loss (train/val): 0.104 / 0.107. Val. acc: 95.2%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3430. Loss (train/val): 0.102 / 0.104. Val. acc: 96.3%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3440. Loss (train/val): 0.102 / 0.107. Val. acc: 95.8%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3450. Loss (train/val): 0.102 / 0.107. Val. acc: 96.0%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3460. Loss (train/val): 0.101 / 0.106. Val. acc: 95.6%, Val. checks: 3/15\n",
            "Epoch: 11/100, iter: 3470. Loss (train/val): 0.101 / 0.106. Val. acc: 95.9%, Val. checks: 4/15\n",
            "Epoch: 11/100, iter: 3480. Loss (train/val): 0.101 / 0.106. Val. acc: 95.8%, Val. checks: 5/15\n",
            "Epoch: 11/100, iter: 3490. Loss (train/val): 0.100 / 0.104. Val. acc: 96.1%, Val. checks: 6/15\n",
            "Epoch: 11/100, iter: 3500. Loss (train/val): 0.104 / 0.110. Val. acc: 95.3%, Val. checks: 7/15\n",
            "Epoch: 11/100, iter: 3510. Loss (train/val): 0.103 / 0.108. Val. acc: 95.5%, Val. checks: 8/15\n",
            "Epoch: 11/100, iter: 3520. Loss (train/val): 0.099 / 0.103. Val. acc: 95.9%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3530. Loss (train/val): 0.099 / 0.104. Val. acc: 95.9%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3540. Loss (train/val): 0.101 / 0.106. Val. acc: 95.9%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3550. Loss (train/val): 0.099 / 0.106. Val. acc: 95.9%, Val. checks: 3/15\n",
            "Epoch: 11/100, iter: 3560. Loss (train/val): 0.098 / 0.105. Val. acc: 95.7%, Val. checks: 4/15\n",
            "Epoch: 11/100, iter: 3570. Loss (train/val): 0.099 / 0.105. Val. acc: 95.9%, Val. checks: 5/15\n",
            "Epoch: 11/100, iter: 3580. Loss (train/val): 0.100 / 0.107. Val. acc: 95.9%, Val. checks: 6/15\n",
            "Epoch: 11/100, iter: 3590. Loss (train/val): 0.098 / 0.103. Val. acc: 95.8%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3600. Loss (train/val): 0.097 / 0.104. Val. acc: 95.8%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3610. Loss (train/val): 0.096 / 0.102. Val. acc: 95.9%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3620. Loss (train/val): 0.096 / 0.102. Val. acc: 95.7%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3630. Loss (train/val): 0.096 / 0.101. Val. acc: 95.8%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3640. Loss (train/val): 0.097 / 0.102. Val. acc: 95.8%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3650. Loss (train/val): 0.098 / 0.102. Val. acc: 96.4%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3660. Loss (train/val): 0.097 / 0.103. Val. acc: 95.7%, Val. checks: 3/15\n",
            "Epoch: 11/100, iter: 3670. Loss (train/val): 0.098 / 0.104. Val. acc: 95.8%, Val. checks: 4/15\n",
            "Epoch: 11/100, iter: 3680. Loss (train/val): 0.095 / 0.101. Val. acc: 95.8%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3690. Loss (train/val): 0.096 / 0.100. Val. acc: 96.0%, Val. checks: 0/15\n",
            "Epoch finished. Elapsed time 9.9591 [s]\n",
            "\n",
            "Epoch: 12/100, iter: 3700. Loss (train/val): 0.097 / 0.101. Val. acc: 95.9%, Val. checks: 1/15\n",
            "Epoch: 12/100, iter: 3710. Loss (train/val): 0.095 / 0.098. Val. acc: 96.0%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3720. Loss (train/val): 0.094 / 0.098. Val. acc: 96.0%, Val. checks: 1/15\n",
            "Epoch: 12/100, iter: 3730. Loss (train/val): 0.098 / 0.103. Val. acc: 96.1%, Val. checks: 2/15\n",
            "Epoch: 12/100, iter: 3740. Loss (train/val): 0.097 / 0.105. Val. acc: 95.8%, Val. checks: 3/15\n",
            "Epoch: 12/100, iter: 3750. Loss (train/val): 0.095 / 0.101. Val. acc: 96.0%, Val. checks: 4/15\n",
            "Epoch: 12/100, iter: 3760. Loss (train/val): 0.094 / 0.099. Val. acc: 95.9%, Val. checks: 5/15\n",
            "Epoch: 12/100, iter: 3770. Loss (train/val): 0.094 / 0.101. Val. acc: 95.9%, Val. checks: 6/15\n",
            "Epoch: 12/100, iter: 3780. Loss (train/val): 0.093 / 0.098. Val. acc: 96.0%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3790. Loss (train/val): 0.094 / 0.097. Val. acc: 95.9%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3800. Loss (train/val): 0.094 / 0.098. Val. acc: 95.9%, Val. checks: 1/15\n",
            "Epoch: 12/100, iter: 3810. Loss (train/val): 0.095 / 0.099. Val. acc: 95.9%, Val. checks: 2/15\n",
            "Epoch: 12/100, iter: 3820. Loss (train/val): 0.094 / 0.098. Val. acc: 96.5%, Val. checks: 3/15\n",
            "Epoch: 12/100, iter: 3830. Loss (train/val): 0.093 / 0.098. Val. acc: 96.1%, Val. checks: 4/15\n",
            "Epoch: 12/100, iter: 3840. Loss (train/val): 0.094 / 0.098. Val. acc: 96.1%, Val. checks: 5/15\n",
            "Epoch: 12/100, iter: 3850. Loss (train/val): 0.093 / 0.096. Val. acc: 96.3%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3860. Loss (train/val): 0.094 / 0.098. Val. acc: 96.4%, Val. checks: 1/15\n",
            "Epoch: 12/100, iter: 3870. Loss (train/val): 0.093 / 0.096. Val. acc: 95.8%, Val. checks: 2/15\n",
            "Epoch: 12/100, iter: 3880. Loss (train/val): 0.092 / 0.096. Val. acc: 95.8%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3890. Loss (train/val): 0.091 / 0.096. Val. acc: 96.1%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3900. Loss (train/val): 0.091 / 0.096. Val. acc: 95.7%, Val. checks: 1/15\n",
            "Epoch: 12/100, iter: 3910. Loss (train/val): 0.091 / 0.098. Val. acc: 96.1%, Val. checks: 2/15\n",
            "Epoch: 12/100, iter: 3920. Loss (train/val): 0.092 / 0.099. Val. acc: 95.8%, Val. checks: 3/15\n",
            "Epoch: 12/100, iter: 3930. Loss (train/val): 0.091 / 0.098. Val. acc: 96.1%, Val. checks: 4/15\n",
            "Epoch: 12/100, iter: 3940. Loss (train/val): 0.092 / 0.100. Val. acc: 96.3%, Val. checks: 5/15\n",
            "Epoch: 12/100, iter: 3950. Loss (train/val): 0.091 / 0.097. Val. acc: 96.3%, Val. checks: 6/15\n",
            "Epoch: 12/100, iter: 3960. Loss (train/val): 0.096 / 0.103. Val. acc: 95.8%, Val. checks: 7/15\n",
            "Epoch: 12/100, iter: 3970. Loss (train/val): 0.094 / 0.101. Val. acc: 96.0%, Val. checks: 8/15\n",
            "Epoch: 12/100, iter: 3980. Loss (train/val): 0.089 / 0.096. Val. acc: 96.0%, Val. checks: 9/15\n",
            "Epoch: 12/100, iter: 3990. Loss (train/val): 0.090 / 0.094. Val. acc: 95.8%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 4000. Loss (train/val): 0.089 / 0.095. Val. acc: 96.3%, Val. checks: 1/15\n",
            "Epoch: 12/100, iter: 4010. Loss (train/val): 0.089 / 0.096. Val. acc: 96.0%, Val. checks: 2/15\n",
            "Epoch: 12/100, iter: 4020. Loss (train/val): 0.093 / 0.100. Val. acc: 96.3%, Val. checks: 3/15\n",
            "Epoch: 12/100, iter: 4030. Loss (train/val): 0.090 / 0.096. Val. acc: 96.2%, Val. checks: 4/15\n",
            "Epoch finished. Elapsed time 10.8432 [s]\n",
            "\n",
            "Epoch: 13/100, iter: 4040. Loss (train/val): 0.089 / 0.097. Val. acc: 96.1%, Val. checks: 5/15\n",
            "Epoch: 13/100, iter: 4050. Loss (train/val): 0.088 / 0.096. Val. acc: 96.0%, Val. checks: 6/15\n",
            "Epoch: 13/100, iter: 4060. Loss (train/val): 0.094 / 0.101. Val. acc: 96.2%, Val. checks: 7/15\n",
            "Epoch: 13/100, iter: 4070. Loss (train/val): 0.090 / 0.097. Val. acc: 95.7%, Val. checks: 8/15\n",
            "Epoch: 13/100, iter: 4080. Loss (train/val): 0.088 / 0.097. Val. acc: 95.7%, Val. checks: 9/15\n",
            "Epoch: 13/100, iter: 4090. Loss (train/val): 0.088 / 0.098. Val. acc: 96.0%, Val. checks: 10/15\n",
            "Epoch: 13/100, iter: 4100. Loss (train/val): 0.087 / 0.097. Val. acc: 96.1%, Val. checks: 11/15\n",
            "Epoch: 13/100, iter: 4110. Loss (train/val): 0.088 / 0.097. Val. acc: 95.8%, Val. checks: 12/15\n",
            "Epoch: 13/100, iter: 4120. Loss (train/val): 0.087 / 0.095. Val. acc: 96.1%, Val. checks: 13/15\n",
            "Epoch: 13/100, iter: 4130. Loss (train/val): 0.088 / 0.096. Val. acc: 96.1%, Val. checks: 14/15\n",
            "Epoch: 13/100, iter: 4140. Loss (train/val): 0.091 / 0.101. Val. acc: 95.9%, Val. checks: 15/15\n",
            "Early stopping\n",
            "Epoch finished. Elapsed time 11.1358 [s]\n",
            "\n",
            "\n",
            "\n",
            "[Beginning training of MLP at logdir \"./tarea_1_logs/exp_xentropyrun_4\"]\n",
            "\n",
            "Epoch: 1/100, iter: 0. Loss (train/val): 0.689 / 0.690. Val. acc: 58.3%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 10. Loss (train/val): 0.616 / 0.616. Val. acc: 80.7%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 20. Loss (train/val): 0.561 / 0.560. Val. acc: 84.2%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 30. Loss (train/val): 0.518 / 0.519. Val. acc: 81.7%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 40. Loss (train/val): 0.511 / 0.514. Val. acc: 72.3%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 50. Loss (train/val): 0.449 / 0.450. Val. acc: 85.9%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 60. Loss (train/val): 0.423 / 0.422. Val. acc: 86.8%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 70. Loss (train/val): 0.402 / 0.402. Val. acc: 86.7%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 80. Loss (train/val): 0.384 / 0.383. Val. acc: 87.4%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 90. Loss (train/val): 0.367 / 0.367. Val. acc: 86.9%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 100. Loss (train/val): 0.353 / 0.352. Val. acc: 87.0%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 110. Loss (train/val): 0.348 / 0.343. Val. acc: 87.7%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 120. Loss (train/val): 0.343 / 0.343. Val. acc: 87.2%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 130. Loss (train/val): 0.328 / 0.323. Val. acc: 88.6%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 140. Loss (train/val): 0.331 / 0.329. Val. acc: 87.0%, Val. checks: 1/15\n",
            "Epoch: 1/100, iter: 150. Loss (train/val): 0.316 / 0.311. Val. acc: 88.5%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 160. Loss (train/val): 0.310 / 0.304. Val. acc: 88.7%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 170. Loss (train/val): 0.315 / 0.303. Val. acc: 88.2%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 180. Loss (train/val): 0.308 / 0.298. Val. acc: 89.3%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 190. Loss (train/val): 0.298 / 0.290. Val. acc: 89.2%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 200. Loss (train/val): 0.316 / 0.302. Val. acc: 88.5%, Val. checks: 1/15\n",
            "Epoch: 1/100, iter: 210. Loss (train/val): 0.291 / 0.282. Val. acc: 89.8%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 220. Loss (train/val): 0.288 / 0.281. Val. acc: 89.5%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 230. Loss (train/val): 0.292 / 0.282. Val. acc: 89.1%, Val. checks: 1/15\n",
            "Epoch: 1/100, iter: 240. Loss (train/val): 0.281 / 0.275. Val. acc: 89.2%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 250. Loss (train/val): 0.278 / 0.270. Val. acc: 89.2%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 260. Loss (train/val): 0.277 / 0.268. Val. acc: 89.6%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 270. Loss (train/val): 0.274 / 0.268. Val. acc: 88.7%, Val. checks: 1/15\n",
            "Epoch: 1/100, iter: 280. Loss (train/val): 0.272 / 0.266. Val. acc: 89.4%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 290. Loss (train/val): 0.270 / 0.265. Val. acc: 88.7%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 300. Loss (train/val): 0.271 / 0.262. Val. acc: 89.8%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 310. Loss (train/val): 0.271 / 0.270. Val. acc: 88.9%, Val. checks: 1/15\n",
            "Epoch: 1/100, iter: 320. Loss (train/val): 0.266 / 0.258. Val. acc: 89.8%, Val. checks: 0/15\n",
            "Epoch: 1/100, iter: 330. Loss (train/val): 0.264 / 0.257. Val. acc: 89.3%, Val. checks: 0/15\n",
            "Epoch finished. Elapsed time 0.9311 [s]\n",
            "\n",
            "Epoch: 2/100, iter: 340. Loss (train/val): 0.269 / 0.265. Val. acc: 89.3%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 350. Loss (train/val): 0.262 / 0.256. Val. acc: 89.8%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 360. Loss (train/val): 0.268 / 0.263. Val. acc: 89.2%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 370. Loss (train/val): 0.259 / 0.253. Val. acc: 88.9%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 380. Loss (train/val): 0.260 / 0.250. Val. acc: 90.5%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 390. Loss (train/val): 0.257 / 0.250. Val. acc: 89.5%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 400. Loss (train/val): 0.256 / 0.246. Val. acc: 90.3%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 410. Loss (train/val): 0.254 / 0.244. Val. acc: 89.6%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 420. Loss (train/val): 0.253 / 0.245. Val. acc: 89.9%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 430. Loss (train/val): 0.254 / 0.248. Val. acc: 89.6%, Val. checks: 2/15\n",
            "Epoch: 2/100, iter: 440. Loss (train/val): 0.252 / 0.245. Val. acc: 89.9%, Val. checks: 3/15\n",
            "Epoch: 2/100, iter: 450. Loss (train/val): 0.253 / 0.245. Val. acc: 89.6%, Val. checks: 4/15\n",
            "Epoch: 2/100, iter: 460. Loss (train/val): 0.265 / 0.260. Val. acc: 89.5%, Val. checks: 5/15\n",
            "Epoch: 2/100, iter: 470. Loss (train/val): 0.250 / 0.242. Val. acc: 89.6%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 480. Loss (train/val): 0.252 / 0.247. Val. acc: 89.9%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 490. Loss (train/val): 0.248 / 0.243. Val. acc: 89.9%, Val. checks: 2/15\n",
            "Epoch: 2/100, iter: 500. Loss (train/val): 0.249 / 0.241. Val. acc: 90.5%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 510. Loss (train/val): 0.246 / 0.239. Val. acc: 90.7%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 520. Loss (train/val): 0.245 / 0.239. Val. acc: 90.0%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 530. Loss (train/val): 0.244 / 0.235. Val. acc: 90.6%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 540. Loss (train/val): 0.244 / 0.235. Val. acc: 90.8%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 550. Loss (train/val): 0.242 / 0.235. Val. acc: 90.2%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 560. Loss (train/val): 0.246 / 0.236. Val. acc: 91.8%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 570. Loss (train/val): 0.261 / 0.247. Val. acc: 91.3%, Val. checks: 2/15\n",
            "Epoch: 2/100, iter: 580. Loss (train/val): 0.253 / 0.238. Val. acc: 92.0%, Val. checks: 3/15\n",
            "Epoch: 2/100, iter: 590. Loss (train/val): 0.244 / 0.233. Val. acc: 90.4%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 600. Loss (train/val): 0.242 / 0.231. Val. acc: 90.2%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 610. Loss (train/val): 0.247 / 0.239. Val. acc: 90.2%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 620. Loss (train/val): 0.242 / 0.230. Val. acc: 90.2%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 630. Loss (train/val): 0.238 / 0.224. Val. acc: 91.1%, Val. checks: 0/15\n",
            "Epoch: 2/100, iter: 640. Loss (train/val): 0.236 / 0.225. Val. acc: 90.8%, Val. checks: 1/15\n",
            "Epoch: 2/100, iter: 650. Loss (train/val): 0.238 / 0.225. Val. acc: 91.5%, Val. checks: 2/15\n",
            "Epoch: 2/100, iter: 660. Loss (train/val): 0.239 / 0.229. Val. acc: 90.4%, Val. checks: 3/15\n",
            "Epoch: 2/100, iter: 670. Loss (train/val): 0.236 / 0.223. Val. acc: 90.7%, Val. checks: 0/15\n",
            "Epoch finished. Elapsed time 1.9066 [s]\n",
            "\n",
            "Epoch: 3/100, iter: 680. Loss (train/val): 0.236 / 0.222. Val. acc: 91.0%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 690. Loss (train/val): 0.236 / 0.224. Val. acc: 91.0%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 700. Loss (train/val): 0.237 / 0.228. Val. acc: 90.7%, Val. checks: 2/15\n",
            "Epoch: 3/100, iter: 710. Loss (train/val): 0.232 / 0.222. Val. acc: 90.9%, Val. checks: 3/15\n",
            "Epoch: 3/100, iter: 720. Loss (train/val): 0.231 / 0.224. Val. acc: 90.5%, Val. checks: 4/15\n",
            "Epoch: 3/100, iter: 730. Loss (train/val): 0.235 / 0.227. Val. acc: 91.8%, Val. checks: 5/15\n",
            "Epoch: 3/100, iter: 740. Loss (train/val): 0.230 / 0.226. Val. acc: 90.8%, Val. checks: 6/15\n",
            "Epoch: 3/100, iter: 750. Loss (train/val): 0.232 / 0.228. Val. acc: 90.6%, Val. checks: 7/15\n",
            "Epoch: 3/100, iter: 760. Loss (train/val): 0.240 / 0.236. Val. acc: 89.3%, Val. checks: 8/15\n",
            "Epoch: 3/100, iter: 770. Loss (train/val): 0.233 / 0.226. Val. acc: 91.6%, Val. checks: 9/15\n",
            "Epoch: 3/100, iter: 780. Loss (train/val): 0.238 / 0.228. Val. acc: 92.2%, Val. checks: 10/15\n",
            "Epoch: 3/100, iter: 790. Loss (train/val): 0.228 / 0.221. Val. acc: 91.1%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 800. Loss (train/val): 0.231 / 0.224. Val. acc: 91.3%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 810. Loss (train/val): 0.232 / 0.226. Val. acc: 90.2%, Val. checks: 2/15\n",
            "Epoch: 3/100, iter: 820. Loss (train/val): 0.228 / 0.218. Val. acc: 92.0%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 830. Loss (train/val): 0.225 / 0.217. Val. acc: 90.8%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 840. Loss (train/val): 0.224 / 0.216. Val. acc: 92.1%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 850. Loss (train/val): 0.225 / 0.219. Val. acc: 90.7%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 860. Loss (train/val): 0.229 / 0.220. Val. acc: 92.2%, Val. checks: 2/15\n",
            "Epoch: 3/100, iter: 870. Loss (train/val): 0.233 / 0.227. Val. acc: 89.6%, Val. checks: 3/15\n",
            "Epoch: 3/100, iter: 880. Loss (train/val): 0.222 / 0.215. Val. acc: 91.7%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 890. Loss (train/val): 0.223 / 0.215. Val. acc: 92.0%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 900. Loss (train/val): 0.221 / 0.212. Val. acc: 92.2%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 910. Loss (train/val): 0.225 / 0.213. Val. acc: 92.7%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 920. Loss (train/val): 0.222 / 0.214. Val. acc: 91.3%, Val. checks: 2/15\n",
            "Epoch: 3/100, iter: 930. Loss (train/val): 0.227 / 0.216. Val. acc: 92.4%, Val. checks: 3/15\n",
            "Epoch: 3/100, iter: 940. Loss (train/val): 0.221 / 0.211. Val. acc: 92.4%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 950. Loss (train/val): 0.218 / 0.210. Val. acc: 92.5%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 960. Loss (train/val): 0.220 / 0.210. Val. acc: 92.1%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 970. Loss (train/val): 0.219 / 0.209. Val. acc: 92.1%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 980. Loss (train/val): 0.218 / 0.211. Val. acc: 92.0%, Val. checks: 1/15\n",
            "Epoch: 3/100, iter: 990. Loss (train/val): 0.215 / 0.206. Val. acc: 92.4%, Val. checks: 0/15\n",
            "Epoch: 3/100, iter: 1000. Loss (train/val): 0.230 / 0.221. Val. acc: 92.6%, Val. checks: 1/15\n",
            "Epoch finished. Elapsed time 2.7650 [s]\n",
            "\n",
            "Epoch: 4/100, iter: 1010. Loss (train/val): 0.222 / 0.214. Val. acc: 93.0%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1020. Loss (train/val): 0.218 / 0.211. Val. acc: 91.7%, Val. checks: 3/15\n",
            "Epoch: 4/100, iter: 1030. Loss (train/val): 0.213 / 0.204. Val. acc: 92.4%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1040. Loss (train/val): 0.212 / 0.204. Val. acc: 92.6%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1050. Loss (train/val): 0.213 / 0.205. Val. acc: 92.9%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1060. Loss (train/val): 0.223 / 0.214. Val. acc: 92.7%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1070. Loss (train/val): 0.211 / 0.205. Val. acc: 92.2%, Val. checks: 3/15\n",
            "Epoch: 4/100, iter: 1080. Loss (train/val): 0.216 / 0.210. Val. acc: 91.3%, Val. checks: 4/15\n",
            "Epoch: 4/100, iter: 1090. Loss (train/val): 0.218 / 0.210. Val. acc: 92.9%, Val. checks: 5/15\n",
            "Epoch: 4/100, iter: 1100. Loss (train/val): 0.216 / 0.210. Val. acc: 92.6%, Val. checks: 6/15\n",
            "Epoch: 4/100, iter: 1110. Loss (train/val): 0.211 / 0.205. Val. acc: 91.5%, Val. checks: 7/15\n",
            "Epoch: 4/100, iter: 1120. Loss (train/val): 0.207 / 0.201. Val. acc: 92.9%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1130. Loss (train/val): 0.206 / 0.200. Val. acc: 92.5%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1140. Loss (train/val): 0.222 / 0.216. Val. acc: 93.0%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1150. Loss (train/val): 0.206 / 0.200. Val. acc: 93.0%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1160. Loss (train/val): 0.205 / 0.199. Val. acc: 92.2%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1170. Loss (train/val): 0.207 / 0.201. Val. acc: 91.8%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1180. Loss (train/val): 0.203 / 0.197. Val. acc: 92.6%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1190. Loss (train/val): 0.202 / 0.195. Val. acc: 92.5%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1200. Loss (train/val): 0.201 / 0.194. Val. acc: 93.1%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1210. Loss (train/val): 0.205 / 0.197. Val. acc: 91.9%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1220. Loss (train/val): 0.207 / 0.200. Val. acc: 91.8%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1230. Loss (train/val): 0.200 / 0.193. Val. acc: 93.3%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1240. Loss (train/val): 0.201 / 0.195. Val. acc: 92.0%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1250. Loss (train/val): 0.203 / 0.196. Val. acc: 93.8%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1260. Loss (train/val): 0.197 / 0.192. Val. acc: 93.3%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1270. Loss (train/val): 0.201 / 0.194. Val. acc: 93.8%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1280. Loss (train/val): 0.196 / 0.190. Val. acc: 92.5%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1290. Loss (train/val): 0.195 / 0.190. Val. acc: 93.0%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1300. Loss (train/val): 0.194 / 0.189. Val. acc: 93.0%, Val. checks: 0/15\n",
            "Epoch: 4/100, iter: 1310. Loss (train/val): 0.194 / 0.192. Val. acc: 92.7%, Val. checks: 1/15\n",
            "Epoch: 4/100, iter: 1320. Loss (train/val): 0.195 / 0.195. Val. acc: 92.3%, Val. checks: 2/15\n",
            "Epoch: 4/100, iter: 1330. Loss (train/val): 0.194 / 0.193. Val. acc: 92.5%, Val. checks: 3/15\n",
            "Epoch: 4/100, iter: 1340. Loss (train/val): 0.192 / 0.191. Val. acc: 92.9%, Val. checks: 4/15\n",
            "Epoch finished. Elapsed time 3.6501 [s]\n",
            "\n",
            "Epoch: 5/100, iter: 1350. Loss (train/val): 0.192 / 0.189. Val. acc: 92.7%, Val. checks: 5/15\n",
            "Epoch: 5/100, iter: 1360. Loss (train/val): 0.191 / 0.189. Val. acc: 92.5%, Val. checks: 6/15\n",
            "Epoch: 5/100, iter: 1370. Loss (train/val): 0.193 / 0.192. Val. acc: 92.5%, Val. checks: 7/15\n",
            "Epoch: 5/100, iter: 1380. Loss (train/val): 0.195 / 0.194. Val. acc: 93.0%, Val. checks: 8/15\n",
            "Epoch: 5/100, iter: 1390. Loss (train/val): 0.191 / 0.190. Val. acc: 92.9%, Val. checks: 9/15\n",
            "Epoch: 5/100, iter: 1400. Loss (train/val): 0.188 / 0.187. Val. acc: 93.2%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1410. Loss (train/val): 0.188 / 0.188. Val. acc: 92.9%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1420. Loss (train/val): 0.188 / 0.188. Val. acc: 93.0%, Val. checks: 2/15\n",
            "Epoch: 5/100, iter: 1430. Loss (train/val): 0.193 / 0.192. Val. acc: 93.5%, Val. checks: 3/15\n",
            "Epoch: 5/100, iter: 1440. Loss (train/val): 0.188 / 0.186. Val. acc: 93.0%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1450. Loss (train/val): 0.185 / 0.183. Val. acc: 93.4%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1460. Loss (train/val): 0.184 / 0.182. Val. acc: 93.6%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1470. Loss (train/val): 0.185 / 0.184. Val. acc: 93.4%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1480. Loss (train/val): 0.193 / 0.192. Val. acc: 93.3%, Val. checks: 2/15\n",
            "Epoch: 5/100, iter: 1490. Loss (train/val): 0.184 / 0.181. Val. acc: 93.2%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1500. Loss (train/val): 0.185 / 0.184. Val. acc: 92.9%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1510. Loss (train/val): 0.182 / 0.183. Val. acc: 93.4%, Val. checks: 2/15\n",
            "Epoch: 5/100, iter: 1520. Loss (train/val): 0.181 / 0.182. Val. acc: 93.4%, Val. checks: 3/15\n",
            "Epoch: 5/100, iter: 1530. Loss (train/val): 0.180 / 0.181. Val. acc: 93.5%, Val. checks: 4/15\n",
            "Epoch: 5/100, iter: 1540. Loss (train/val): 0.180 / 0.179. Val. acc: 93.8%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1550. Loss (train/val): 0.181 / 0.179. Val. acc: 93.5%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1560. Loss (train/val): 0.181 / 0.180. Val. acc: 93.5%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1570. Loss (train/val): 0.185 / 0.184. Val. acc: 93.6%, Val. checks: 2/15\n",
            "Epoch: 5/100, iter: 1580. Loss (train/val): 0.181 / 0.180. Val. acc: 93.8%, Val. checks: 3/15\n",
            "Epoch: 5/100, iter: 1590. Loss (train/val): 0.179 / 0.178. Val. acc: 93.9%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1600. Loss (train/val): 0.176 / 0.174. Val. acc: 93.9%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1610. Loss (train/val): 0.175 / 0.174. Val. acc: 93.7%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1620. Loss (train/val): 0.175 / 0.173. Val. acc: 93.6%, Val. checks: 0/15\n",
            "Epoch: 5/100, iter: 1630. Loss (train/val): 0.182 / 0.180. Val. acc: 92.5%, Val. checks: 1/15\n",
            "Epoch: 5/100, iter: 1640. Loss (train/val): 0.176 / 0.175. Val. acc: 93.1%, Val. checks: 2/15\n",
            "Epoch: 5/100, iter: 1650. Loss (train/val): 0.185 / 0.184. Val. acc: 92.4%, Val. checks: 3/15\n",
            "Epoch: 5/100, iter: 1660. Loss (train/val): 0.177 / 0.176. Val. acc: 93.1%, Val. checks: 4/15\n",
            "Epoch: 5/100, iter: 1670. Loss (train/val): 0.190 / 0.188. Val. acc: 94.0%, Val. checks: 5/15\n",
            "Epoch finished. Elapsed time 4.5220 [s]\n",
            "\n",
            "Epoch: 6/100, iter: 1680. Loss (train/val): 0.172 / 0.169. Val. acc: 93.7%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1690. Loss (train/val): 0.171 / 0.169. Val. acc: 93.7%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1700. Loss (train/val): 0.175 / 0.174. Val. acc: 92.9%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1710. Loss (train/val): 0.174 / 0.174. Val. acc: 93.0%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 1720. Loss (train/val): 0.172 / 0.171. Val. acc: 93.4%, Val. checks: 3/15\n",
            "Epoch: 6/100, iter: 1730. Loss (train/val): 0.170 / 0.169. Val. acc: 93.5%, Val. checks: 4/15\n",
            "Epoch: 6/100, iter: 1740. Loss (train/val): 0.181 / 0.179. Val. acc: 92.2%, Val. checks: 5/15\n",
            "Epoch: 6/100, iter: 1750. Loss (train/val): 0.169 / 0.168. Val. acc: 93.9%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1760. Loss (train/val): 0.171 / 0.170. Val. acc: 94.4%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1770. Loss (train/val): 0.169 / 0.170. Val. acc: 93.2%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 1780. Loss (train/val): 0.167 / 0.166. Val. acc: 93.7%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1790. Loss (train/val): 0.170 / 0.170. Val. acc: 93.9%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1800. Loss (train/val): 0.166 / 0.166. Val. acc: 94.4%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1810. Loss (train/val): 0.165 / 0.166. Val. acc: 93.9%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1820. Loss (train/val): 0.165 / 0.167. Val. acc: 94.0%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1830. Loss (train/val): 0.172 / 0.172. Val. acc: 92.7%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 1840. Loss (train/val): 0.164 / 0.165. Val. acc: 94.2%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1850. Loss (train/val): 0.163 / 0.162. Val. acc: 93.5%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1860. Loss (train/val): 0.163 / 0.162. Val. acc: 93.3%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1870. Loss (train/val): 0.162 / 0.163. Val. acc: 93.5%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1880. Loss (train/val): 0.163 / 0.163. Val. acc: 93.4%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 1890. Loss (train/val): 0.164 / 0.162. Val. acc: 93.3%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1900. Loss (train/val): 0.160 / 0.159. Val. acc: 93.7%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1910. Loss (train/val): 0.159 / 0.158. Val. acc: 94.0%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1920. Loss (train/val): 0.158 / 0.158. Val. acc: 93.9%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1930. Loss (train/val): 0.158 / 0.157. Val. acc: 94.0%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1940. Loss (train/val): 0.162 / 0.161. Val. acc: 93.7%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1950. Loss (train/val): 0.157 / 0.158. Val. acc: 94.3%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 1960. Loss (train/val): 0.157 / 0.157. Val. acc: 93.7%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 1970. Loss (train/val): 0.160 / 0.160. Val. acc: 93.7%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 1980. Loss (train/val): 0.157 / 0.159. Val. acc: 94.2%, Val. checks: 2/15\n",
            "Epoch: 6/100, iter: 1990. Loss (train/val): 0.155 / 0.155. Val. acc: 94.4%, Val. checks: 0/15\n",
            "Epoch: 6/100, iter: 2000. Loss (train/val): 0.156 / 0.157. Val. acc: 94.4%, Val. checks: 1/15\n",
            "Epoch: 6/100, iter: 2010. Loss (train/val): 0.168 / 0.171. Val. acc: 93.9%, Val. checks: 2/15\n",
            "Epoch finished. Elapsed time 5.3985 [s]\n",
            "\n",
            "Epoch: 7/100, iter: 2020. Loss (train/val): 0.153 / 0.154. Val. acc: 94.6%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2030. Loss (train/val): 0.158 / 0.159. Val. acc: 94.5%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2040. Loss (train/val): 0.160 / 0.162. Val. acc: 94.8%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2050. Loss (train/val): 0.157 / 0.156. Val. acc: 94.0%, Val. checks: 3/15\n",
            "Epoch: 7/100, iter: 2060. Loss (train/val): 0.151 / 0.151. Val. acc: 94.3%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2070. Loss (train/val): 0.151 / 0.152. Val. acc: 94.0%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2080. Loss (train/val): 0.155 / 0.155. Val. acc: 94.0%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2090. Loss (train/val): 0.149 / 0.151. Val. acc: 94.0%, Val. checks: 3/15\n",
            "Epoch: 7/100, iter: 2100. Loss (train/val): 0.153 / 0.154. Val. acc: 94.0%, Val. checks: 4/15\n",
            "Epoch: 7/100, iter: 2110. Loss (train/val): 0.149 / 0.150. Val. acc: 94.3%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2120. Loss (train/val): 0.150 / 0.150. Val. acc: 94.2%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2130. Loss (train/val): 0.151 / 0.151. Val. acc: 94.0%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2140. Loss (train/val): 0.148 / 0.149. Val. acc: 94.5%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2150. Loss (train/val): 0.147 / 0.149. Val. acc: 94.2%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2160. Loss (train/val): 0.150 / 0.155. Val. acc: 94.3%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2170. Loss (train/val): 0.147 / 0.150. Val. acc: 94.0%, Val. checks: 3/15\n",
            "Epoch: 7/100, iter: 2180. Loss (train/val): 0.146 / 0.148. Val. acc: 94.5%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2190. Loss (train/val): 0.147 / 0.149. Val. acc: 94.2%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2200. Loss (train/val): 0.152 / 0.156. Val. acc: 94.6%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2210. Loss (train/val): 0.144 / 0.147. Val. acc: 93.8%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2220. Loss (train/val): 0.145 / 0.148. Val. acc: 94.0%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2230. Loss (train/val): 0.144 / 0.147. Val. acc: 94.6%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2240. Loss (train/val): 0.143 / 0.146. Val. acc: 94.6%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2250. Loss (train/val): 0.142 / 0.144. Val. acc: 94.2%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2260. Loss (train/val): 0.149 / 0.152. Val. acc: 94.6%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2270. Loss (train/val): 0.141 / 0.143. Val. acc: 94.5%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2280. Loss (train/val): 0.141 / 0.142. Val. acc: 94.5%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2290. Loss (train/val): 0.146 / 0.147. Val. acc: 94.2%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2300. Loss (train/val): 0.140 / 0.141. Val. acc: 94.4%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2310. Loss (train/val): 0.139 / 0.140. Val. acc: 94.4%, Val. checks: 0/15\n",
            "Epoch: 7/100, iter: 2320. Loss (train/val): 0.142 / 0.143. Val. acc: 93.9%, Val. checks: 1/15\n",
            "Epoch: 7/100, iter: 2330. Loss (train/val): 0.138 / 0.140. Val. acc: 94.4%, Val. checks: 2/15\n",
            "Epoch: 7/100, iter: 2340. Loss (train/val): 0.139 / 0.142. Val. acc: 94.7%, Val. checks: 3/15\n",
            "Epoch: 7/100, iter: 2350. Loss (train/val): 0.140 / 0.141. Val. acc: 94.3%, Val. checks: 4/15\n",
            "Epoch finished. Elapsed time 6.2957 [s]\n",
            "\n",
            "Epoch: 8/100, iter: 2360. Loss (train/val): 0.144 / 0.147. Val. acc: 93.9%, Val. checks: 5/15\n",
            "Epoch: 8/100, iter: 2370. Loss (train/val): 0.138 / 0.142. Val. acc: 94.4%, Val. checks: 6/15\n",
            "Epoch: 8/100, iter: 2380. Loss (train/val): 0.138 / 0.144. Val. acc: 94.0%, Val. checks: 7/15\n",
            "Epoch: 8/100, iter: 2390. Loss (train/val): 0.138 / 0.142. Val. acc: 94.0%, Val. checks: 8/15\n",
            "Epoch: 8/100, iter: 2400. Loss (train/val): 0.136 / 0.141. Val. acc: 94.4%, Val. checks: 9/15\n",
            "Epoch: 8/100, iter: 2410. Loss (train/val): 0.136 / 0.140. Val. acc: 94.8%, Val. checks: 10/15\n",
            "Epoch: 8/100, iter: 2420. Loss (train/val): 0.144 / 0.149. Val. acc: 95.0%, Val. checks: 11/15\n",
            "Epoch: 8/100, iter: 2430. Loss (train/val): 0.143 / 0.144. Val. acc: 94.3%, Val. checks: 12/15\n",
            "Epoch: 8/100, iter: 2440. Loss (train/val): 0.136 / 0.139. Val. acc: 95.1%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2450. Loss (train/val): 0.135 / 0.137. Val. acc: 95.0%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2460. Loss (train/val): 0.146 / 0.150. Val. acc: 94.9%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2470. Loss (train/val): 0.133 / 0.135. Val. acc: 94.3%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2480. Loss (train/val): 0.136 / 0.139. Val. acc: 95.1%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2490. Loss (train/val): 0.134 / 0.135. Val. acc: 94.5%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2500. Loss (train/val): 0.131 / 0.134. Val. acc: 94.5%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2510. Loss (train/val): 0.131 / 0.134. Val. acc: 94.3%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2520. Loss (train/val): 0.131 / 0.135. Val. acc: 94.9%, Val. checks: 2/15\n",
            "Epoch: 8/100, iter: 2530. Loss (train/val): 0.131 / 0.134. Val. acc: 94.4%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2540. Loss (train/val): 0.132 / 0.135. Val. acc: 94.6%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2550. Loss (train/val): 0.131 / 0.136. Val. acc: 94.8%, Val. checks: 2/15\n",
            "Epoch: 8/100, iter: 2560. Loss (train/val): 0.141 / 0.143. Val. acc: 94.6%, Val. checks: 3/15\n",
            "Epoch: 8/100, iter: 2570. Loss (train/val): 0.129 / 0.132. Val. acc: 94.3%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2580. Loss (train/val): 0.130 / 0.132. Val. acc: 94.6%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2590. Loss (train/val): 0.129 / 0.131. Val. acc: 95.0%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2600. Loss (train/val): 0.128 / 0.130. Val. acc: 94.8%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2610. Loss (train/val): 0.130 / 0.131. Val. acc: 94.8%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2620. Loss (train/val): 0.132 / 0.135. Val. acc: 95.1%, Val. checks: 2/15\n",
            "Epoch: 8/100, iter: 2630. Loss (train/val): 0.128 / 0.130. Val. acc: 94.6%, Val. checks: 3/15\n",
            "Epoch: 8/100, iter: 2640. Loss (train/val): 0.127 / 0.131. Val. acc: 94.6%, Val. checks: 4/15\n",
            "Epoch: 8/100, iter: 2650. Loss (train/val): 0.131 / 0.133. Val. acc: 94.5%, Val. checks: 5/15\n",
            "Epoch: 8/100, iter: 2660. Loss (train/val): 0.127 / 0.128. Val. acc: 94.8%, Val. checks: 0/15\n",
            "Epoch: 8/100, iter: 2670. Loss (train/val): 0.125 / 0.128. Val. acc: 94.8%, Val. checks: 1/15\n",
            "Epoch: 8/100, iter: 2680. Loss (train/val): 0.128 / 0.131. Val. acc: 94.5%, Val. checks: 2/15\n",
            "Epoch finished. Elapsed time 7.2582 [s]\n",
            "\n",
            "Epoch: 9/100, iter: 2690. Loss (train/val): 0.124 / 0.129. Val. acc: 95.1%, Val. checks: 3/15\n",
            "Epoch: 9/100, iter: 2700. Loss (train/val): 0.125 / 0.132. Val. acc: 94.6%, Val. checks: 4/15\n",
            "Epoch: 9/100, iter: 2710. Loss (train/val): 0.125 / 0.132. Val. acc: 95.0%, Val. checks: 5/15\n",
            "Epoch: 9/100, iter: 2720. Loss (train/val): 0.123 / 0.127. Val. acc: 95.2%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2730. Loss (train/val): 0.129 / 0.131. Val. acc: 95.0%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2740. Loss (train/val): 0.122 / 0.127. Val. acc: 94.7%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2750. Loss (train/val): 0.124 / 0.129. Val. acc: 95.3%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2760. Loss (train/val): 0.123 / 0.126. Val. acc: 94.7%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2770. Loss (train/val): 0.126 / 0.131. Val. acc: 95.3%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2780. Loss (train/val): 0.123 / 0.126. Val. acc: 95.0%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2790. Loss (train/val): 0.124 / 0.130. Val. acc: 95.5%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2800. Loss (train/val): 0.130 / 0.137. Val. acc: 95.5%, Val. checks: 2/15\n",
            "Epoch: 9/100, iter: 2810. Loss (train/val): 0.121 / 0.123. Val. acc: 95.2%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2820. Loss (train/val): 0.121 / 0.124. Val. acc: 95.0%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2830. Loss (train/val): 0.120 / 0.123. Val. acc: 95.0%, Val. checks: 2/15\n",
            "Epoch: 9/100, iter: 2840. Loss (train/val): 0.120 / 0.122. Val. acc: 95.3%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2850. Loss (train/val): 0.122 / 0.123. Val. acc: 95.2%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2860. Loss (train/val): 0.128 / 0.135. Val. acc: 95.5%, Val. checks: 2/15\n",
            "Epoch: 9/100, iter: 2870. Loss (train/val): 0.118 / 0.123. Val. acc: 95.1%, Val. checks: 3/15\n",
            "Epoch: 9/100, iter: 2880. Loss (train/val): 0.118 / 0.123. Val. acc: 95.0%, Val. checks: 4/15\n",
            "Epoch: 9/100, iter: 2890. Loss (train/val): 0.118 / 0.123. Val. acc: 94.8%, Val. checks: 5/15\n",
            "Epoch: 9/100, iter: 2900. Loss (train/val): 0.117 / 0.124. Val. acc: 94.8%, Val. checks: 6/15\n",
            "Epoch: 9/100, iter: 2910. Loss (train/val): 0.117 / 0.122. Val. acc: 94.9%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2920. Loss (train/val): 0.116 / 0.121. Val. acc: 94.9%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 2930. Loss (train/val): 0.117 / 0.123. Val. acc: 95.0%, Val. checks: 1/15\n",
            "Epoch: 9/100, iter: 2940. Loss (train/val): 0.116 / 0.122. Val. acc: 94.8%, Val. checks: 2/15\n",
            "Epoch: 9/100, iter: 2950. Loss (train/val): 0.124 / 0.128. Val. acc: 94.9%, Val. checks: 3/15\n",
            "Epoch: 9/100, iter: 2960. Loss (train/val): 0.117 / 0.126. Val. acc: 95.6%, Val. checks: 4/15\n",
            "Epoch: 9/100, iter: 2970. Loss (train/val): 0.116 / 0.123. Val. acc: 95.1%, Val. checks: 5/15\n",
            "Epoch: 9/100, iter: 2980. Loss (train/val): 0.116 / 0.123. Val. acc: 94.8%, Val. checks: 6/15\n",
            "Epoch: 9/100, iter: 2990. Loss (train/val): 0.120 / 0.128. Val. acc: 95.6%, Val. checks: 7/15\n",
            "Epoch: 9/100, iter: 3000. Loss (train/val): 0.117 / 0.123. Val. acc: 95.5%, Val. checks: 8/15\n",
            "Epoch: 9/100, iter: 3010. Loss (train/val): 0.114 / 0.119. Val. acc: 95.2%, Val. checks: 0/15\n",
            "Epoch: 9/100, iter: 3020. Loss (train/val): 0.114 / 0.119. Val. acc: 95.5%, Val. checks: 1/15\n",
            "Epoch finished. Elapsed time 8.1317 [s]\n",
            "\n",
            "Epoch: 10/100, iter: 3030. Loss (train/val): 0.121 / 0.129. Val. acc: 95.5%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3040. Loss (train/val): 0.113 / 0.121. Val. acc: 95.6%, Val. checks: 3/15\n",
            "Epoch: 10/100, iter: 3050. Loss (train/val): 0.113 / 0.118. Val. acc: 95.6%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3060. Loss (train/val): 0.114 / 0.118. Val. acc: 95.3%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3070. Loss (train/val): 0.117 / 0.123. Val. acc: 95.0%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3080. Loss (train/val): 0.113 / 0.120. Val. acc: 94.9%, Val. checks: 3/15\n",
            "Epoch: 10/100, iter: 3090. Loss (train/val): 0.113 / 0.122. Val. acc: 95.2%, Val. checks: 4/15\n",
            "Epoch: 10/100, iter: 3100. Loss (train/val): 0.113 / 0.122. Val. acc: 95.7%, Val. checks: 5/15\n",
            "Epoch: 10/100, iter: 3110. Loss (train/val): 0.111 / 0.118. Val. acc: 95.2%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3120. Loss (train/val): 0.114 / 0.122. Val. acc: 95.6%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3130. Loss (train/val): 0.112 / 0.120. Val. acc: 95.5%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3140. Loss (train/val): 0.111 / 0.117. Val. acc: 95.3%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3150. Loss (train/val): 0.110 / 0.115. Val. acc: 95.3%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3160. Loss (train/val): 0.110 / 0.114. Val. acc: 95.2%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3170. Loss (train/val): 0.109 / 0.116. Val. acc: 95.3%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3180. Loss (train/val): 0.109 / 0.115. Val. acc: 95.5%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3190. Loss (train/val): 0.109 / 0.115. Val. acc: 95.1%, Val. checks: 3/15\n",
            "Epoch: 10/100, iter: 3200. Loss (train/val): 0.109 / 0.115. Val. acc: 95.6%, Val. checks: 4/15\n",
            "Epoch: 10/100, iter: 3210. Loss (train/val): 0.108 / 0.115. Val. acc: 95.3%, Val. checks: 5/15\n",
            "Epoch: 10/100, iter: 3220. Loss (train/val): 0.108 / 0.116. Val. acc: 95.0%, Val. checks: 6/15\n",
            "Epoch: 10/100, iter: 3230. Loss (train/val): 0.108 / 0.115. Val. acc: 95.1%, Val. checks: 7/15\n",
            "Epoch: 10/100, iter: 3240. Loss (train/val): 0.109 / 0.117. Val. acc: 95.6%, Val. checks: 8/15\n",
            "Epoch: 10/100, iter: 3250. Loss (train/val): 0.107 / 0.115. Val. acc: 95.1%, Val. checks: 9/15\n",
            "Epoch: 10/100, iter: 3260. Loss (train/val): 0.107 / 0.113. Val. acc: 95.5%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3270. Loss (train/val): 0.111 / 0.117. Val. acc: 95.8%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3280. Loss (train/val): 0.109 / 0.112. Val. acc: 95.7%, Val. checks: 0/15\n",
            "Epoch: 10/100, iter: 3290. Loss (train/val): 0.108 / 0.113. Val. acc: 95.6%, Val. checks: 1/15\n",
            "Epoch: 10/100, iter: 3300. Loss (train/val): 0.106 / 0.113. Val. acc: 95.3%, Val. checks: 2/15\n",
            "Epoch: 10/100, iter: 3310. Loss (train/val): 0.111 / 0.114. Val. acc: 95.3%, Val. checks: 3/15\n",
            "Epoch: 10/100, iter: 3320. Loss (train/val): 0.107 / 0.116. Val. acc: 95.7%, Val. checks: 4/15\n",
            "Epoch: 10/100, iter: 3330. Loss (train/val): 0.107 / 0.114. Val. acc: 95.6%, Val. checks: 5/15\n",
            "Epoch: 10/100, iter: 3340. Loss (train/val): 0.108 / 0.118. Val. acc: 95.8%, Val. checks: 6/15\n",
            "Epoch: 10/100, iter: 3350. Loss (train/val): 0.109 / 0.114. Val. acc: 95.5%, Val. checks: 7/15\n",
            "Epoch finished. Elapsed time 8.9929 [s]\n",
            "\n",
            "Epoch: 11/100, iter: 3360. Loss (train/val): 0.111 / 0.124. Val. acc: 95.5%, Val. checks: 8/15\n",
            "Epoch: 11/100, iter: 3370. Loss (train/val): 0.105 / 0.115. Val. acc: 95.2%, Val. checks: 9/15\n",
            "Epoch: 11/100, iter: 3380. Loss (train/val): 0.104 / 0.114. Val. acc: 95.3%, Val. checks: 10/15\n",
            "Epoch: 11/100, iter: 3390. Loss (train/val): 0.103 / 0.112. Val. acc: 95.5%, Val. checks: 11/15\n",
            "Epoch: 11/100, iter: 3400. Loss (train/val): 0.104 / 0.111. Val. acc: 95.7%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3410. Loss (train/val): 0.105 / 0.112. Val. acc: 95.6%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3420. Loss (train/val): 0.105 / 0.116. Val. acc: 96.1%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3430. Loss (train/val): 0.104 / 0.113. Val. acc: 96.1%, Val. checks: 3/15\n",
            "Epoch: 11/100, iter: 3440. Loss (train/val): 0.102 / 0.111. Val. acc: 95.6%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3450. Loss (train/val): 0.113 / 0.120. Val. acc: 95.5%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3460. Loss (train/val): 0.102 / 0.112. Val. acc: 95.6%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3470. Loss (train/val): 0.103 / 0.116. Val. acc: 95.7%, Val. checks: 3/15\n",
            "Epoch: 11/100, iter: 3480. Loss (train/val): 0.102 / 0.116. Val. acc: 95.9%, Val. checks: 4/15\n",
            "Epoch: 11/100, iter: 3490. Loss (train/val): 0.102 / 0.114. Val. acc: 95.8%, Val. checks: 5/15\n",
            "Epoch: 11/100, iter: 3500. Loss (train/val): 0.107 / 0.121. Val. acc: 96.0%, Val. checks: 6/15\n",
            "Epoch: 11/100, iter: 3510. Loss (train/val): 0.101 / 0.110. Val. acc: 95.6%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3520. Loss (train/val): 0.101 / 0.108. Val. acc: 95.6%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3530. Loss (train/val): 0.103 / 0.109. Val. acc: 95.7%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3540. Loss (train/val): 0.100 / 0.110. Val. acc: 95.8%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3550. Loss (train/val): 0.107 / 0.120. Val. acc: 95.8%, Val. checks: 3/15\n",
            "Epoch: 11/100, iter: 3560. Loss (train/val): 0.100 / 0.112. Val. acc: 96.0%, Val. checks: 4/15\n",
            "Epoch: 11/100, iter: 3570. Loss (train/val): 0.101 / 0.110. Val. acc: 95.7%, Val. checks: 5/15\n",
            "Epoch: 11/100, iter: 3580. Loss (train/val): 0.102 / 0.116. Val. acc: 95.6%, Val. checks: 6/15\n",
            "Epoch: 11/100, iter: 3590. Loss (train/val): 0.099 / 0.110. Val. acc: 95.9%, Val. checks: 7/15\n",
            "Epoch: 11/100, iter: 3600. Loss (train/val): 0.100 / 0.111. Val. acc: 95.8%, Val. checks: 8/15\n",
            "Epoch: 11/100, iter: 3610. Loss (train/val): 0.105 / 0.111. Val. acc: 95.6%, Val. checks: 9/15\n",
            "Epoch: 11/100, iter: 3620. Loss (train/val): 0.099 / 0.107. Val. acc: 95.9%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3630. Loss (train/val): 0.098 / 0.107. Val. acc: 95.9%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3640. Loss (train/val): 0.098 / 0.106. Val. acc: 95.7%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3650. Loss (train/val): 0.098 / 0.109. Val. acc: 95.8%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3660. Loss (train/val): 0.098 / 0.105. Val. acc: 95.7%, Val. checks: 0/15\n",
            "Epoch: 11/100, iter: 3670. Loss (train/val): 0.102 / 0.114. Val. acc: 96.3%, Val. checks: 1/15\n",
            "Epoch: 11/100, iter: 3680. Loss (train/val): 0.100 / 0.110. Val. acc: 96.1%, Val. checks: 2/15\n",
            "Epoch: 11/100, iter: 3690. Loss (train/val): 0.097 / 0.104. Val. acc: 95.8%, Val. checks: 0/15\n",
            "Epoch finished. Elapsed time 9.8755 [s]\n",
            "\n",
            "Epoch: 12/100, iter: 3700. Loss (train/val): 0.099 / 0.104. Val. acc: 95.8%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3710. Loss (train/val): 0.096 / 0.105. Val. acc: 95.8%, Val. checks: 1/15\n",
            "Epoch: 12/100, iter: 3720. Loss (train/val): 0.098 / 0.106. Val. acc: 95.9%, Val. checks: 2/15\n",
            "Epoch: 12/100, iter: 3730. Loss (train/val): 0.097 / 0.106. Val. acc: 95.8%, Val. checks: 3/15\n",
            "Epoch: 12/100, iter: 3740. Loss (train/val): 0.096 / 0.105. Val. acc: 95.9%, Val. checks: 4/15\n",
            "Epoch: 12/100, iter: 3750. Loss (train/val): 0.096 / 0.105. Val. acc: 95.7%, Val. checks: 5/15\n",
            "Epoch: 12/100, iter: 3760. Loss (train/val): 0.096 / 0.105. Val. acc: 95.7%, Val. checks: 6/15\n",
            "Epoch: 12/100, iter: 3770. Loss (train/val): 0.095 / 0.106. Val. acc: 95.7%, Val. checks: 7/15\n",
            "Epoch: 12/100, iter: 3780. Loss (train/val): 0.095 / 0.108. Val. acc: 96.0%, Val. checks: 8/15\n",
            "Epoch: 12/100, iter: 3790. Loss (train/val): 0.104 / 0.119. Val. acc: 95.8%, Val. checks: 9/15\n",
            "Epoch: 12/100, iter: 3800. Loss (train/val): 0.099 / 0.112. Val. acc: 96.1%, Val. checks: 10/15\n",
            "Epoch: 12/100, iter: 3810. Loss (train/val): 0.097 / 0.110. Val. acc: 96.1%, Val. checks: 11/15\n",
            "Epoch: 12/100, iter: 3820. Loss (train/val): 0.094 / 0.104. Val. acc: 96.0%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3830. Loss (train/val): 0.093 / 0.104. Val. acc: 95.9%, Val. checks: 1/15\n",
            "Epoch: 12/100, iter: 3840. Loss (train/val): 0.095 / 0.108. Val. acc: 96.2%, Val. checks: 2/15\n",
            "Epoch: 12/100, iter: 3850. Loss (train/val): 0.093 / 0.104. Val. acc: 95.8%, Val. checks: 3/15\n",
            "Epoch: 12/100, iter: 3860. Loss (train/val): 0.094 / 0.104. Val. acc: 96.1%, Val. checks: 4/15\n",
            "Epoch: 12/100, iter: 3870. Loss (train/val): 0.093 / 0.103. Val. acc: 95.7%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3880. Loss (train/val): 0.092 / 0.103. Val. acc: 95.9%, Val. checks: 1/15\n",
            "Epoch: 12/100, iter: 3890. Loss (train/val): 0.092 / 0.102. Val. acc: 95.9%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 3900. Loss (train/val): 0.093 / 0.106. Val. acc: 96.0%, Val. checks: 1/15\n",
            "Epoch: 12/100, iter: 3910. Loss (train/val): 0.092 / 0.105. Val. acc: 96.1%, Val. checks: 2/15\n",
            "Epoch: 12/100, iter: 3920. Loss (train/val): 0.094 / 0.104. Val. acc: 95.8%, Val. checks: 3/15\n",
            "Epoch: 12/100, iter: 3930. Loss (train/val): 0.092 / 0.105. Val. acc: 96.0%, Val. checks: 4/15\n",
            "Epoch: 12/100, iter: 3940. Loss (train/val): 0.096 / 0.106. Val. acc: 95.6%, Val. checks: 5/15\n",
            "Epoch: 12/100, iter: 3950. Loss (train/val): 0.093 / 0.108. Val. acc: 95.9%, Val. checks: 6/15\n",
            "Epoch: 12/100, iter: 3960. Loss (train/val): 0.092 / 0.106. Val. acc: 95.9%, Val. checks: 7/15\n",
            "Epoch: 12/100, iter: 3970. Loss (train/val): 0.092 / 0.104. Val. acc: 96.2%, Val. checks: 8/15\n",
            "Epoch: 12/100, iter: 3980. Loss (train/val): 0.094 / 0.103. Val. acc: 95.7%, Val. checks: 9/15\n",
            "Epoch: 12/100, iter: 3990. Loss (train/val): 0.093 / 0.102. Val. acc: 96.0%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 4000. Loss (train/val): 0.092 / 0.103. Val. acc: 95.9%, Val. checks: 1/15\n",
            "Epoch: 12/100, iter: 4010. Loss (train/val): 0.091 / 0.101. Val. acc: 96.1%, Val. checks: 0/15\n",
            "Epoch: 12/100, iter: 4020. Loss (train/val): 0.091 / 0.104. Val. acc: 96.3%, Val. checks: 1/15\n",
            "Epoch: 12/100, iter: 4030. Loss (train/val): 0.090 / 0.102. Val. acc: 96.2%, Val. checks: 2/15\n",
            "Epoch finished. Elapsed time 10.7577 [s]\n",
            "\n",
            "Epoch: 13/100, iter: 4040. Loss (train/val): 0.092 / 0.101. Val. acc: 96.0%, Val. checks: 0/15\n",
            "Epoch: 13/100, iter: 4050. Loss (train/val): 0.096 / 0.109. Val. acc: 96.1%, Val. checks: 1/15\n",
            "Epoch: 13/100, iter: 4060. Loss (train/val): 0.091 / 0.101. Val. acc: 96.1%, Val. checks: 0/15\n",
            "Epoch: 13/100, iter: 4070. Loss (train/val): 0.089 / 0.100. Val. acc: 96.1%, Val. checks: 0/15\n",
            "Epoch: 13/100, iter: 4080. Loss (train/val): 0.098 / 0.103. Val. acc: 96.1%, Val. checks: 1/15\n",
            "Epoch: 13/100, iter: 4090. Loss (train/val): 0.094 / 0.108. Val. acc: 96.3%, Val. checks: 2/15\n",
            "Epoch: 13/100, iter: 4100. Loss (train/val): 0.090 / 0.102. Val. acc: 96.5%, Val. checks: 3/15\n",
            "Epoch: 13/100, iter: 4110. Loss (train/val): 0.089 / 0.099. Val. acc: 96.0%, Val. checks: 0/15\n",
            "Epoch: 13/100, iter: 4120. Loss (train/val): 0.088 / 0.099. Val. acc: 96.1%, Val. checks: 1/15\n",
            "Epoch: 13/100, iter: 4130. Loss (train/val): 0.089 / 0.099. Val. acc: 96.3%, Val. checks: 2/15\n",
            "Epoch: 13/100, iter: 4140. Loss (train/val): 0.088 / 0.100. Val. acc: 95.9%, Val. checks: 3/15\n",
            "Epoch: 13/100, iter: 4150. Loss (train/val): 0.089 / 0.100. Val. acc: 96.0%, Val. checks: 4/15\n",
            "Epoch: 13/100, iter: 4160. Loss (train/val): 0.090 / 0.100. Val. acc: 96.2%, Val. checks: 5/15\n",
            "Epoch: 13/100, iter: 4170. Loss (train/val): 0.089 / 0.098. Val. acc: 96.1%, Val. checks: 0/15\n",
            "Epoch: 13/100, iter: 4180. Loss (train/val): 0.089 / 0.099. Val. acc: 96.2%, Val. checks: 1/15\n",
            "Epoch: 13/100, iter: 4190. Loss (train/val): 0.088 / 0.101. Val. acc: 96.2%, Val. checks: 2/15\n",
            "Epoch: 13/100, iter: 4200. Loss (train/val): 0.090 / 0.107. Val. acc: 96.0%, Val. checks: 3/15\n",
            "Epoch: 13/100, iter: 4210. Loss (train/val): 0.088 / 0.101. Val. acc: 96.0%, Val. checks: 4/15\n",
            "Epoch: 13/100, iter: 4220. Loss (train/val): 0.086 / 0.099. Val. acc: 96.1%, Val. checks: 5/15\n",
            "Epoch: 13/100, iter: 4230. Loss (train/val): 0.088 / 0.100. Val. acc: 96.1%, Val. checks: 6/15\n",
            "Epoch: 13/100, iter: 4240. Loss (train/val): 0.088 / 0.103. Val. acc: 95.9%, Val. checks: 7/15\n",
            "Epoch: 13/100, iter: 4250. Loss (train/val): 0.086 / 0.099. Val. acc: 95.9%, Val. checks: 8/15\n",
            "Epoch: 13/100, iter: 4260. Loss (train/val): 0.088 / 0.103. Val. acc: 96.1%, Val. checks: 9/15\n",
            "Epoch: 13/100, iter: 4270. Loss (train/val): 0.089 / 0.105. Val. acc: 96.0%, Val. checks: 10/15\n",
            "Epoch: 13/100, iter: 4280. Loss (train/val): 0.086 / 0.099. Val. acc: 96.2%, Val. checks: 11/15\n",
            "Epoch: 13/100, iter: 4290. Loss (train/val): 0.086 / 0.099. Val. acc: 96.0%, Val. checks: 12/15\n",
            "Epoch: 13/100, iter: 4300. Loss (train/val): 0.085 / 0.102. Val. acc: 96.1%, Val. checks: 13/15\n",
            "Epoch: 13/100, iter: 4310. Loss (train/val): 0.086 / 0.099. Val. acc: 96.2%, Val. checks: 14/15\n",
            "Epoch: 13/100, iter: 4320. Loss (train/val): 0.085 / 0.098. Val. acc: 96.2%, Val. checks: 15/15\n",
            "Early stopping\n",
            "Epoch finished. Elapsed time 11.5129 [s]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uszFkplq3avX"
      },
      "source": [
        "### Tasa de acierto en validación computada sobre varias ejecuciones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HNS4VAkflQvE",
        "outputId": "7f574bf1-4b78-4e9f-c152-3faa15c0a08e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "val_acc_history = np.array([run['val_acc_history'][-1] for run in stats_history])\n",
        "val_acc_mean = val_acc_history.mean()\n",
        "val_acc_std = val_acc_history.std()\n",
        "print('Validation accuracy %.3f +/- %.3f' % (val_acc_mean, val_acc_std))\n",
        "print(val_acc_history)\n",
        "\n",
        "train_acc_history = np.array([run['train_acc_history'][-1] for run in stats_history])\n",
        "train_acc_mean = train_acc_history.mean()\n",
        "train_acc_std = train_acc_history.std()\n",
        "print('Train accuracy %.3f +/- %.3f' % (train_acc_mean, train_acc_std))\n",
        "print(train_acc_history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation accuracy 0.960 +/- 0.003\n",
            "[0.9556277  0.95671    0.96428573 0.9599567  0.9632035 ]\n",
            "Train accuracy 0.966 +/- 0.004\n",
            "[0.95964    0.9662275  0.97049546 0.9658564  0.9692893 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tDrpTQWXa19D"
      },
      "source": [
        "## Visualización de Resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8ypv7zhabDJT"
      },
      "source": [
        "### Algunas estadísticas del entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XsDDF3BoboJS",
        "outputId": "266021e0-4be9-4a75-9cf4-b54a1aaf8df9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "#train_stats = train_stats_ce\n",
        "#train_stats = train_stats_mse\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(13, 5))\n",
        "\n",
        "ax[0].plot(train_stats_ce['iteration_history'], train_stats_ce['val_loss_history'], label='val ce')\n",
        "ax[0].plot(train_stats_ce['iteration_history'], train_stats_ce['train_loss_history'], label='train ce')\n",
        "ax[0].plot(train_stats_mse['iteration_history'], train_stats_mse['val_loss_history'], label='val mse')\n",
        "ax[0].plot(train_stats_mse['iteration_history'], train_stats_mse['train_loss_history'], label='train mse')\n",
        "ax[0].set_xlabel('Iteration')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_title('Loss evolution during training')\n",
        "ax[0].set_ylim(0,1)\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(train_stats_ce['iteration_history'], train_stats_ce['val_acc_history'], label='val ce')\n",
        "ax[1].plot(train_stats_ce['iteration_history'], train_stats_ce['train_acc_history'], label='train ce')\n",
        "ax[1].plot(train_stats_mse['iteration_history'], train_stats_mse['val_acc_history'], label='val mse')\n",
        "ax[1].plot(train_stats_mse['iteration_history'], train_stats_mse['train_acc_history'], label='train mse')\n",
        "ax[1].set_xlabel('Iteration')\n",
        "ax[1].set_ylabel('Accuracy')\n",
        "ax[1].set_title('Accuracy evolution during training')\n",
        "ax[1].set_ylim(0,1)\n",
        "ax[1].legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6f76122908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwwAAAFNCAYAAACt5OibAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMIiIiIiIyITUYRARERERkQn9HxraoYq/fQotAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 936x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PF87rmUxbMtW"
      },
      "source": [
        "### Estadísticas del desempeño final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XKM3ashMUVZa",
        "outputId": "f924ef10-9ed2-42e6-b025-f476c676cb3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def confusion_matrix(labels, predictions):\n",
        "    \"\"\"Calcula la matriz de confusion.\n",
        "    \n",
        "    Args:\n",
        "        labels: Array binario 1-D con las etiquetas reales.\n",
        "        predictions: Array binario 1-D con las predicciones.\n",
        "        \n",
        "    Returns:\n",
        "        TP: Numero de verdaderos positivos.\n",
        "        FP: Numero de falsos positivos.\n",
        "        FN: Numero de falsos negativos.\n",
        "        TN: Numero de verdaderos negativos.\n",
        "    \"\"\"\n",
        "    # Map labels and predictions to {0, 1, 2, 3}\n",
        "    encoded_data = 2 * labels + predictions  \n",
        "    TN = np.sum(encoded_data == 0)  # True negatives\n",
        "    FP = np.sum(encoded_data == 1)  # False positives\n",
        "    FN = np.sum(encoded_data == 2)  # False negatives\n",
        "    TP = np.sum(encoded_data == 3)  # True positives\n",
        "    return TP, FP, FN, TN\n",
        "\n",
        "\n",
        "def performance_metrics(TP, FP, FN, TN):\n",
        "    \"\"\"Calcula metricas de desempeño.\n",
        "    \n",
        "    Args:\n",
        "        TP: Numero de verdaderos positivos.\n",
        "        FP: Numero de falsos positivos.\n",
        "        FN: Numero de falsos negativos.\n",
        "        TN: Numero de verdaderos negativos.\n",
        "    \n",
        "    Returns:\n",
        "        accuracy: Porcentaje de clasificaciones correctas del detector.\n",
        "        precision: Precision del detector.\n",
        "        recall: Recall/Sensibilidad del detector.\n",
        "    \"\"\"\n",
        "    accuracy = 100.0 * (TP + TN) / (TP + TN + FP + FN)\n",
        "    precision = 100.0 * TP / (TP + FP)\n",
        "    recall = 100.0 * TP / (TP + FN)\n",
        "    print('TP: %d, TN: %d, FP: %d, FN: %d' %(TP,TN,FP,FN))\n",
        "    print('%1.4f%% Accuracy (Porcentaje de clasificaciones correctas)' % (accuracy))\n",
        "    print('%1.4f%% Precision' % (precision))\n",
        "    print('%1.4f%% Recall' % (recall))\n",
        "    print('')\n",
        "    return accuracy, precision, recall\n",
        "\n",
        "\n",
        "def roc_curve(labels, probabilities):\n",
        "    \"\"\"Calcula la curva ROC.\n",
        "    \n",
        "    Args:\n",
        "        labels: Array binario 1-D con las etiquetas reales.\n",
        "        probabilities: Array 1-D continuo en el rango [0, 1] con las\n",
        "            probabilidades de la clase 1.\n",
        "        \n",
        "    Returns:\n",
        "        tpr: Array 1-D con los valores de Tasa de Verdaderos Positivos (TPR).\n",
        "        fpr: Array 1-D con los valores de Tasa de Falsos Positivos (FPR).\n",
        "    \"\"\"\n",
        "    tpr = []\n",
        "    fpr = []\n",
        "    for threshold in np.linspace(0, 1, 1000):\n",
        "        probabilities_with_threshold = (probabilities > threshold).astype(np.float)\n",
        "        TP, FP, FN, TN = confusion_matrix(\n",
        "            labels, \n",
        "            probabilities_with_threshold)\n",
        "        tpr.append(TP/(TP+FN))\n",
        "        fpr.append(FP/(FP+TN))\n",
        "    return np.array(tpr), np.array(fpr)\n",
        "  \n",
        "  \n",
        "def detection_performance_given_threshold(true_labels, prediction, threshold):\n",
        "    probabilities_with_threshold = (prediction > threshold).astype(np.float)\n",
        "    TP, FP, FN, TN = confusion_matrix(\n",
        "        true_labels, \n",
        "        probabilities_with_threshold)    \n",
        "    return TP, FP, FN, TN       \n",
        "\n",
        "  \n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.grid('off')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "\n",
        "predicted_train_labels = mlp.predict_label(training_images)\n",
        "predicted_val_labels = mlp.predict_label(validation_images)\n",
        "predicted_test_labels = mlp.predict_label(testing_images)\n",
        "\n",
        "cnf_matrix = sk_conf_mat(testing_labels, predicted_test_labels)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=['%d' % RUT_veri_number, 'no %d' % RUT_veri_number],\n",
        "                      title='Confusion matrix')\n",
        "\n",
        "\n",
        "print('Training results:')\n",
        "TP, FP, FN, TN = confusion_matrix(training_labels, predicted_train_labels)\n",
        "accuracy, precision, recall = performance_metrics(TP, FP, FN, TN)\n",
        "\n",
        "print('Validation results:')\n",
        "TP, FP, FN, TN = confusion_matrix(validation_labels, predicted_val_labels)\n",
        "accuracy, precision, recall = performance_metrics(TP, FP, FN, TN)\n",
        "\n",
        "print('Test results:')\n",
        "TP, FP, FN, TN = confusion_matrix(testing_labels, predicted_test_labels)\n",
        "accuracy, precision, recall = performance_metrics(TP, FP, FN, TN)\n",
        "\n",
        "predicted_test_proba = mlp.predict_proba(testing_images)\n",
        "tpr, fpr = roc_curve(testing_labels, predicted_test_proba[:, 1])\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(13, 5))\n",
        "\n",
        "ax[0].set_title('ROC Curve')\n",
        "ax[0].plot(fpr, tpr, linewidth=2, alpha=0.5)\n",
        "ax[0].set_xlabel('False Positive Rate')\n",
        "ax[0].set_ylabel('True Positive Rate')\n",
        "\n",
        "ax[1].set_title('DET Curve')\n",
        "ax[1].plot(fpr, 1.0 - tpr, linewidth=2, alpha=0.5)\n",
        "ax[1].set_xlabel('False Positive Rate')\n",
        "ax[1].set_ylabel('False Negative Rate')\n",
        "ax[1].set_yscale('log')\n",
        "ax[1].set_xscale('log')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[932  42]\n",
            " [ 26 948]]\n",
            "Training results:\n",
            "TP: 5254, TN: 5193, FP: 196, FN: 135\n",
            "96.9289% Accuracy (Porcentaje de clasificaciones correctas)\n",
            "96.4037% Precision\n",
            "97.4949% Recall\n",
            "\n",
            "Validation results:\n",
            "TP: 445, TN: 445, FP: 17, FN: 17\n",
            "96.3203% Accuracy (Porcentaje de clasificaciones correctas)\n",
            "96.3203% Precision\n",
            "96.3203% Recall\n",
            "\n",
            "Test results:\n",
            "TP: 948, TN: 932, FP: 42, FN: 26\n",
            "96.5092% Accuracy (Porcentaje de clasificaciones correctas)\n",
            "95.7576% Precision\n",
            "97.3306% Recall\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/__init__.py:424: MatplotlibDeprecationWarning: \n",
            "Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
            "  warn_deprecated(\"2.2\", \"Passing one of 'on', 'true', 'off', 'false' as a \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAEYCAYAAAAtTS8wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xm8VWW9x/HPl0FEBEFBVEAxRVEp\nBZRM00zU0Exocgwncso083rLqauWpTlchzTnHK9zmfOUWamJMThPiOQAooAKooAy/O4f6zm0hTPs\nA+uw9z7r++61Xuy11rPX+m3RX8+0nqWIwMysiNpUOgAzs0pxAjSzwnICNLPCcgI0s8JyAjSzwnIC\nNLPCcgIsKEkdJd0taZak25bjOvtJeijP2CpF0naSXq10HLbiyPMAq5ukfYFjgf7AbOAZ4NcR8fhy\nXnckcBSwTUQsWO5Aq5ykAPpFxMRKx2LVwzXAKibpWOB84DdAT2Bd4PfA8Bwuvx4woQjJrxyS2lU6\nBquAiPBWhRuwGvAx8P1GynQgS5DvpO18oEM6twMwGfgvYBowFTgonTsN+AyYn+4xCjgVuKHk2n2B\nANql/QOBSWS10H8D+5Ucf7zke9sAY4BZ6c9tSs79DfgV8ES6zkNA9wZ+W138PyuJfwSwGzAB+AA4\nsaT8EOBJYGYqexGwUjr3j/RbPkm/d6+S6/8ceBe4vu5Y+s4G6R6D0v46wHRgh0r/u+Etx//OKh2A\ntwb+YmAYsKAuATVQ5pfAaGBNoAfwT+BX6dwO6fu/BNqnxDEH6JbOL5nwGkyAQCfgI2DjdG5tYLP0\neXECBFYHPgRGpu/tk/bXSOf/BrwObAR0TPtnNvDb6uL/nxT/ISkB3Qh0BjYD5gLrp/KDga3TffsC\nLwPHlFwvgA3ruf5vyf6PpGNpAkxlDgFeAlYBHgTOqfS/F97y3dwErl5rADOi8SbqfsAvI2JaREwn\nq9mNLDk/P52fHxH3kdV+Nl7GeBYBAyR1jIipEfFiPWW+CbwWEddHxIKIuAl4BfhWSZmrI2JCRMwF\nbgW2aOSe88n6O+cDNwPdgQsiYna6/0vA5gARMS4iRqf7vgFcBnytjN90SkR8muL5nIi4ApgIPEWW\n9E9q4npWY5wAq9f7QPcm+qbWAd4s2X8zHVt8jSUS6Bxg1eYGEhGfkDUbDwemSrpXUv8y4qmLqVfJ\n/rvNiOf9iFiYPtclqPdKzs+t+76kjSTdI+ldSR+R9Zt2b+TaANMjYl4TZa4ABgC/i4hPmyhrNcYJ\nsHo9CXxK1u/VkHfIBjPqrJuOLYtPyJp6ddYqPRkRD0bEzmQ1oVfIEkNT8dTFNGUZY2qOS8ji6hcR\nXYATATXxnUanQEhalaxf9SrgVEmr5xGoVQ8nwCoVEbPI+r8uljRC0iqS2kvaVdJZqdhNwMmSekjq\nnsrfsIy3fAbYXtK6klYDTqg7IamnpOGSOpEl5Y/Jmo9Lug/YSNK+ktpJ2gvYFLhnGWNqjs5k/ZQf\np9rpEUucfw/4QjOveQEwNiJ+CNwLXLrcUVpVcQKsYhFxLtkcwJPJBgDeBn4M/DkVOR0YCzwHPA+M\nT8eW5V4PA7eka43j80mrTYrjHbKR0a+xdIIhIt4HdicbeX6fbAR394iYsSwxNdNxwL5ko8tXkP2W\nUqcC10qaKWnPpi4maTjZQFTd7zwWGCRpv9witorzRGgzKyzXAM2ssJwAzaywnADNrLCcAM2ssFrN\nA+Bq1zHUoUulw7B6bNG/T6VDsEY8PX7cjIjokdf12nZZL2LBUg/W1CvmTn8wIoblde/maj0JsEMX\nOmzqGQrV6B+Pn1PpEKwRnVduu+TTO8slFsylw8ZNzjQCYN4zFzf1tE6LajUJ0MyqhUC10bvmBGhm\n+RLQpm2loyiLE6CZ5U9NPYZdHZwAzSxnbgKbWZG5BmhmhSS5D9DMCsxNYDMrLDeBzayYPAhiZkXl\neYBmVlyuAZpZkbVxH6CZFZFwDdDMisrzAM2syDwNxswKy01gMyskyTVAMysw9wGaWTF5HqCZFZmb\nwGZWSJ4HaGbF5SawmRWZB0HMrLDcB2hmhSQ3gc2syFwDNLMiEtCmjWuAZlZESlsNqI00bWY1REjl\nbU1eSfqppBclvSDpJkkrS1pf0lOSJkq6RdJKqWyHtD8xne/b1PWdAM0sd3kkQEm9gKOBLSNiANAW\n2Bv4LXBeRGwIfAiMSl8ZBXyYjp+XyjXKCdDMctemTZuytjK0AzpKagesAkwFdgRuT+evBUakz8PT\nPun8UDWRZZ0AzSxfasbWiIiYApwDvEWW+GYB44CZEbEgFZsM9EqfewFvp+8uSOXXaOweToBmlis1\nrw+wu6SxJduhi68jdSOr1a0PrAN0AoblGatHgc0sd+UMcCQzImLLBs7tBPw7Iqana/4J2BboKqld\nquX1Bqak8lOAPsDk1GReDXi/sZu7BmhmucupD/AtYGtJq6S+vKHAS8CjwPdSmQOAO9Pnu9I+6fxf\nIyIau4FrgGaWr5zmAUbEU5JuB8YDC4CngcuBe4GbJZ2ejl2VvnIVcL2kicAHZCPGjXICNLPcNaMJ\n3KiIOAU4ZYnDk4Ah9ZSdB3y/Odd3AjSzXNUNgtQCJ0Azy50ToJkVk0BtnADNrKBcAzSzwnICNLNC\n8iCINenIvbfjoBFbI4mr/zyai276B/9z+DB2334AiyKY/sHHHHraTUyd8RF7DxvEsfvviCQ+njOP\no8/8I8+/9k6lf0JhLFy4kO23GcLa66zD7XfczagDfsD48eNo3749g7fcigsvvpT27dtXOszqUUN9\ngH4SpAI23WAtDhqxNdsdcD5D9j2HXb+6KV/o3Z3zrn+UIfuew9b7ncv9j7/ECT/cBYA33vmAXQ67\nmK32OZszrnqYi09s1lQnW06/v+hCNt64/+L9PffZl/HPvcRT455l3ty5XHv1lRWMrjrltR5gS3MC\nrID+fXsy5oW3mPvpfBYuXMRj419nxNe/yOxPPl1cZpWOK1H3FM/o595g5uy5APzr+TfptWbXisRd\nRFMmT+bB++/jgINGLT72jWG7Lf4PePBWQ5gyeUojVygmJ0Br0IuvT2XbLdZn9dVWoWOH9gzbZhN6\n98yS2qlH7Mpr9/yCvYcN4leXPbDUdw8c/mUe/OfLKzrkwvr5f/+UX/3mzHqfW50/fz4333gDO+3y\njQpEVuVyWA5rRajaBFjfUtiVjikvr74xjXOve5S7f3cYd114KM9OmMLCRVlt79RL7qff7r/i5gfG\nc/ieX/3c97YfvCEH7PFlTr7onkqEXTj333cPPXqsycBBg+s9/9Ojj2Tbr27Htl/dbgVHVt0k5bkg\naouqfAT1aGQp7Fbj2rueYtv9z2Pnwy5m5uy5vPbWtM+dv+X+cYzY8UuL9wdsuDaXnLwn3z/uD3ww\na86KDreQRv/zn9x3791sttEXOHD/ffnH3x7lhweOBOCM03/JjBnTOeOscyscZXVyE3j5LbkUdqsa\n9uzRbVUA+vTsyvCvf5FbHhjPBn26Lz6/+9cGMOGNaYvL3HzWQYw65UYmvjW9IvEW0Wmn/4ZXX3+L\nFydM4prrbmT7Hb7OlddczzV/uJK//OUhrr7uxqqoxVSjWkmAVTkNJiKmSKpbCnsu8FBEPLRkubR6\nbLaC7EqdV2iMy+um3x7I6qutwvwFizjmrD8x6+N5XPqLvem3Xg8WLQreevdDjj4je+3BCT/chdVX\nW4Xzf/5dABYsWMRXDzivkuEX2jFH/Yh1112PoV/bFoA9hn+b40/6RYWjqjKVz21lURPrBVZEWgr7\nj8BewEzgNuD2iLihoe+06dQzOmy63wqK0Jpj+uPnVDoEa0TnlduOa2RV5mbrsFa/6L3fhWWVnfS/\nu+V67+aq1vr74qWwI2I+8CdgmwrHZGZlECCVt1VaVTaBKVkKm6wJPBQYW9mQzKw81dG/V46qTICN\nLIVtZjWgRvJfdSZAaHApbDOrdoI2NfIscNUmQDOrTcIJ0MwKzE1gMyssD4KYWTFVyRSXcjgBmlmu\nhGrmEUEnQDPLnWuAZlZY7gM0s2JyH6CZFZXnAZpZobkJbGaFVSP5zwnQzHIm1wDNrKCyeYBOgGZW\nUDVSAXQCNLP8uQlsZsXkeYBmVlTZPMDaeBa4NqI0s5qS50uRJHWVdLukVyS9LOkrklaX9LCk19Kf\n3VJZSbpQ0kRJz0ka1Ni1nQDNLHc5vxj9AuCBiOgPbA68DBwPPBIR/YBH0j7ArkC/tB0KXNLYhZ0A\nzSxfZdb+ysl/klYDtgeuAoiIzyJiJjAcuDYVuxYYkT4PB66LzGigq6S1G7q+E6CZ5UqUV/srswa4\nPjAduFrS05KulNQJ6BkRU1OZd4Ge6XMv4O2S709Ox+rlBGhmuWvbRmVtQHdJY0u2Q5e4VDtgEHBJ\nRAwEPuE/zV0AIiKAWJY4PQpsZrlrxjSYGRGxZSPnJwOTI+KptH87WQJ8T9LaETE1NXGnpfNTgD4l\n3++djtWrwRqgpC6NbeX8MjMrHim/QZCIeBd4W9LG6dBQ4CXgLuCAdOwA4M70+S5g/zQavDUwq6Sp\nvJTGaoAvklUrS6Os2w9g3SajN7NCyvlR4KOA/5O0EjAJOIis8narpFHAm8Ceqex9wG7ARGBOKtug\nBhNgRPRp6JyZWWPyXAwhIp4B6msmD62nbABHlnvtsgZBJO0t6cT0ubekweXewMyKRaSR4DL+V2lN\nJkBJFwFfB0amQ3OAS1syKDOrbW1U3lZp5YwCbxMRgyQ9DRARH6S2uJnZ0pr3lEdFlZMA50tqQ5pn\nI2kNYFGLRmVmNUtQN8ev6pXTB3gx8Eegh6TTgMeB37ZoVGZW0/JcDKElNVkDjIjrJI0DdkqHvh8R\nL7RsWGZWy1pTExigLTCfrBnsx+fMrEHVUrsrRzmjwCcBNwHrkD1WcqOkE1o6MDOrXW2lsrZKK6cG\nuD8wMCLmAEj6NfA0cEZLBmZmtas1NYGnLlGuXTpmZrYUUR1z/MrRYAKUdB5Zn98HwIuSHkz7uwBj\nVkx4ZlZzWsk8wLqR3heBe0uOj265cMysNaj5F6NHxFUrMhAzax1aRRO4jqQNgF8DmwIr1x2PiI1a\nMC4zq2G10gQuZ07fNcDVZIl9V+BW4JYWjMnMapzK3CqtnAS4SkQ8CBARr0fEyWSJ0MxsKRK0kcra\nKq2caTCfpsUQXpd0ONn6+p1bNiwzq2U1PwhS4qdAJ+Bosr7A1YCDWzIoM6ttVVC5K0s5iyHUvY1p\nNv9ZFNXMrF6iOpq35WhsIvQdNPKuzYj4TotEtIwG9u/DE0/+b6XDsHp02+rHlQ7BVqQaWgyhsRrg\nRSssCjNrVaphoYNyNDYR+pEVGYiZtQ6iduYBlrseoJlZ2WpkENgJ0Mzy1+oSoKQOEfFpSwZjZrVP\nakUvRZI0RNLzwGtpf3NJv2vxyMysZtXKS5HKeRTuQmB34H2AiHiW7EXpZmZLyVaDaT2PwrWJiDeX\nGNVZ2ELxmFkrUCtvTisnAb4taQgQktoCRwETWjYsM6tVkmqmD7CcBHgEWTN4XeA94C/pmJlZvaqg\ndVuWcp4FngbsvQJiMbNWokYqgGWtCH0F9TwTHBGHtkhEZlbT6gZBakE5TeC/lHxeGfg28HbLhGNm\nNU/QtkZGQcppAn9u+XtJ1wOPt1hEZlbzVBUL3jdtWR6FWx/omXcgZtY6tLa3wn3If/oA25C9KP34\nlgzKzGpbq0iAymY/b072HhCARRHR4CKpZmaQ73JYaf7xWGBKROwuaX3gZmANYBwwMiI+k9QBuA4Y\nTPbk2l4R8UZj1260qzIlu/siYmHanPzMrFFKgyDlbGX6CfByyf5vgfMiYkPgQ2BUOj4K+DAdPy+V\na1Q5ITwjaWDZoZpZ4eX1LLCk3sA3gSvTvoAdgdtTkWuBEenz8LRPOj9UTVRFG3snSLuIWAAMBMZI\neh34hKyPMyJiUJPRm1nhNHMQpLuksSX7l0fE5SX75wM/4z+v4l0DmJlyE8BkoFf63Is0RS8iFkia\nlcrPaOjmjfUB/gsYBOxR5g8xMwOa9SjcjIjYsv5raHdgWkSMk7RDTqF9TmMJUAAR8XpL3NjMWieh\nvF6KtC2wh6TdyB7C6AJcAHQtaaH25j+DtFOAPsBkSe3I3mH+fmM3aCwB9pB0bEMnI8LvoDSzpSmf\naTARcQJwAkCqAR4XEftJug34HtlI8AHAnekrd6X9J9P5vzY1cNtYAmwLrAo1MqXbzKpGCz8L/HPg\nZkmnA08DV6XjVwHXS5pINl+5yUVcGkuAUyPil8sbqZkVS/ZazHyvGRF/A/6WPk8ChtRTZh7w/eZc\nt8k+QDOz5moNC6IOXWFRmFmrIVrBkvgR8cGKDMTMWgnl+yhcS/KL0c0sd7WR/pwAzSxngrzmAbY4\nJ0Azy12N5D8nQDPLm9wHaGbF1CpGgc3MlpVrgGZWTGpdr8U0Myubm8BmVmhuAptZYdVG+nMCNLOc\neSK0mRVajeQ/J0Azy5tQjTSCnQDNLHeuAZpZIUnuAzSzAquR/Fcz8xVbrbfffptv7PR1Bn5pUwZt\nvhkXXXjB4nO/v+h3bD6gP4M234wTj/9ZBaMsliP32YGxt53IuNtP4sf77vC5cz8ZuSNzn76INbp2\nAqDLqitz+/mH8dQtxzPu9pMYucfWFYi4+qjM/1Waa4AV1q5dO84861wGDhrE7Nmz2ebLgxm6085M\nm/Ye99x9J/8a9ywdOnRg2rRplQ61EDbdYG0O+s42bDfybD6bv5C7Lv4R9z32ApPenkHvnl0ZuvUm\nvDX1P4ulH7bn9rwy6V2+d8xldO+2Ks/e8Qtuvm8M8xcsrOCvqCyRz2sxVwTXACts7bXXZuCgQQB0\n7tyZ/v034Z13pnD5ZZdw3M+Op0OHDgCsueaalQyzMPqvvxZjXniDufPms3DhIh4bN5ERO24BwFnH\nfZeTLvgzpa+aDWDVTtnfUaeOHfhw1hwWLFxUidCrShuprK3SnACryJtvvMEzzzzNVkO+zMQJE3ji\n8cfYbpsvs/OOX2PsmDGVDq8QXnz9HbYduCGrr9aJjiu3Z9hXN6P3Wt3YfYcv8s60mTw/Ycrnyl96\n89/pv/5aTHro14y97USOO/t2mngXdyG4CbwcJK0LXAt0JXtB+/ERcV9lo2pZH3/8Mfvs+V3OPvd8\nunTpwoKFC/jggw/4xxOjGTtmDD/Yd09enjCpZp6xrFWv/vs9zr3mYe7+/ZHMmfcZz746mZXat+Nn\nB3+D3X900VLld95mE557dTLDDr2QL/Tpzr2X/Jgn9nqd2Z/Mq0D01cFN4OV3MnBrRAwke7v77ysc\nT4uaP38+++z5XfbaZz9GfPs7APTq1ZsR3/4OkthqyBDatGnDjBkzKhxpMVz75yfZdr+z2HnU+cz8\naA4vvz6V9Xqtwb9uOYFX7j2NXmt25ckbf07PNTozco+tufOvzwIw6e0ZvDHlfTbu27PCv6DSyq3/\nVT5LtkgClNRX0suSrpD0oqSHJHVM57aQNFrSc5LukNStnksE0CV9Xg14pyXirAYRweGHjGLj/pvw\nk58eu/j4t/YYwd//9igAr02YwGeffUb37t0rFWah9Oi2KgB91urG8B0354a7n2K9oSfQ/5un0P+b\npzBl2ky+su9vee/92bz97ofsMGRjANZcvTMb9e3Jv6cU/P+olNUAy9kqrSWbwP2AfSLiEEm3At8F\nbgCuA46KiL9L+iVwCnDMEt89FXhI0lFAJ2CnFoyzov75xBPc+H/XM2DAF/ny4Kyz/bTTf8MBBx3M\nYT88mMFbDGCl9itx5R+udfN3BbnpnB+yetdOzF+wkGPOvJVZH89tsOyZVzzA5af9gDG3nogEJ11w\nJ+/P/GQFRlt9siZwbfy7qpbosJXUF3g4Ivql/Z8D7YHfAc9HxLrp+AbAbRExaInvH5tiO1fSV4Cr\ngAERsWiJcocChwL0WXfdwRNefzP332LLr9tWP650CNaIec9cPC4itszrept8cWBcfcejZZX9Sr9u\nud67uVqyD/DTks8LaV5tcxRwK0BEPAmsDCzV/ouIyyNiy4jYskf3HssTq5nlSWVuFbZCB0EiYhbw\noaTt0qGRwN/rKfoWMBRA0iZkCXD6CgnSzJZbrQyCVGIazAHApZJWASYBB9VT5r+AKyT9lGxA5MDw\n5CqzmlENAxzlaJEEGBFvAANK9s8p+fwM0OgDkxHxErBtS8RmZitAkROgmRVX1r1XGxnQCdDM8qXa\nWQ7LCdDMcucEaGYFVR0jvOWo1meBzayGSeVtTV9HfSQ9Kuml9FjtT9Lx1SU9LOm19Ge3dFySLpQ0\nMT1uO6ix6zsBmlmuyp0DXWYdcQHwXxGxKdnskSMlbQocDzySnjZ7JO0D7Er2GG4/sqfELmns4k6A\nZpY7SWVtTYmIqRExPn2eDbwM9AKGky2ZR/pzRPo8HLguMqOBrpLWbuj67gM0s9w1YxCku6SxJfuX\nR8Tl9V9TfYGBwFNAz4iYmk69C9StQdYLeLvka5PTsanUwwnQzHLXjCGQGeUshiBpVeCPwDER8VFp\n7TEiQtIyPSnmJrCZ5SvnTkBJ7cmS3/9FxJ/S4ffqmrbpz7q3hk0B+pR8vXc6Vi8nQDPLVd16gHm8\nFElZVe8q4OWI+N+SU3eRrStA+vPOkuP7p9HgrYFZJU3lpbgJbGa5y3EW4LZkq0Y9L+mZdOxE4Ezg\nVkmjgDeBPdO5+4DdgInAHOpfbGUxJ0Azy19OGTAiHm/kakPrKR/AkeVe3wnQzHJXK0+COAGaWe78\nLLCZFZYToJkVktcDNLPi8nqAZlZkNZL/nADNLG/lLXRQDZwAzSx3NZL/nADNLF9V8s7zsjgBmln+\naiQDOgGaWe7KWeigGjgBmlnuaiP9OQGaWd48D9DMiq02MqAToJnlKlsQtdJRlMcJ0Mxy5yawmRWW\nF0Mws+KqjfznBGhm+ZLcB2hmBeYmsJkVV23kPydAM8tfjeQ/J0Azy5+nwZhZIQnVzGIIbSodgJlZ\npbgGaGa5q5EKoBOgmeXP02DMrJA8EdrMis0J0MyKyk1gMyssD4KYWWE5AZpZYdVKE1gRUekYciFp\nOvBmpePIUXdgRqWDsHq1tr+b9SKiR14Xk/QA2T+jcsyIiGF53bu5Wk0CbG0kjY2ILSsdhy3Nfzet\nhx+FM7PCcgI0s8JyAqxel1c6AGuQ/25aCfcBmllhuQZoZoXlBGhmheUEaJYDqVaefbBSfhKkSkja\nGugHvAaMj4jPKhySNUHSVkAPYFpEjJWkcKd6TXENsApI2oNsZHEn4DhgvcpGZE2R9A3gRmAH4BFJ\nX3Lyqz2uAVaYpDWAI4F9I+IFSX8AtpA0C/goIuZVNkJbkqS1gV8DR0bEQ5I+BjpI6h0RkyscnjWD\na4CVtwDoCPSX1IWsRrE/cD5wsqROFYzN6jcTGA18JGkj4FjgaGCMpL3AfYK1wgmwwiJiFnAhcALw\nEHB1RHwLuBLoDWxYwfCsfnX9s4cA9wBnR8RI4CDgMklfdHO4NjgBVoGIuJ2s/+8x4Ol07K9AZ9wf\nWFXSQMdC4BjgKOAPwF8BIuIB4BZglcpFaM3hPsAqEREfSvorsKekz4CVgfWB5yobmZWKiJDUJiIW\nAAtSU/cHkiYDQ4HtyfoHrQb4UbgqIqkrWf/fd4F5wM8i4tnKRlVsDU1tSUlwkaSOwG3A+8DGwMER\n8dKKjtOWjRNgFZLUmezv5qNKx1JkpclP0o7Ax2SVwDHp2EoR8ZmktmQ19vYRMbNyEVtzOQGaNUHS\n0cBIsv7ZDYD7I+KcdK5t6hO0GuQ+QLNGSFoH2BvYIyKmStoEuFLSWxFxq5NfbfMosFmJBubvzQM+\nAYiIl4GbgV4rMi5rGU6AZskSfX79ACLiHWAC8MeSop2BfkpWfKSWFzeBzVgq+f0YOFrSaOB+sic9\nzpX0NHAfMBz4nic71z4nQDOyoV1YvDDFl4BdgR2BIUCXiDhC0u5AW+CaiHitYsFabjwKbJZI6gU8\nCfwlIg6WtBLwHeArwBvAZRExp4IhWs7cB2iWRMQUskfchknaO63JeCswHlgT6FDJ+Cx/bgKblYiI\nP0n6FDhDEhFxs6TrgU4RMbvS8Vm+nADNlhAR90paBFwuaUFarMLJrxVyH6BZAyTtDLweEZMqHYu1\nDCdAMyssD4KYWWE5AZpZYTkBmllhOQGaWWE5AZpZYTkBtkKSFkp6RtILkm6TtMwv6ZG0g6R70uc9\nJB3fSNmukn60DPc4VdJx5R5fosw1kr7XjHv1lfRCc2O01skJsHWaGxFbRMQAslc4Hl56Mq3i1Oy/\n+4i4KyLObKRIV6DZCdCsUpwAW7/HgA1TzedVSdcBLwB9JO0i6UlJ41NNcVUAScMkvSJpPNliAKTj\nB0q6KH3uKekOSc+mbRvgTGCDVPs8O5X7b0ljJD0n6bSSa50kaYKkx8leJtQoSYek6zwr6Y9L1Gp3\nkjQ2XW/3VL6tpLNL7n3Y8v6DtNbHCbAVk9SObFmn59OhfsDvI2IzshWOTwZ2iohBwFjgWEkrA1cA\n3wIGA2s1cPkLgb9HxObAIOBF4HiyJye2iIj/lrRLuucQYAtgsKTtJQ0mW2Z+C2A3YKsyfs6fImKr\ndL+XgVEl5/qme3wTuDT9hlHArIjYKl3/EEnrl3EfKxA/C9w6dZT0TPr8GHAVsA7wZkSMTse3BjYF\nnkiLGq9EthRUf+DfdevdSboBOLSee+xI9gpP0nsxZknqtkSZXdL2dNpflSwhdgbuqFtaStJdZfym\nAZJOJ2tmrwo8WHLu1ohYBLwmaVL6DbsAXyrpH1wt3XtCGfeygnACbJ3mRsQWpQdSkvuk9BDwcETs\ns0S5z31vOQk4IyIuW+IexyzDta4BRkTEs5IOBHYoObfk85yR7n1URJQmSiT1XYZ7WyvlJnBxjQa2\nlbQhgKROkjYCXgH6Stogldunge8/AhyRvttW0mpkK6Z0LinzIHBwSd9iL0lrAv8ARkjqqOwdyN8q\nI97OwFRJ7YH9ljj3fUltUsxfAF5N9z4ilUfSRpI6lXEfKxDXAAsqIqanmtRNkuoW+jw5IiZIOhS4\nV9IcsiZ053ou8ROy5aJGAQuBIyLiSUlPpGkm96d+wE2AJ1MN9GPgBxExXtItwLPANGBMGSH/AngK\nmJ7+LI3pLeBfQBfg8IiYJ+lflKPOAAAAP0lEQVRKsr7B8cpuPh0YUd4/HSsKrwZjZoXlJrCZFZYT\noJkVlhOgmRWWE6CZFZYToJkVlhOgmRWWE6CZFdb/A5OpsPlAiR4OAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwwAAAFQCAYAAAASrPneAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERHR0v8A8nXZsZl6aP8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 936x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lsWfO5koe6wg"
      },
      "source": [
        "### Performance según umbral de clasificación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "neruAyQbecmn",
        "outputId": "1e98e4ca-6445-460f-9913-ed9fa729a450",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predicted_validation_proba = mlp.predict_proba(validation_images)\n",
        "threshold = 0.5\n",
        "TP, FP, FN, TN = detection_performance_given_threshold(validation_labels, predicted_validation_proba[:, 1], threshold=threshold)\n",
        "print('TP: %d, TN: %d, FP: %d, FN: %d' %(TP,TN,FP,FN))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TP: 445, TN: 445, FP: 17, FN: 17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X2zFsvMobYvZ"
      },
      "source": [
        "### Visualización de clasificaciones en el test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KyzPqoEgV0ky",
        "outputId": "a5f7491f-dc5d-4dc0-a4b2-2838b7b010f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        }
      },
      "source": [
        "def show_classifications(images, labels, probabilities, result_type='TP'):\n",
        "    \"\"\" Muestra ejemplos de imagenes para tipos de errores.\n",
        "    \n",
        "    Args:\n",
        "        images: Array de dimensiones (n_ejemplos, n_pixeles) con imagenes.\n",
        "        labels: Array de dimensiones (n_ejemplos,) con las etiquetas reales.\n",
        "        probabilities: Array de dimensiones (n_ejemplos,) con las probabilidades\n",
        "            de la clase 1.\n",
        "        result_type: 'TP', 'FP', 'FN', o 'TP', tipo de error a mostrar.\n",
        "    \"\"\"\n",
        "    dict_types = {'TN': 0, 'FP': 1, 'FN': 2, 'TP': 3}\n",
        "    predictions = (probabilities > 0.5).astype(np.int32)\n",
        "    encoded_data = 2 * labels + predictions \n",
        "    useful = np.where(encoded_data == dict_types[result_type])[0]\n",
        "    size = min(4, useful.shape[0])\n",
        "    chosen = np.random.choice(useful, size=size, replace=False)\n",
        "    fig, ax = plt.subplots(1, 4, figsize=(10,4))\n",
        "    for i, idx in enumerate(chosen):\n",
        "        image = images[idx, :]\n",
        "        digit = labels[idx]\n",
        "        predicted_label = predictions[idx]\n",
        "        proba = probabilities[idx]\n",
        "        ax[i].imshow(image.reshape((28, 28)))\n",
        "        ax[i].set_title(\"True Class: %d\\nPredicted Class: %d\\nProb. of Class 1: %1.4f\"\n",
        "                        % (digit, predicted_label, proba))\n",
        "        ax[i].axis('off')\n",
        "    for j in range(i+1, 4):\n",
        "        ax[j].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "predicted_test_proba = mlp.predict_proba(testing_images)\n",
        "predicted_test_proba = predicted_test_proba[:, 1]\n",
        "\n",
        "print('True Positives:')\n",
        "show_classifications(testing_images, testing_labels, predicted_test_proba , result_type='TP')\n",
        "\n",
        "print('True Negatives:')\n",
        "show_classifications(testing_images, testing_labels, predicted_test_proba , result_type='TN')\n",
        "\n",
        "print('False Positive:')\n",
        "show_classifications(testing_images, testing_labels, predicted_test_proba , result_type='FP')\n",
        "\n",
        "print('False Negative:')\n",
        "show_classifications(testing_images, testing_labels, predicted_test_proba , result_type='FN')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True Positives:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlsAAADGCAYAAADlokXFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "True Negatives:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAloAAADGCAYAAAAKYC77AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5BegLjX4XQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "False Positive:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAloAAADGCAYAAAAKYC77AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkJH/DxRB7j71nrF2AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "False Negative:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAloAAADGCAYAAAAKYC77AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0QIAAEjJ/weADUaBRxhtRQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q30tuRmImaI0"
      },
      "source": [
        "## TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "skWOwVB_h0T-",
        "outputId": "c6db2c3f-22c1-4339-b3a2-16bc200be4f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# ----- Descarga de ngrok para crear tunel\n",
        "%%bash\n",
        "file=\"ngrok-stable-linux-amd64.zip\"\n",
        "if [ -f \"$file\" ]\n",
        "then\n",
        "\techo \"$file already downloaded.\"\n",
        "else\n",
        "    wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "    unzip ngrok-stable-linux-amd64.zip\n",
        "fi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "--2019-08-26 02:20:37--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.73.84.118, 54.165.51.142, 52.201.75.180, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.73.84.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13607069 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  0%  936K 14s\n",
            "    50K .......... .......... .......... .......... ..........  0% 1.81M 11s\n",
            "   100K .......... .......... .......... .......... ..........  1% 60.4M 7s\n",
            "   150K .......... .......... .......... .......... ..........  1% 90.4M 5s\n",
            "   200K .......... .......... .......... .......... ..........  1% 1.88M 6s\n",
            "   250K .......... .......... .......... .......... ..........  2% 66.6M 5s\n",
            "   300K .......... .......... .......... .......... ..........  2% 81.4M 4s\n",
            "   350K .......... .......... .......... .......... ..........  3% 1.93M 4s\n",
            "   400K .......... .......... .......... .......... ..........  3% 61.4M 4s\n",
            "   450K .......... .......... .......... .......... ..........  3% 93.9M 3s\n",
            "   500K .......... .......... .......... .......... ..........  4%  103M 3s\n",
            "   550K .......... .......... .......... .......... ..........  4% 95.0M 3s\n",
            "   600K .......... .......... .......... .......... ..........  4% 98.3M 3s\n",
            "   650K .......... .......... .......... .......... ..........  5%  111M 2s\n",
            "   700K .......... .......... .......... .......... ..........  5% 92.8M 2s\n",
            "   750K .......... .......... .......... .......... ..........  6% 2.11M 3s\n",
            "   800K .......... .......... .......... .......... ..........  6% 78.2M 2s\n",
            "   850K .......... .......... .......... .......... ..........  6%  104M 2s\n",
            "   900K .......... .......... .......... .......... ..........  7% 95.2M 2s\n",
            "   950K .......... .......... .......... .......... ..........  7%  102M 2s\n",
            "  1000K .......... .......... .......... .......... ..........  7% 97.5M 2s\n",
            "  1050K .......... .......... .......... .......... ..........  8%  101M 2s\n",
            "  1100K .......... .......... .......... .......... ..........  8% 2.03M 2s\n",
            "  1150K .......... .......... .......... .......... ..........  9% 63.4M 2s\n",
            "  1200K .......... .......... .......... .......... ..........  9% 96.2M 2s\n",
            "  1250K .......... .......... .......... .......... ..........  9%  102M 2s\n",
            "  1300K .......... .......... .......... .......... .......... 10%  101M 2s\n",
            "  1350K .......... .......... .......... .......... .......... 10%  100M 2s\n",
            "  1400K .......... .......... .......... .......... .......... 10%  103M 2s\n",
            "  1450K .......... .......... .......... .......... .......... 11%  108M 2s\n",
            "  1500K .......... .......... .......... .......... .......... 11% 93.8M 1s\n",
            "  1550K .......... .......... .......... .......... .......... 12%  103M 1s\n",
            "  1600K .......... .......... .......... .......... .......... 12%  139M 1s\n",
            "  1650K .......... .......... .......... .......... .......... 12%  484M 1s\n",
            "  1700K .......... .......... .......... .......... .......... 13%  439M 1s\n",
            "  1750K .......... .......... .......... .......... .......... 13%  443M 1s\n",
            "  1800K .......... .......... .......... .......... .......... 13%  358M 1s\n",
            "  1850K .......... .......... .......... .......... .......... 14% 2.31M 1s\n",
            "  1900K .......... .......... .......... .......... .......... 14% 62.5M 1s\n",
            "  1950K .......... .......... .......... .......... .......... 15% 74.3M 1s\n",
            "  2000K .......... .......... .......... .......... .......... 15% 63.8M 1s\n",
            "  2050K .......... .......... .......... .......... .......... 15%  132M 1s\n",
            "  2100K .......... .......... .......... .......... .......... 16%  447M 1s\n",
            "  2150K .......... .......... .......... .......... .......... 16%  463M 1s\n",
            "  2200K .......... .......... .......... .......... .......... 16%  425M 1s\n",
            "  2250K .......... .......... .......... .......... .......... 17%  414M 1s\n",
            "  2300K .......... .......... .......... .......... .......... 17%  435M 1s\n",
            "  2350K .......... .......... .......... .......... .......... 18%  452M 1s\n",
            "  2400K .......... .......... .......... .......... .......... 18%  470M 1s\n",
            "  2450K .......... .......... .......... .......... .......... 18%  369M 1s\n",
            "  2500K .......... .......... .......... .......... .......... 19%  468M 1s\n",
            "  2550K .......... .......... .......... .......... .......... 19%  471M 1s\n",
            "  2600K .......... .......... .......... .......... .......... 19%  440M 1s\n",
            "  2650K .......... .......... .......... .......... .......... 20%  439M 1s\n",
            "  2700K .......... .......... .......... .......... .......... 20%  479M 1s\n",
            "  2750K .......... .......... .......... .......... .......... 21%  343M 1s\n",
            "  2800K .......... .......... .......... .......... .......... 21%  498M 1s\n",
            "  2850K .......... .......... .......... .......... .......... 21%  372M 1s\n",
            "  2900K .......... .......... .......... .......... .......... 22%  479M 1s\n",
            "  2950K .......... .......... .......... .......... .......... 22%  456M 1s\n",
            "  3000K .......... .......... .......... .......... .......... 22%  462M 1s\n",
            "  3050K .......... .......... .......... .......... .......... 23%  434M 1s\n",
            "  3100K .......... .......... .......... .......... .......... 23%  465M 1s\n",
            "  3150K .......... .......... .......... .......... .......... 24% 2.19M 1s\n",
            "  3200K .......... .......... .......... .......... .......... 24% 65.6M 1s\n",
            "  3250K .......... .......... .......... .......... .......... 24% 71.2M 1s\n",
            "  3300K .......... .......... .......... .......... .......... 25% 62.0M 1s\n",
            "  3350K .......... .......... .......... .......... .......... 25% 72.6M 1s\n",
            "  3400K .......... .......... .......... .......... .......... 25% 88.6M 1s\n",
            "  3450K .......... .......... .......... .......... .......... 26% 97.4M 1s\n",
            "  3500K .......... .......... .......... .......... .......... 26%  108M 1s\n",
            "  3550K .......... .......... .......... .......... .......... 27%  101M 1s\n",
            "  3600K .......... .......... .......... .......... .......... 27%  114M 1s\n",
            "  3650K .......... .......... .......... .......... .......... 27% 79.9M 1s\n",
            "  3700K .......... .......... .......... .......... .......... 28% 92.5M 1s\n",
            "  3750K .......... .......... .......... .......... .......... 28% 92.2M 1s\n",
            "  3800K .......... .......... .......... .......... .......... 28%  104M 1s\n",
            "  3850K .......... .......... .......... .......... .......... 29% 85.7M 1s\n",
            "  3900K .......... .......... .......... .......... .......... 29% 84.9M 1s\n",
            "  3950K .......... .......... .......... .......... .......... 30% 96.2M 1s\n",
            "  4000K .......... .......... .......... .......... .......... 30%  137M 1s\n",
            "  4050K .......... .......... .......... .......... .......... 30%  427M 1s\n",
            "  4100K .......... .......... .......... .......... .......... 31%  478M 1s\n",
            "  4150K .......... .......... .......... .......... .......... 31%  434M 1s\n",
            "  4200K .......... .......... .......... .......... .......... 31%  338M 1s\n",
            "  4250K .......... .......... .......... .......... .......... 32%  364M 1s\n",
            "  4300K .......... .......... .......... .......... .......... 32%  393M 1s\n",
            "  4350K .......... .......... .......... .......... .......... 33%  360M 1s\n",
            "  4400K .......... .......... .......... .......... .......... 33%  422M 1s\n",
            "  4450K .......... .......... .......... .......... .......... 33%  382M 0s\n",
            "  4500K .......... .......... .......... .......... .......... 34%  410M 0s\n",
            "  4550K .......... .......... .......... .......... .......... 34%  393M 0s\n",
            "  4600K .......... .......... .......... .......... .......... 34%  355M 0s\n",
            "  4650K .......... .......... .......... .......... .......... 35%  442M 0s\n",
            "  4700K .......... .......... .......... .......... .......... 35%  496M 0s\n",
            "  4750K .......... .......... .......... .......... .......... 36%  472M 0s\n",
            "  4800K .......... .......... .......... .......... .......... 36%  382M 0s\n",
            "  4850K .......... .......... .......... .......... .......... 36%  483M 0s\n",
            "  4900K .......... .......... .......... .......... .......... 37%  331M 0s\n",
            "  4950K .......... .......... .......... .......... .......... 37%  445M 0s\n",
            "  5000K .......... .......... .......... .......... .......... 38%  422M 0s\n",
            "  5050K .......... .......... .......... .......... .......... 38%  358M 0s\n",
            "  5100K .......... .......... .......... .......... .......... 38%  357M 0s\n",
            "  5150K .......... .......... .......... .......... .......... 39%  366M 0s\n",
            "  5200K .......... .......... .......... .......... .......... 39%  401M 0s\n",
            "  5250K .......... .......... .......... .......... .......... 39%  342M 0s\n",
            "  5300K .......... .......... .......... .......... .......... 40%  375M 0s\n",
            "  5350K .......... .......... .......... .......... .......... 40%  399M 0s\n",
            "  5400K .......... .......... .......... .......... .......... 41%  371M 0s\n",
            "  5450K .......... .......... .......... .......... .......... 41%  453M 0s\n",
            "  5500K .......... .......... .......... .......... .......... 41% 3.75M 0s\n",
            "  5550K .......... .......... .......... .......... .......... 42%  165M 0s\n",
            "  5600K .......... .......... .......... .......... .......... 42%  144M 0s\n",
            "  5650K .......... .......... .......... .......... .......... 42%  225M 0s\n",
            "  5700K .......... .......... .......... .......... .......... 43%  183M 0s\n",
            "  5750K .......... .......... .......... .......... .......... 43%  230M 0s\n",
            "  5800K .......... .......... .......... .......... .......... 44%  244M 0s\n",
            "  5850K .......... .......... .......... .......... .......... 44%  212M 0s\n",
            "  5900K .......... .......... .......... .......... .......... 44%  165M 0s\n",
            "  5950K .......... .......... .......... .......... .......... 45%  256M 0s\n",
            "  6000K .......... .......... .......... .......... .......... 45%  226M 0s\n",
            "  6050K .......... .......... .......... .......... .......... 45%  260M 0s\n",
            "  6100K .......... .......... .......... .......... .......... 46%  190M 0s\n",
            "  6150K .......... .......... .......... .......... .......... 46%  236M 0s\n",
            "  6200K .......... .......... .......... .......... .......... 47%  198M 0s\n",
            "  6250K .......... .......... .......... .......... .......... 47%  288M 0s\n",
            "  6300K .......... .......... .......... .......... .......... 47%  291M 0s\n",
            "  6350K .......... .......... .......... .......... .......... 48% 96.3M 0s\n",
            "  6400K .......... .......... .......... .......... .......... 48% 64.1M 0s\n",
            "  6450K .......... .......... .......... .......... .......... 48% 88.9M 0s\n",
            "  6500K .......... .......... .......... .......... .......... 49% 88.8M 0s\n",
            "  6550K .......... .......... .......... .......... .......... 49%  109M 0s\n",
            "  6600K .......... .......... .......... .......... .......... 50%  102M 0s\n",
            "  6650K .......... .......... .......... .......... .......... 50%  102M 0s\n",
            "  6700K .......... .......... .......... .......... .......... 50% 94.4M 0s\n",
            "  6750K .......... .......... .......... .......... .......... 51%  113M 0s\n",
            "  6800K .......... .......... .......... .......... .......... 51% 94.8M 0s\n",
            "  6850K .......... .......... .......... .......... .......... 51%  120M 0s\n",
            "  6900K .......... .......... .......... .......... .......... 52% 93.8M 0s\n",
            "  6950K .......... .......... .......... .......... .......... 52% 92.5M 0s\n",
            "  7000K .......... .......... .......... .......... .......... 53% 68.7M 0s\n",
            "  7050K .......... .......... .......... .......... .......... 53% 66.4M 0s\n",
            "  7100K .......... .......... .......... .......... .......... 53% 72.3M 0s\n",
            "  7150K .......... .......... .......... .......... .......... 54%  257M 0s\n",
            "  7200K .......... .......... .......... .......... .......... 54%  351M 0s\n",
            "  7250K .......... .......... .......... .......... .......... 54%  296M 0s\n",
            "  7300K .......... .......... .......... .......... .......... 55%  333M 0s\n",
            "  7350K .......... .......... .......... .......... .......... 55%  337M 0s\n",
            "  7400K .......... .......... .......... .......... .......... 56% 3.57M 0s\n",
            "  7450K .......... .......... .......... .......... .......... 56% 84.1M 0s\n",
            "  7500K .......... .......... .......... .......... .......... 56% 77.9M 0s\n",
            "  7550K .......... .......... .......... .......... .......... 57% 80.0M 0s\n",
            "  7600K .......... .......... .......... .......... .......... 57% 77.6M 0s\n",
            "  7650K .......... .......... .......... .......... .......... 57% 83.5M 0s\n",
            "  7700K .......... .......... .......... .......... .......... 58%  101M 0s\n",
            "  7750K .......... .......... .......... .......... .......... 58%  126M 0s\n",
            "  7800K .......... .......... .......... .......... .......... 59%  524M 0s\n",
            "  7850K .......... .......... .......... .......... .......... 59%  483M 0s\n",
            "  7900K .......... .......... .......... .......... .......... 59%  464M 0s\n",
            "  7950K .......... .......... .......... .......... .......... 60%  559M 0s\n",
            "  8000K .......... .......... .......... .......... .......... 60%  424M 0s\n",
            "  8050K .......... .......... .......... .......... .......... 60%  505M 0s\n",
            "  8100K .......... .......... .......... .......... .......... 61%  523M 0s\n",
            "  8150K .......... .......... .......... .......... .......... 61%  460M 0s\n",
            "  8200K .......... .......... .......... .......... .......... 62%  555M 0s\n",
            "  8250K .......... .......... .......... .......... .......... 62%  355M 0s\n",
            "  8300K .......... .......... .......... .......... .......... 62%  462M 0s\n",
            "  8350K .......... .......... .......... .......... .......... 63%  517M 0s\n",
            "  8400K .......... .......... .......... .......... .......... 63%  474M 0s\n",
            "  8450K .......... .......... .......... .......... .......... 63%  528M 0s\n",
            "  8500K .......... .......... .......... .......... .......... 64%  477M 0s\n",
            "  8550K .......... .......... .......... .......... .......... 64%  562M 0s\n",
            "  8600K .......... .......... .......... .......... .......... 65%  507M 0s\n",
            "  8650K .......... .......... .......... .......... .......... 65%  481M 0s\n",
            "  8700K .......... .......... .......... .......... .......... 65%  447M 0s\n",
            "  8750K .......... .......... .......... .......... .......... 66%  313M 0s\n",
            "  8800K .......... .......... .......... .......... .......... 66%  507M 0s\n",
            "  8850K .......... .......... .......... .......... .......... 66%  476M 0s\n",
            "  8900K .......... .......... .......... .......... .......... 67%  444M 0s\n",
            "  8950K .......... .......... .......... .......... .......... 67%  531M 0s\n",
            "  9000K .......... .......... .......... .......... .......... 68%  453M 0s\n",
            "  9050K .......... .......... .......... .......... .......... 68%  490M 0s\n",
            "  9100K .......... .......... .......... .......... .......... 68%  504M 0s\n",
            "  9150K .......... .......... .......... .......... .......... 69%  411M 0s\n",
            "  9200K .......... .......... .......... .......... .......... 69%  374M 0s\n",
            "  9250K .......... .......... .......... .......... .......... 69%  458M 0s\n",
            "  9300K .......... .......... .......... .......... .......... 70%  427M 0s\n",
            "  9350K .......... .......... .......... .......... .......... 70%  471M 0s\n",
            "  9400K .......... .......... .......... .......... .......... 71%  481M 0s\n",
            "  9450K .......... .......... .......... .......... .......... 71%  474M 0s\n",
            "  9500K .......... .......... .......... .......... .......... 71%  518M 0s\n",
            "  9550K .......... .......... .......... .......... .......... 72%  444M 0s\n",
            "  9600K .......... .......... .......... .......... .......... 72%  475M 0s\n",
            "  9650K .......... .......... .......... .......... .......... 72%  424M 0s\n",
            "  9700K .......... .......... .......... .......... .......... 73% 2.62M 0s\n",
            "  9750K .......... .......... .......... .......... .......... 73% 86.7M 0s\n",
            "  9800K .......... .......... .......... .......... .......... 74% 78.2M 0s\n",
            "  9850K .......... .......... .......... .......... .......... 74% 80.5M 0s\n",
            "  9900K .......... .......... .......... .......... .......... 74% 89.2M 0s\n",
            "  9950K .......... .......... .......... .......... .......... 75% 85.2M 0s\n",
            " 10000K .......... .......... .......... .......... .......... 75% 60.3M 0s\n",
            " 10050K .......... .......... .......... .......... .......... 76%  112M 0s\n",
            " 10100K .......... .......... .......... .......... .......... 76% 93.3M 0s\n",
            " 10150K .......... .......... .......... .......... .......... 76%  104M 0s\n",
            " 10200K .......... .......... .......... .......... .......... 77%  107M 0s\n",
            " 10250K .......... .......... .......... .......... .......... 77% 92.7M 0s\n",
            " 10300K .......... .......... .......... .......... .......... 77% 79.8M 0s\n",
            " 10350K .......... .......... .......... .......... .......... 78% 92.1M 0s\n",
            " 10400K .......... .......... .......... .......... .......... 78%  107M 0s\n",
            " 10450K .......... .......... .......... .......... .......... 79%  110M 0s\n",
            " 10500K .......... .......... .......... .......... .......... 79%  103M 0s\n",
            " 10550K .......... .......... .......... .......... .......... 79% 95.3M 0s\n",
            " 10600K .......... .......... .......... .......... .......... 80%  107M 0s\n",
            " 10650K .......... .......... .......... .......... .......... 80%  107M 0s\n",
            " 10700K .......... .......... .......... .......... .......... 80%  104M 0s\n",
            " 10750K .......... .......... .......... .......... .......... 81%  106M 0s\n",
            " 10800K .......... .......... .......... .......... .......... 81%  102M 0s\n",
            " 10850K .......... .......... .......... .......... .......... 82%  109M 0s\n",
            " 10900K .......... .......... .......... .......... .......... 82%  102M 0s\n",
            " 10950K .......... .......... .......... .......... .......... 82%  106M 0s\n",
            " 11000K .......... .......... .......... .......... .......... 83% 74.8M 0s\n",
            " 11050K .......... .......... .......... .......... .......... 83% 82.2M 0s\n",
            " 11100K .......... .......... .......... .......... .......... 83%  109M 0s\n",
            " 11150K .......... .......... .......... .......... .......... 84%  112M 0s\n",
            " 11200K .......... .......... .......... .......... .......... 84%  113M 0s\n",
            " 11250K .......... .......... .......... .......... .......... 85%  437M 0s\n",
            " 11300K .......... .......... .......... .......... .......... 85%  522M 0s\n",
            " 11350K .......... .......... .......... .......... .......... 85%  415M 0s\n",
            " 11400K .......... .......... .......... .......... .......... 86%  537M 0s\n",
            " 11450K .......... .......... .......... .......... .......... 86%  496M 0s\n",
            " 11500K .......... .......... .......... .......... .......... 86%  392M 0s\n",
            " 11550K .......... .......... .......... .......... .......... 87%  433M 0s\n",
            " 11600K .......... .......... .......... .......... .......... 87% 4.54M 0s\n",
            " 11650K .......... .......... .......... .......... .......... 88% 72.4M 0s\n",
            " 11700K .......... .......... .......... .......... .......... 88% 84.6M 0s\n",
            " 11750K .......... .......... .......... .......... .......... 88% 81.1M 0s\n",
            " 11800K .......... .......... .......... .......... .......... 89% 96.9M 0s\n",
            " 11850K .......... .......... .......... .......... .......... 89%  101M 0s\n",
            " 11900K .......... .......... .......... .......... .......... 89%  110M 0s\n",
            " 11950K .......... .......... .......... .......... .......... 90%  168M 0s\n",
            " 12000K .......... .......... .......... .......... .......... 90%  377M 0s\n",
            " 12050K .......... .......... .......... .......... .......... 91%  463M 0s\n",
            " 12100K .......... .......... .......... .......... .......... 91%  489M 0s\n",
            " 12150K .......... .......... .......... .......... .......... 91%  490M 0s\n",
            " 12200K .......... .......... .......... .......... .......... 92%  494M 0s\n",
            " 12250K .......... .......... .......... .......... .......... 92%  426M 0s\n",
            " 12300K .......... .......... .......... .......... .......... 92%  495M 0s\n",
            " 12350K .......... .......... .......... .......... .......... 93%  366M 0s\n",
            " 12400K .......... .......... .......... .......... .......... 93%  432M 0s\n",
            " 12450K .......... .......... .......... .......... .......... 94%  477M 0s\n",
            " 12500K .......... .......... .......... .......... .......... 94%  460M 0s\n",
            " 12550K .......... .......... .......... .......... .......... 94%  512M 0s\n",
            " 12600K .......... .......... .......... .......... .......... 95%  379M 0s\n",
            " 12650K .......... .......... .......... .......... .......... 95%  492M 0s\n",
            " 12700K .......... .......... .......... .......... .......... 95%  374M 0s\n",
            " 12750K .......... .......... .......... .......... .......... 96%  467M 0s\n",
            " 12800K .......... .......... .......... .......... .......... 96%  248M 0s\n",
            " 12850K .......... .......... .......... .......... .......... 97%  384M 0s\n",
            " 12900K .......... .......... .......... .......... .......... 97%  421M 0s\n",
            " 12950K .......... .......... .......... .......... .......... 97%  419M 0s\n",
            " 13000K .......... .......... .......... .......... .......... 98%  377M 0s\n",
            " 13050K .......... .......... .......... .......... .......... 98%  394M 0s\n",
            " 13100K .......... .......... .......... .......... .......... 98%  404M 0s\n",
            " 13150K .......... .......... .......... .......... .......... 99%  317M 0s\n",
            " 13200K .......... .......... .......... .......... .......... 99%  319M 0s\n",
            " 13250K .......... .......... .......... ........             100%  431M=0.4s\n",
            "\n",
            "2019-08-26 02:20:37 (36.5 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13607069/13607069]\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x5DIdn6KhPpf",
        "outputId": "dcfd54bf-35d1-4f4a-d223-b4971da2815c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# ----- Ejecutar despues de nueva corrida para actualizar Tensorboard\n",
        "\n",
        "LOG_DIR = logdir_father  # este es el logdir de nuestros summaries\n",
        "print(\"Showing summaries at %s\" % (LOG_DIR))\n",
        "\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "print('Click URL to open TensorBoard:')\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Showing summaries at ./tarea_1_logs/\n",
            "Click URL to open TensorBoard:\n",
            "https://f0131b01.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7b6uAwj7q2TA"
      },
      "source": [
        "Si quieren borrar su log_dir, utilicen lo siguiente:\n",
        "\n",
        "Ejecutar comandos en bash\n",
        "* %%bash\n",
        "\n",
        "para mirar lo que hay en el directorio actual:\n",
        "* ls \n",
        "\n",
        "para borrar la carpeta \"tarea_1_logs\"\n",
        "* rm -r tarea_1_logs\n",
        "\n",
        "buscar procesos asociados a tensorboard\n",
        "* ps aux | grep tensorboard\n",
        "\n",
        "terminar un proceso\n",
        "* kill -9 process_id (process_id es el pid del proceso)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WDbgq2lP4xZy",
        "outputId": "ab764282-f1c8-4df9-8f13-7a5f7b29a775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "%%bash\n",
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ngrok\n",
            "ngrok-stable-linux-amd64.zip\n",
            "sample_data\n",
            "tarea_1_logs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "veKtB6KRkLIu",
        "colab": {}
      },
      "source": [
        "#!rm -r tarea_1_logs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JviRuQufPeZf",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}